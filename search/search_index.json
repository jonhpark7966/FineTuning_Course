{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":[" "]},"docs":[{"location":"","title":"LLM Fine-Tuning \uc6cc\ud06c\uc0f5","text":"<p>LLM(Large Language Model) Fine-Tuning \uac15\uc758\uc5d0 \uc624\uc2e0 \uac83\uc744 \ud658\uc601\ud569\ub2c8\ub2e4. \uc774 \uac15\uc758\uc5d0\uc11c\ub294 \ub2e4\uc74c \uc8fc\uc81c\ub4e4\uc744 \ub2e4\ub8f9\ub2c8\ub2e4:</p>"},{"location":"#_1","title":"\ud83d\udcda \uac15\uc758 \uac1c\uc694","text":"<ul> <li>3~5\uc77c\uac04\uc758 \ud480\ud0c0\uc784 \uac15\uc758\ub97c \uac00\uc815\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4   </li> <li>\uc774\ub860 &amp; \uc2e4\uc2b5\uc744 \ubcd1\ud589\ud569\ub2c8\ub2e4</li> <li>\ud30c\uc778\ud29c\ub2dd\uacfc \uad00\ub828\ub41c \uc9c0\uc2dd\ub4e4\uc744 \uc804\ub2ec\ud558\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4</li> </ul>"},{"location":"#_2","title":"\ud83d\udccb \uc8fc\uc694 \uc8fc\uc81c","text":"<ul> <li>Fine-Tuning \uac1c\ub150 \ubc0f \ubc29\ubc95\ub860</li> <li>Parameter-Efficient Fine-Tuning (PEFT)</li> <li>Alignment \ubc0f Reasoning \ubaa8\ub378 \uac1c\ubc1c</li> <li>\uc2a4\ud0c0\uc77c \ud29c\ub2dd, \ud55c\uad6d\uc5b4 \ud655\uc7a5 \ub4f1 \uc77c\ubc18\uc801\uc778 \uc0ac\uc6a9\ucc98 \uc2e4\uc2b5</li> <li>\ubaa8\ub378 \ud3c9\uac00 \ubc0f \uc11c\ube59</li> </ul>"},{"location":"#_3","title":"\ud83c\udfaf \uc774 \uc6cc\ud06c\uc0f5\uc758 \ubaa9\ud45c","text":"<ul> <li>LLM Fine-Tuning\uc758 \ud575\uc2ec \uac1c\ub150 \uc774\ud574</li> <li>\ub2e4\uc591\ud55c Fine-Tuning \uae30\ubc95 \uc2e4\uc2b5 \uacbd\ud5d8</li> <li>\uc2e4\uc81c \ud504\ub85c\uc81d\ud2b8\uc5d0 \uc801\uc6a9 \uac00\ub2a5\ud55c \uc9c0\uc2dd \uc2b5\ub4dd</li> <li>\ucd5c\uc2e0 \uc5f0\uad6c \ub3d9\ud5a5 \ud30c\uc545 \ubc0f \ubd84\uc11d \ub2a5\ub825 \ud5a5\uc0c1 </li> </ul> <p>\ubcf8 \uac15\uc758\ub294 \uac1c\ubc1c \uc9c0\uc2dd  (\ud2b9\ud788 python) \uc774 \uae30\ubcf8\uc740 \ub418\uc5b4\uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4  </p>"},{"location":"case_studies/alpaca/","title":"Alpaca - Llama Instruction Tuning","text":""},{"location":"case_studies/alpaca/#alpaca-20233","title":"Alpaca \ud504\ub85c\uc81d\ud2b8 \uac1c\uc694 - 2023.3","text":"<p>Alpaca\ub294 Stanford \ub300\ud559\uc5d0\uc11c \uac1c\ubc1c\ud55c \uacbd\ub7c9 \uba85\ub839\uc5b4-\ub530\ub974\uae30(instruction-following) \uc5b8\uc5b4 \ubaa8\ub378\uc785\ub2c8\ub2e4. Meta\uc758 LLaMA 7B\ub97c \uae30\ubc18\uc73c\ub85c 52K\uac1c\uc758 \uc9c0\uc2dc-\uc751\ub2f5 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \ud30c\uc778\ud29c\ub2dd\ub418\uc5c8\uc73c\uba70, GPT-3.5 \uc218\uc900\uc758 \uc131\ub2a5\uc744 \ubcf4\uc774\uba74\uc11c\ub3c4 \uc7ac\ud604 \ube44\uc6a9\uc774 \ub9e4\uc6b0 \ub0ae\ub2e4\ub294 \ud2b9\uc9d5\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\uac1c\ubc1c \ubc30\uacbd: Closed Source LLM (ex. \ub2f9\uc2dc\uc5d0\ub294 GPT-3.5)\uc5d0 \ud544\uc801\ud558\uba74\uc11c\ub3c4 \ud559\uacc4 \uc5f0\uad6c\uc790\ub4e4\uc774 \uc27d\uac8c \ud65c\uc6a9\ud560 \uc218 \uc788\ub294 \uc624\ud508\uc18c\uc2a4 \ub300\uc548\uc744 \uc81c\uacf5\ud558\uace0\uc790 \ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc800\ube44\uc6a9 \uc811\uadfc: \uc804\uccb4 \ud30c\uc778\ud29c\ub2dd \ube44\uc6a9\uc774 $600 \ubbf8\ub9cc\uc73c\ub85c, \ub370\uc774\ud130 \uc0dd\uc131\uc5d0 \uc57d $500, \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc57d $100\uc774 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\uc9c4\uc815\ud55c \uc624\ud508\uc18c\uc2a4: \ubaa8\ub378 \ucf54\ub4dc, \ub370\uc774\ud130 (\ub370\uc774\ud130 \uc0dd\uc131 \ucf54\ub4dc), \ud559\uc2b5 \ucf54\ub4dc \ubaa8\ub450 \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4. </li> </ul>"},{"location":"case_studies/alpaca/#_1","title":"\ub370\uc774\ud130 \ubc0f \ubc29\ubc95\ub860","text":""},{"location":"case_studies/alpaca/#1","title":"1. \ub370\uc774\ud130 \uc0dd\uc131 \uacfc\uc815","text":"<ul> <li>\uc0ac\ub78c\uc774 \uc9c1\uc811 175\uac1c\uc758 \uc2dc\ub4dc(instruction-output \uc30d) \ub97c \uc900\ube44\ud558\uace0</li> <li>OpenAI GPT-3.5 (text-davinci-003) \ubaa8\ub378\uc744 \ud65c\uc6a9\ud55c self-instruct \uae30\ubc95\uc73c\ub85c 52K \uaddc\ubaa8\uc758 Instruction-Output \uc30d \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \uc0dd\uc131 \ube44\uc6a9 &lt;$500 (OpenAI API \uc0ac\uc6a9)</li> </ul>"},{"location":"case_studies/alpaca/#2","title":"2. \ubaa8\ub378 \ud29c\ub2dd \uacfc\uc815","text":"<ul> <li>Meta\uc758 LLaMA 7B \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c SFT (Supervised Fine-Tuning) \ub97c \ud569\ub2c8\ub2e4. </li> <li>80GB A100 GPU 8\ub300\ub85c 3\uc2dc\uac04 \uc18c\uc694 (\uc57d $100 \ube44\uc6a9) \ub418\uc5c8\ub2e4\uace0 \ud558\ub354\uad70\uc694. </li> <li>HuggingFace \ud504\ub808\uc784\uc6cc\ud06c \ud65c\uc6a9\ud588\uace0, \ucf54\ub4dc\ub3c4 \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4. \ud559\uc2b5 \ucf54\ub4dc </li> </ul>"},{"location":"case_studies/alpaca/#_2","title":"\uacb0\uacfc &amp; \uc758\uc758","text":""},{"location":"case_studies/alpaca/#_3","title":"\uc131\ub2a5","text":"<ul> <li>\uc2f1\uae00\ud134 \uc9c0\uc2dc \uc218\ud589 \ud3c9\uac00\uc5d0\uc11c OpenAI text-davinci-003\uc640 \ub300\ub4f1\ud55c \uc218\uc900 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4!</li> <li>179\uac1c \ube44\uad50 \uc911 Alpaca 90\uac74, GPT-3.5 89\uac74\uc73c\ub85c \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. </li> <li>7B \uc218\uc900\uc758 \uc791\uc740 \ubaa8\ub378\ub85c\ub3c4 \uc2e4\uc0ac\uc6a9 \uc218\uc900\uc758 \uc131\ub2a5 \ub2ec\uc131 \uac00\ub2a5\uc131 \uc785\uc99d\ud55c \uac83\uc774 \uac00\uc7a5 \ud070 \uc758\uc758\ub77c\uace0 \ubcf4\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4. </li> </ul>"},{"location":"case_studies/alpaca/#_4","title":"\ub370\uc774\ud130 \ud6a8\uc728\uc131","text":"<ul> <li>52k \uac1c\uc758 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \uc9c0\ub3c4 \ub370\uc774\ud130\ub85c\ub3c4 \ud070 \uc131\ub2a5 \uac1c\uc120\uc744 \uc774\ub8e8\uc5c8\uc2b5\ub2c8\ub2e4</li> <li>GPT-3.5\ub85c \uc0dd\uc131\ub41c \uace0\ud488\uc9c8 \ub370\uc774\ud130\uc758 \ud6a8\uacfc\uc131\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4</li> <li>\ube44\uc6a9 \ub300\ube44 \ub192\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4</li> </ul>"},{"location":"case_studies/alpaca/#_5","title":"\ub77c\uc774\uc120\uc2a4 \ubc0f \uc81c\ud55c \uc0ac\ud56d","text":"<ul> <li>\ube44\uc601\ub9ac \uc5f0\uad6c \ubaa9\uc801\uc73c\ub85c\ub9cc \uacf5\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4 (LLaMA \ub77c\uc774\uc120\uc2a4 \uc81c\uc57d)</li> <li>\ud658\uac01(hallucination) \ud604\uc0c1\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4</li> </ul>"},{"location":"case_studies/alpaca/#_6","title":"\ud30c\uc0dd \ubaa8\ub378 \ubc0f \uc601\ud5a5","text":""},{"location":"case_studies/alpaca/#_7","title":"\ub2e4\uc591\ud55c \uc5b8\uc5b4 \ubc84\uc804","text":"<ul> <li>KoAlpaca: \ud55c\uad6d\uc5b4 \uba85\ub839\uc5b4 \ub300\uc751\uc744 \uc704\ud55c \ud30c\uc0dd \ubaa8\ub378\uc774 \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4</li> <li>\uac01 \uc5b8\uc5b4\uad8c\ubcc4 \ucd5c\uc801\ud654 \ubaa8\ub378\ub4e4\uc774 \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4</li> </ul>"},{"location":"case_studies/alpaca/#_8","title":"\uc624\ud508\uc18c\uc2a4 \uc0dd\ud0dc\uacc4 \uc601\ud5a5","text":"<ul> <li>Meta\uc758 LLaMA \uc720\ucd9c \uc774\ud6c4 \ub2e4\uc218\uc758 \ud30c\uc0dd LLM\uc774 \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4</li> <li>Vicuna-13B \ub4f1 \uc131\ub2a5 \uac1c\uc120\ub41c \ud6c4\uc18d \ubaa8\ub378\ub4e4\uc774 \ucd9c\ud604\ud588\uc2b5\ub2c8\ub2e4</li> <li>\uc624\ud508\uc18c\uc2a4 LLM\uc758 \ubbfc\uc8fc\ud654\ub97c \ucd09\uc9c4\ud588\uc2b5\ub2c8\ub2e4</li> </ul> <p>\ubcf4\ub2e4 \uc790\uc138\ud55c \ub0b4\uc6a9\uc774 \uad81\uae08\ud558\uc2dc\uba74, LLM \ubc30\uacbd\uc9c0\uc2dd \ubb38\uc11c\ub97c \ucc38\uc870\ud574 \uc8fc\uc138\uc694.</p> <p>\ucc38\uace0 \uc790\ub8cc: Stanford CRFM - Alpaca \ud504\ub85c\uc81d\ud2b8 </p>"},{"location":"case_studies/deepseek/","title":"DeepSeek - \uc911\uad6d\uc758 \uc624\ud508\uc18c\uc2a4 MoE \ubaa8\ub378 \ud601\uc2e0 \uc0ac\ub840","text":"<p>\uadf8 \uc720\uba85\ud55c DeepSeek \uc785\ub2c8\ub2e4. \ubbf8\uad6d \uc99d\uc2dc \uc2dc\uc7a5\uc744 \ub5a8\uc5b4\ub728\ub9b0 \ubaa8\ub378\uc774\uc8e0... 25\ub144 \uc124\ub0a0\uc5d0 \uc9d1\uc5d0 \uac00\ub2c8\uae4c 91\uc138 \ud560\uba38\ub2c8\ub3c4 \uc800\ud55c\ud14c deepseek \uac00 \ubb54\ub370 \ub274\uc2a4\uc5d0\uc11c \uc800\ub7ec\ub0d0\uace0 \ubb3c\uc5b4\ubcf4\uc2dc\ub354\ub77c\uace0\uc694...?!</p>"},{"location":"case_studies/deepseek/#deepseek-v3-r1-202312","title":"DeepSeek V3 &amp; R1 \uc2dc\ub9ac\uc988 - 2023.12","text":"<p>DeepSeek\uc740 \uc911\uad6d \uc2a4\ud0c0\ud2b8\uc5c5\uc774 \uac1c\ubc1c\ud55c \uc624\ud508 \uc6e8\uc774\ud2b8 LLM \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4. \ud2b9\ud788 V3\ub294 Mixture-of-Experts(MoE) \uad6c\uc870\ub97c \ucc44\ud0dd\ud558\uc5ec 671B \ud30c\ub77c\ubbf8\ud130 \uc911 \ud1a0\ud070\ub2f9 37B \uac1c\ub9cc \ud65c\uc131\ud654\ud558\ub294 \ud6a8\uc728\uc801\uc778 \uc124\uacc4\uac00 \ud2b9\uc9d5\uc785\ub2c8\ub2e4. R1\uc740 V3\ub97c \uae30\ubc18\uc73c\ub85c \uac15\ud654\ud559\uc2b5\uc744 \ud1b5\ud574 Reasoning \ub2a5\ub825\uc744 \uac16\ucd94\ub3c4\ub85d \ud2b9\ud654\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4.</p> <ul> <li>\uac1c\ubc1c \ubc30\uacbd: \uace0\uc131\ub2a5 AI\ub97c \uc800\ub834\ud55c \ube44\uc6a9\uc73c\ub85c \uc81c\uacf5\ud558\uace0\uc790 \ud588\uc2b5\ub2c8\ub2e4. OpenAI \ub300\ube44 \"50\ubc30 \uc800\ub834\ud55c \uc774\uc6a9\ub8cc\"\ub97c \ubaa9\ud45c\ub85c \ud588\ub2e4\ub124\uc694, \uc2e4\uc81c\ub85c \uc5c4\uccad \uc2f8\uac8c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.  </li> <li>\uae30\uc220\uc801 \ud601\uc2e0: MoE \uad6c\uc870, \ub2e4\uc911 \ud1a0\ud070 \uc608\uce21(MTP), FP8 (\ubcf4\ud1b5\uc740 BF16\uc744 \uac00\uc7a5 \ub9ce\uc774 \uc501\ub2c8\ub2e4) \uc5f0\uc0b0, Multi-Head Latent Attention(MLA) \ub4f1 \uc5ec\ub7ec \ucd5c\uc2e0 \uae30\ubc95\uc744 \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ubc18\ucbe4 \uc624\ud508\uc18c\uc2a4: \ub370\uc774\ud130\ub294 \uacf5\uac1c\ud558\uc9c0 \uc54a\uc558\uc9c0\ub9cc, \uc5b4\ub5bb\uac8c \ub9cc\ub4e4\uc5c8\ub294\uc9c0\uc5d0 \ub300\ud55c \uaf64 \uae30\uc220 \ubcf4\uace0\uc11c, \uadf8\ub9ac\uace0 \uc5c4\uccad\ub098\uac8c \ub9ce\uc740 \ub3c4\uad6c\ub4e4\uc744 \uc624\ud508\uc18c\uc2a4\ub85c \uacf5\uac1c\ud558\uc5ec \ub204\uad6c\ub098 \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac DeepSeek\uc774 \ub300\uc911\uc801\uc73c\ub85c \uc774\uc288\ub97c \ub9cc\ub4e4\uac8c \ub41c \uc6d0\uc778\uc740 \ud06c\uac8c 2\uac00\uc9c0 \uc785\ub2c8\ub2e4,  \uccab\ubc88\uc9f8\ub294 V3 \ub77c\ub294 671B\uc758 \ud06c\uace0 \uc88b\uc740 \ubaa8\ub378\uc740 (\ubcf8\uc778\ub4e4 \uc8fc\uc7a5) 100\uc5b5\uc5d0 \uc815\ub3c4\uc758 \uc2fc \ube44\uc6a9\uc73c\ub85c \ub2ec\uc131\ud588\uace0, \ub450\ubc88\uc9f8\ub294 R1 \uc774\ub77c\ub294 Reasoning \ubaa8\ub378 (OpenAI o1 \uac19\uc740) \uc744 \ub9cc\ub4dc\ub294 \ube44\ubc95 \uc18c\uc2a4\ub97c \uc0c1\uc138\ud558\uac8c \uacf5\uac1c\ud55c \uac83\uc774\uc8e0. \uac11\ub860\uc744\ubc15\uc774 \uc788\uc2b5\ub2c8\ub2e4\ub9cc, \ub2e4\uc18c \uacfc\uc815\uc774 \uc788\ub354\ub77c\ub3c4 2\uac00\uc9c0 \uc694\uc18c \ub2e4 contribution\uc774 \ub9e4\uc6b0 \ud06c\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. <p>(TODO) \ubcf4\ub2e4 \uc0c1\uc138\ud788 \uc801\uc744 \uc608\uc815.</p>"},{"location":"case_studies/deepseek/#_1","title":"\ubaa8\ub378 \uad6c\uc870 \ubc0f \uae30\uc220\uc801 \ud2b9\uc9d5","text":""},{"location":"case_studies/deepseek/#1","title":"1. \uae30\ubcf8 \uc544\ud0a4\ud14d\ucc98","text":"<ul> <li>Transformer \uae30\ubc18: 61\uce35\uc758 Transformer \uad6c\uc870\ub97c \ucc44\ud0dd\ud588\uc2b5\ub2c8\ub2e4.</li> <li>MoE \uad6c\uc870: \ucd1d 6,710\uc5b5 \uac1c(671B)\uc758 \ud30c\ub77c\ubbf8\ud130 \uc911 \ud1a0\ud070\ub2f9 \uc57d 370\uc5b5 \uac1c(37B)\ub9cc \ud65c\uc131\ud654\ub429\ub2c8\ub2e4.</li> <li>\uc804\ubb38\uac00 \ub124\ud2b8\uc6cc\ud06c: \uac01 MoE \uce35\uc5d0 256\uac1c\uc758 \uc804\ubb38\uac00 \uc911 8\uac1c\ub9cc \uc120\ud0dd\uc801\uc73c\ub85c \ud65c\uc131\ud654\ub429\ub2c8\ub2e4.</li> <li>\ub0b4\ubd80 \uad6c\uc131: \uc228\uae40 \ud06c\uae30(dim) 7168, \uc8fc\uc758 \ud5e4\ub4dc(head) \uc218 128\uac1c\ub85c \uad6c\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2","title":"2. \uae30\uc220\uc801 \ud601\uc2e0","text":"<ul> <li>\ubd80\ud558 \uade0\ud615 \uc804\ub7b5: \uae30\uc874 MoE \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub358 \ubcf4\uc870\uc190\uc2e4 \uc5c6\uc774\ub3c4 \uc804\ubb38\uac00 \ubd80\ud558\ub97c \uade0\ub4f1\ud558\uac8c \ubd84\uc0b0\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.</li> <li>\ub2e4\uc911 \ud1a0\ud070 \uc608\uce21(MTP): \ud55c \ubc88\uc5d0 \uc5ec\ub7ec \ud1a0\ud070\uc744 \uc608\uce21\ud558\ub3c4\ub85d \ud559\uc2b5\ud558\uc5ec \uc0dd\uc131 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.</li> <li>FP8 \uc800\uc815\ubc00\ub3c4 \uc5f0\uc0b0: 8\ube44\ud2b8 \ubd80\ub3d9\uc18c\uc218\uc810 \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\ud558\uc5ec \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc774\uace0 \uc5f0\uc0b0\uc18d\ub3c4\ub97c \ub192\uc600\uc2b5\ub2c8\ub2e4.</li> <li>\ubd84\uc0b0 \ubcd1\ub82c \ud559\uc2b5 \ucd5c\uc801\ud654: \ub178\ub4dc \uac04 \ud1b5\uc2e0\uacfc \uacc4\uc0b0\uc744 \uac70\uc758 \uc644\uc804\ud788 \uc911\ucca9\uc2dc\ucf1c \ud559\uc2b5 \ud6a8\uc728\uc744 \uadf9\ub300\ud654\ud588\uc2b5\ub2c8\ub2e4.</li> <li>Multi-Head Latent Attention(MLA): \uc790\uccb4 \ucd5c\uc801\ud654\ub41c \uc5b4\ud150\uc158 \uae30\ubc95\uc73c\ub85c \uba54\ubaa8\ub9ac-\ud1b5\uc2e0 \ud6a8\uc728\uc744 \uc720\uc9c0\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#3-v3-r1","title":"3. V3\uc640 R1\uc758 \ucc28\uc774\uc810","text":"<ul> <li>V3: \uc77c\ubc18\uc801 \ub300\ud654 \ubc0f \ub2e4\ubaa9\uc801 \ud65c\uc6a9\uc5d0 \ucd5c\uc801\ud654\ub41c \uae30\ubcf8 \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>R1(DeepThink): V3\ub97c \uae30\ubc18\uc73c\ub85c \uac15\ud654\ud559\uc2b5\uc744 \ud1b5\ud574 \uace0\ub09c\ub3c4 \ucd94\ub860\uc5d0 \ud2b9\ud654\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ucd94\ub860 \ubc29\uc2dd: R1\uc740 \ub0b4\ubd80\uc801\uc73c\ub85c \uc790\uc2e0\ub9cc\uc758 \ucd94\ub860 \uacfc\uc815\uc744 \ud14d\uc2a4\ud2b8 \ud615\ud0dc\ub85c \uc804\uac1c\ud558\ub294 chain-of-thought \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4.</li> <li>\uc751\ub2f5 \uc18d\ub3c4: V3\ub294 \uc57d 60 \ud1a0\ud070/\ucd08\ub85c \ube60\ub978 \ubc18\uba74, R1\uc740 \ub354 \uae4a\uc740 \ucd94\ub860 \uacfc\uc815\uc73c\ub85c \uc778\ud574 \uc0c1\ub300\uc801\uc73c\ub85c \ub290\ub9bd\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#_2","title":"\ud559\uc2b5 \ub370\uc774\ud130 \ubc0f \ud6c8\ub828 \ubc29\uc2dd","text":""},{"location":"case_studies/deepseek/#1-v3","title":"1. V3\uc758 \ud559\uc2b5 \ub370\uc774\ud130","text":"<ul> <li>\uaddc\ubaa8: \uc57d 14.8\uc870(Trillion) \ud1a0\ud070 \uaddc\ubaa8\uc758 \ubc29\ub300\ud55c \ud14d\uc2a4\ud2b8\ub85c \uc0ac\uc804\ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\uad6c\uc131: \uc601\uc5b4\uc640 \uc911\uad6d\uc5b4\ub97c \uc911\uc2ec\uc73c\ub85c \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c \ud3ec\ud568\ud558\uba70, \uc218\ud559 \ubc0f \ud504\ub85c\uadf8\ub798\ubc0d \uc0d8\ud50c \ube44\uc911\uc744 \ub298\ub838\uc2b5\ub2c8\ub2e4.</li> <li>\ub2e4\uc591\uc131: \ub274\uc2a4, \ubc31\uacfc\uc0ac\uc804, \uc6f9 \ud06c\ub864\ub9c1, \ubb38\ud559, \ucf54\ub4dc, \uacfc\ud559 \ub17c\ubb38 \ub4f1 \ub2e4\uc591\ud55c \ub3c4\uba54\uc778\uc758 \uace0\ud488\uc9c8 \ud14d\uc2a4\ud2b8\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.</li> <li>\uc804\ucc98\ub9ac: \uc911\ubcf5 \uc81c\uac70 \ubc0f \uc815\uc81c\ub97c \ucca0\uc800\ud788 \ud588\uc73c\uba70, \ubb38\uc11c \ub2e8\uc704\ub85c \uc2dc\ud000\uc2a4\ub97c \ud328\ud0b9\ud558\uc5ec \uc77c\uad00\uc131 \uc788\ub294 \ud559\uc2b5 \uc0d8\ud50c\uc744 \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2-v3","title":"2. V3\uc758 \uc0ac\uc804\ud559\uc2b5 \ubc0f \ubbf8\uc138\uc870\uc815","text":"<ul> <li>\ud559\uc2b5 \ud658\uacbd: \ub300\uaddc\ubaa8 \ubcd1\ub82c GPU \ud074\ub7ec\uc2a4\ud130(\uc57d 2,000\uac1c\uc758 H800 GPU)\uc5d0\uc11c \uc218\ubc31\uc5b5 \ub2e8\uacc4\ub85c \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ud559\uc2b5 \ube44\uc6a9: \uc57d 278\ub9cc GPU-\uc2dc\uac04(\uc57d $560\ub9cc \ub2ec\ub7ec) \uc218\uc900\uc73c\ub85c \ucd94\uc0b0\ub429\ub2c8\ub2e4.</li> <li>\ucee8\ud14d\uc2a4\ud2b8 \ud655\uc7a5: 4K \ud1a0\ud070 \uc2dc\ud000\uc2a4 \uae38\uc774\ub85c \ud559\uc2b5\ud55c \ub4a4 \ub2e8\uacc4\uc801\uc73c\ub85c 128K\uae4c\uc9c0 \ud655\uc7a5\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ubbf8\uc138\uc870\uc815: \uc778\uac04 \ud53c\ub4dc\ubc31\uc744 \ud65c\uc6a9\ud55c \uc9c0\ub3c4 \ubbf8\uc138\uc870\uc815(SFT)\uacfc \uac15\ud654\ud559\uc2b5 \ub2e8\uacc4\ub97c \ud3ec\ud568\ud55c \ucd5c\uc801\ud654\ub97c \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc9c0\uc2dd \uc99d\ub958: R1 \ubaa8\ub378\uc758 \ucd94\ub860 \ud328\ud134\uc744 \ud65c\uc6a9\ud574 V3\ub97c \ud55c\ubc88 \ub354 \uac15\ud654\ud558\ub294 \ub3c5\ud2b9\ud55c \uc811\uadfc\uc744 \ucde8\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#3-r1","title":"3. R1\uc758 \ud6c8\ub828 \uacfc\uc815","text":"<ul> <li>\uae30\ubc18 \ubaa8\ub378: V3-Base(\uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378)\uc744 \ud1a0\ub300\ub85c \uace0\ub09c\ub3c4 \ubb38\uc81c\ud574\uacb0 \ub2a5\ub825\uc744 \ubc30\uc591\ud558\ub294 \ubcc4\ub3c4 \ud6c8\ub828\uc744 \uac70\ucce4\uc2b5\ub2c8\ub2e4.</li> <li>R1-Zero: \uc804\uc801\uc73c\ub85c \uac15\ud654\ud559\uc2b5(RL)\ub9cc\uc73c\ub85c \ud559\uc2b5\uc2dc\ud0a8 \uc2e4\ud5d8\uc801 \ubaa8\ub378\uc744 \uba3c\uc800 \uac1c\ubc1c\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc790\uc728 \ud559\uc2b5: \uc778\uac04\uc774 \ub9cc\ub4e0 \uc815\ub2f5 \uc608\uc2dc \uc5c6\uc774, \ubaa8\ub378\uc774 \uc8fc\uc5b4\uc9c4 \ubb38\uc81c\uc5d0 \ub300\ud574 \uc5ec\ub7ec \uc2dc\ub3c4\ub97c \uc0dd\uc131\ud558\uace0 \uaddc\uce59 \uae30\ubc18 \ubcf4\uc0c1\uc744 \ud1b5\ud574 \uac1c\uc120\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ubcf4\uc644 \uc870\uce58: \uc18c\ub7c9\uc758 \uace0\ud488\uc9c8 \ub370\uc774\ud130\ub85c \uc9c0\ub3c4 \ubbf8\uc138\uc870\uc815(SFT)\uc744 \uc9c4\ud589\ud558\uc5ec \"\ucf5c\ub4dc \uc2a4\ud0c0\ud2b8\" \ub2e8\uacc4\ub97c \uac70\ucce4\uc2b5\ub2c8\ub2e4.</li> <li>\ud63c\ud569 \uac15\ud654\ud559\uc2b5: \ub2e8\uc21c\ud55c \uaddc\uce59 \ubcf4\uc0c1\ubfd0 \uc544\ub2c8\ub77c \ubaa8\ub378 \ucd9c\ub825\uc5d0 \ub300\ud55c \ud3c9\uac00\uad00\uc810 \ubcf4\uc0c1\ub3c4 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>GRPO \uae30\ubc95: DeepSeek \uace0\uc720\uc758 Group Relative Policy Optimization\uc774\ub77c\ub294 \ub300\uaddc\ubaa8 RL \ucd5c\uc801\ud654 \uae30\ubc95\uc744 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc9c0\uc2dd \uc99d\ub958: R1\uc758 \ub17c\ub9ac\ub825\uc744 \uc555\ucd95\ud574 \ub2f4\uc740 \uc18c\ud615 \ubaa8\ub378\ub4e4(R1-Lite)\ub3c4 \uc5ec\ub7ec \ubc84\uc804 \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#_3","title":"\uc131\ub2a5","text":""},{"location":"case_studies/deepseek/#1_1","title":"1. \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc","text":"<ul> <li>MMLU: V3\ub294 \uc57d 88~89%, R1\uc740 89.8~90.8%\uc758 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec GPT-4 \uacc4\uc5f4\uacfc \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ub0c8\uc2b5\ub2c8\ub2e4.</li> <li>MATH-500: V3\uac00 90% \ub0b4\uc678, R1\uc740 92% \uc774\uc0c1\uc758 \uc815\ud655\ub3c4\ub97c \uae30\ub85d\ud574 \ucd5c\ucca8\ub2e8 \uc218\uc900\uc785\ub2c8\ub2e4.</li> <li>AIME 2024: \uace0\ub4f1 \uc218\ud559 \uacbd\uc2dc \ub300\ud68c \ubb38\uc81c\uc5d0\uc11c R1\uc774 79.8% \uc815\ub2f5\ub960\uc744 \ub2ec\uc131\ud558\uc5ec, V3(\uc57d 39%)\ubcf4\ub2e4 \ud06c\uac8c \ud5a5\uc0c1\ub41c \ubb38\uc81c\ud574\uacb0\ub825\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.</li> <li>HumanEval: V3\ub294 \ucf54\ub529 \ub2a5\ub825 \ud3c9\uac00\uc5d0\uc11c 82.6%\uc758 \uc815\ub2f5\ub960(Pass@1)\uc744 \uae30\ub85d\ud574 OpenAI GPT-4\uc5d0 \ud544\uc801\ud558\uac70\ub098 \uc55e\uc130\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2_1","title":"2. \uc18d\ub3c4 \ubc0f \ud6a8\uc728\uc131","text":"<ul> <li>\ucd94\ub860 \uc18d\ub3c4: V3\ub294 \uc57d 60 \ud1a0\ud070/\ucd08\ub85c \uc774\uc804 \uc138\ub300(V2)\ubcf4\ub2e4 3\ubc30 \uc774\uc0c1 \ube60\ub985\ub2c8\ub2e4.</li> <li>\ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774: \ucd5c\ub300 128K \ud1a0\ud070\uae4c\uc9c0 \uae34 \ubb38\ub9e5\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4(API\uc5d0\uc11c\ub294 64K\ub85c \uc81c\ud55c).</li> <li>R1\uc758 \ud2b9\uc131: \ubcf5\uc7a1\ud55c \ubb38\uc81c\uc5d0\uc11c R1\uc740 \ub2e8\uacc4\ubcc4 \ucd94\ub860\uc744 \uc218\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 \uc751\ub2f5 \uc0dd\uc131\uc774 \ub2e4\uc18c \ub290\ub9bd\ub2c8\ub2e4.</li> <li>\ud6a8\uc728\uc131: MoE \uad6c\uc870 \ub355\ubd84\uc5d0 \ud544\uc694\ud55c \uc77c\ubd80 \ud30c\ub77c\ubbf8\ud130\ub9cc \ud65c\uc131\ud654\ud558\uc5ec \uc751\ub2f5 \uc9c0\uc5f0\uc774 \uc801\uace0 \ud6a8\uc728\uc801\uc785\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#_4","title":"\ud65c\uc6a9 \uc0ac\ub840","text":""},{"location":"case_studies/deepseek/#1_2","title":"1. \ub300\ud45c \uc0ac\uc6a9 \ubd84\uc57c","text":"<ul> <li>V3: ChatGPT\uc640 \uc720\uc0ac\ud55c AI \ube44\uc11c\ub85c\uc11c \uc77c\uc0c1 \uc5b8\uc5b4 \ub300\ud654, \uae00\uc4f0\uae30 \ubcf4\uc870, \uc815\ubcf4 \uac80\uc0c9, \ubc88\uc5ed \ub4f1\uc5d0 \ub450\ub8e8 \ud65c\uc6a9\ub429\ub2c8\ub2e4.</li> <li>R1: \uace0\ub09c\ub3c4 \ubb38\uc81c \ud574\uacb0 \ubc0f \uc804\ubb38 \ubd84\uc57c\uc5d0 \uac15\uc810\uc744 \ubcf4\uc774\uba70, \uc218\ud559 \ubb38\uc81c \ud480\uc774, \uc54c\uace0\ub9ac\uc998 \uc124\uacc4, \ub17c\ub9ac \ud37c\uc990, \ubcf5\uc7a1\ud55c \uc9c8\uc758 \uc751\ub2f5 \ub4f1\uc5d0 \uc801\ud569\ud569\ub2c8\ub2e4.</li> <li>\ub2e4\uad6d\uc5b4 \uc9c0\uc6d0: V3\ub294 \ub2e4\uc911\uc5b8\uc5b4 \uc9c0\uc6d0\uc774 \uc6b0\uc218\ud558\uc5ec \uad6d\uc81c \uc11c\ube44\uc2a4\uc5d0 \uc801\ud569\ud558\uace0, R1\uc740 \ub2e8\uc77c\uc5b8\uc5b4 \uace0\ub09c\ub3c4 \ubd84\uc11d\uc5d0 \uc801\ud569\ud569\ub2c8\ub2e4.</li> <li>\uae34 \ub9e5\ub77d \ucc98\ub9ac: \ub450 \ubaa8\ub378 \ubaa8\ub450 \ucd5c\ub300 64K \ud1a0\ud070 \uc774\uc0c1\uc758 \uae34 \ub9e5\ub77d\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc5b4, \uc7a5\ubb38 \ubb38\uc11c \uc694\uc57d\uc774\ub098 \uae34 \ub300\ud654 \uc720\uc9c0\uc5d0 \ud65c\uc6a9\ub429\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2_2","title":"2. \ub3c4\uc785 \ud604\ud669","text":"<ul> <li>\uc790\uccb4 \uc11c\ube44\uc2a4: \ub525\uc2dc\ud06c \uc790\uccb4 \uc571\uacfc \uc6f9(chat.deepseek.com)\uc744 \ud1b5\ud574 \uc77c\ubc18 \uc0ac\uc6a9\uc790\uac00 V3\uc640 R1\uc744 \uc9c1\uc811 \uccb4\ud5d8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>API \uc81c\uacf5: \uae30\uc5c5\uacfc \uac1c\ubc1c\uc790\ub97c \uc704\ud574 API \ud615\ud0dc\ub85c V3/R1 \uc811\uadfc\uc744 \uc81c\uacf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud074\ub77c\uc6b0\ub4dc \ud1b5\ud569: \uad6c\uae00 GCP\uc758 Vertex AI, AWS\uc758 Bedrock, MS Azure AI \ub4f1\uc758 \uc11c\ube44\uc2a4\uc5d0 DeepSeek \ubaa8\ub378\uc774 \ub4f1\ub85d\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc5f0\uad6c \ud65c\uc6a9: Hugging Face\uc758 Open-R1 \ud504\ub85c\uc81d\ud2b8\ucc98\ub7fc DeepSeek\uc758 \uacf5\uac1c \uac00\uc911\uce58\ub97c \ud65c\uc6a9\ud55c \uc7ac\ud604 \uc2e4\ud5d8\uc774 \uc9c4\ud589 \uc911\uc785\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#_5","title":"\uac00\uaca9 \ubc0f \uc811\uadfc\uc131","text":""},{"location":"case_studies/deepseek/#1_3","title":"1. \uc774\uc6a9 \ube44\uc6a9","text":"<ul> <li>\ubb34\ub8cc \uc635\uc158: \uac1c\uc778 \uc0ac\uc6a9\uc790\ub294 \ub525\uc2dc\ud06c \uacf5\uc2dd \uc6f9\uc774\ub098 \ubaa8\ubc14\uc77c \uc571\uc5d0\uc11c DeepSeek-V3\ub97c \ubb34\ub8cc\ub85c \uc774\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>API \uc694\uae08: V3\ub294 \uc785\ub825 100\ub9cc \ud1a0\ud070\ub2f9 $0.27, \ucd9c\ub825 100\ub9cc \ud1a0\ud070\ub2f9 $1.10\uc758 \uae30\ubcf8\uc694\uae08\uc785\ub2c8\ub2e4.</li> <li>R1 \uc694\uae08: \uc785\ub825 $0.55/\ubc31\ub9cc, \ucd9c\ub825 $2.19/\ubc31\ub9cc \ud1a0\ud070\uc73c\ub85c V3\ubcf4\ub2e4 \uc57d 2\ubc30 \uc218\uc900\uc785\ub2c8\ub2e4.</li> <li>\ud560\uc778 \uc815\ucc45: \ube44\ud63c\uc7a1 \uc2dc\uac04\ub300\uc5d0\ub294 V3 \ube44\uc6a9 50%, R1 \ube44\uc6a9 75% \ud560\uc778\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> <li>\ube44\uc6a9 \ud6a8\uc728\uc131: OpenAI \ub300\ube44 \"30~50\ubc30 \uc800\ub834\ud55c \uc774\uc6a9\ub8cc\"\ub97c \uac15\uc810\uc73c\ub85c \ub0b4\uc138\uc6b0\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2_3","title":"2. \uc811\uadfc\uc131","text":"<ul> <li>\uc624\ud508\uc18c\uc2a4: \uc644\uc804 \uc624\ud508\uc18c\uc2a4\ub85c \ucd9c\uc2dc\ub418\uc5b4, \ub204\uad6c\ub098 \ubaa8\ub378 \uac00\uc911\uce58\uc640 \uae30\uc220 \ubcf4\uace0\uc11c\ub97c \uc5f4\ub78c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc790\uccb4 \ubc30\ud3ec: \ucda9\ubd84\ud55c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc774 \uc788\ub294 \uc870\uc9c1\uc774\ub77c\uba74 \uc9c1\uc811 \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc544 \uc790\uccb4 \uc778\ud504\ub77c\uc5d0\uc11c \ubc30\ud3ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud074\ub77c\uc6b0\ub4dc API: \uc77c\ubc18 \uc0ac\uc6a9\uc790\ub098 \uc18c\uaddc\ubaa8 \uac1c\ubc1c\ud300\uc740 DeepSeek\uc758 \ud074\ub77c\uc6b0\ub4dc API\ub97c \ud1b5\ud574 \uc190\uc27d\uac8c \ubaa8\ub378\uc744 \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uacbd\ub7c9 \ubc84\uc804: R1-Lite \ub4f1 \uacbd\ub7c9\ud654\ub41c \ubc84\uc804\uc740 \uac1c\ubcc4 \uc11c\ubc84\ub098 PC\uc5d0\uc11c\ub3c4 \uc2e4\ud589 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#_6","title":"\uc758\uc758","text":""},{"location":"case_studies/deepseek/#1_4","title":"1. \uae30\uc220\uc801 \ud601\uc2e0","text":"<ul> <li>MoE \uad6c\uc870\ub97c \ub300\uaddc\ubaa8\ub85c \uc131\uacf5\uc801\uc73c\ub85c \uc801\uc6a9\ud558\uc5ec \ud6a8\uc728\uc801\uc778 \ucd08\uac70\ub300 \ubaa8\ub378\uc744 \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ubcf5\uc7a1\ud55c \uac15\ud654\ud559\uc2b5\uc744 \ud1b5\ud574 \ubaa8\ub378\uc774 \uc2a4\uc2a4\ub85c \ucd94\ub860 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub860\uc744 \uac1c\ubc1c\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc624\ud508\uc18c\uc2a4 \uacf5\uac1c\ub97c \ud1b5\ud574 AI \uc5f0\uad6c \ucee4\ubba4\ub2c8\ud2f0\uc5d0 \uae30\uc5ec\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/deepseek/#2_4","title":"2. \uc0b0\uc5c5\uc801 \uc601\ud5a5","text":"<ul> <li>\uace0\uc131\ub2a5 AI\ub97c \uc800\ub834\ud55c \ube44\uc6a9\uc73c\ub85c \uc81c\uacf5\ud558\uc5ec AI \ub3c4\uc785 \uc7a5\ubcbd\uc744 \ub0ae\ucdc4\uc2b5\ub2c8\ub2e4.</li> <li>\uc911\uad6d \uae30\uc5c5\uc774 \uae00\ub85c\ubc8c AI \uc2dc\uc7a5\uc5d0\uc11c \uae30\uc220\uc801 \uacbd\uc7c1\ub825\uc744 \uc785\uc99d\ud55c \uc0ac\ub840\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ub2e4\uc591\ud55c \uc0b0\uc5c5 \ubd84\uc57c\uc5d0\uc11c \ud65c\uc6a9 \uac00\ub2a5\ud55c \ubc94\uc6a9 \ubc0f \ud2b9\ud654 \ubaa8\ub378\uc744 \uc81c\uacf5\ud588\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ucc38\uace0 \uc790\ub8cc: - DeepSeek \uacf5\uc2dd \ud648\ud398\uc774\uc9c0 \ubc0f API \ubb38\uc11c (https://www.deepseek.com/) - DeepSeek-V3 Technical Report (https://arxiv.org/html/2412.19437v1) - HuggingFace DeepSeek-V3 \ubaa8\ub378 \uce74\ub4dc (https://huggingface.co/deepseek-ai/DeepSeek-V3) - Open-R1: a fully open reproduction of DeepSeek-R1 (https://huggingface.co/blog/open-r1) </p> <p>[\ucc38\uace0, \uc791\uc131 \uc911] - https://x.com/markchen90/status/1884303237186216272 - https://www.youtube.com/watch?v=Z-ELkZ_azYM</p>"},{"location":"case_studies/solar/","title":"Solar","text":"<p>TBD</p>"},{"location":"case_studies/zephyr/","title":"Zephyr - HuggingFace\uc758 DPO \uc801\uc6a9 \uc0ac\ub840","text":""},{"location":"case_studies/zephyr/#huggingface-zephyr-202310","title":"HuggingFace Zephyr \uc2dc\ub9ac\uc988 - 2023.10","text":"<p>Zephyr\ub294 HuggingFace\uc758 H4 \ud300\uc774 \uac1c\ubc1c\ud55c \uc624\ud508\uc18c\uc2a4 LLM \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4. Mistral-7B\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uba70, \uae30\uc874 RLHF(Reinforcement Learning from Human Feedback) \ub300\uc2e0 DPO(Direct Preference Optimization)\ub77c\ub294 \ud6a8\uc728\uc801\uc778 alignment \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4. \uc801\uc740 \ucef4\ud4e8\ud305 \uc790\uc6d0\uc73c\ub85c\ub3c4 \uace0\ud488\uc9c8 \ub300\ud654 \ubaa8\ub378\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc74c\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\uac1c\ubc1c \ubc30\uacbd: \ubcf5\uc7a1\ud55c RLHF \uacfc\uc815 \uc5c6\uc774\ub3c4 \ud6a8\uacfc\uc801\uc73c\ub85c LLM\uc744 alignment \ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \uc5f0\uad6c\ud558\uace0\uc790 \ud588\uc2b5\ub2c8\ub2e4, RLHF (\ub610\ub294 PPO) \ub294 \ub108\ubb34 \uc5b4\ub835\uac70\ub4e0\uc694...</li> <li>\ub370\uc774\ud130\uc5d0\uc11c \uc778\uac04\uc758 \uac1c\uc785 \uc81c\uac70!: Human \ud53c\ub4dc\ubc31 \uc5c6\uc774 AI \uc0dd\uc131 \ub370\uc774\ud130\ub9cc\uc73c\ub85c alignment\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> <li>\uc9c4\uc815\ud55c \uc624\ud508\uc18c\uc2a4: \uc7ac\ud604 \ubb38\uc11c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4! Alignment Handbook</li> </ul>"},{"location":"case_studies/zephyr/#_1","title":"\ub370\uc774\ud130 \ubc0f \ubc29\ubc95\ub860","text":"<p>Zephyr 7B alpha &amp; beta</p>"},{"location":"case_studies/zephyr/#1-1-sft","title":"1-1. \uae30\ubc18 \ubaa8\ub378 \ubc0f SFT \uacfc\uc815","text":"<ul> <li>\uae30\ubc18 \ubaa8\ub378: Mistral-7B-v0.1\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc9c0\ub3c4 \ud559\uc2b5 \ubbf8\uc138\uc870\uc815(SFT): UltraChat \ub370\uc774\ud130\uc14b(ChatGPT\ub85c \uc0dd\uc131\ub41c 147\ub9cc \uac1c\uc758 \ub300\ud654)\uc744 \uc815\uc81c\ud558\uc5ec \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \uc815\uc81c \uacfc\uc815: <ul> <li>\uc6d0\ubcf8 \ub370\uc774\ud130\uc5d0\uc11c \ubc1c\uacac\ub41c \ub300\ubb38\uc790 \uc624\ub958 \uc218\uc815(truecasing)</li> <li>\uacfc\ub3c4\ud55c \uac70\ubd80 \uc751\ub2f5\uc774\ub098 \uba74\ucc45 \ubb38\uad6c \uc81c\uac70</li> <li>\uc57d 20\ub9cc \uac1c\uc758 \uace0\ud488\uc9c8 \uc608\uc81c\ub97c \uc120\ubcc4\ud558\uc5ec \ucd5c\uc885 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9</li> </ul> </li> </ul>"},{"location":"case_studies/zephyr/#1-2-dpo-alignment","title":"1-2. DPO alignment \uacfc\uc815","text":"<ul> <li>\uc120\ud638\ub3c4 \ub370\uc774\ud130: UltraFeedback \ub370\uc774\ud130\uc14b(6.4\ub9cc \uac1c\uc758 \ud504\ub86c\ud504\ud2b8\uc640 \uac01\uac01 4\uac1c\uc758 \uc751\ub2f5)\uc744 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>AI \ud53c\ub4dc\ubc31: GPT-4\uac00 \uac01 \uc751\ub2f5\uc744 \uc720\uc6a9\uc131, \uc815\uc9c1\uc131 \ub4f1 \uc5ec\ub7ec \uae30\uc900\uc73c\ub85c \ud3c9\uac00\ud55c \uc810\uc218\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc120\ud638\ub3c4 \uc30d \uad6c\uc131: \uac01 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 \uac00\uc7a5 \ub192\uc740 \uc810\uc218\uc758 \uc751\ub2f5\uc744 \"\uc120\ud638\" \uc751\ub2f5\uc73c\ub85c, \ub098\uba38\uc9c0 \uc911 \ud558\ub098\ub97c \ubb34\uc791\uc704\ub85c \"\ube44\uc120\ud638\" \uc751\ub2f5\uc73c\ub85c \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4.</li> <li>DPO \ud6c8\ub828: <ul> <li>SFT \ubaa8\ub378\uc744 \ucc38\uc870 \ubaa8\ub378\ub85c \uc0ac\uc6a9</li> <li>\uc120\ud638 \uc751\ub2f5\uc758 \ud655\ub960\uc744 \ub192\uc774\uace0 \ube44\uc120\ud638 \uc751\ub2f5\uc758 \ud655\ub960\uc744 \ub0ae\ucd94\ub294 \ubc29\ud5a5\uc73c\ub85c \ubaa8\ub378 \uac00\uc911\uce58 \uc870\uc815</li> <li>HuggingFace\uc758 TRL \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640 DPOTrainer \ud65c\uc6a9</li> </ul> </li> </ul> <p>Zephyr ORPO</p>"},{"location":"case_studies/zephyr/#2-orpoodds-ratio-preference-optimization","title":"2. ORPO(Odds Ratio Preference Optimization)","text":"<ul> <li>Zephyr \uc2dc\ub9ac\uc988\uc758 \ucd5c\uc2e0 \ubaa8\ub378\uc778 Zephyr-141B\uc5d0 \uc801\uc6a9\ub41c \uc0c8\ub85c\uc6b4 alignment \uc54c\uace0\ub9ac\uc998\uc785\ub2c8\ub2e4.</li> <li>DPO\uc640 \ub2ec\ub9ac \ucc38\uc870 \ubaa8\ub378\uc774 \ud544\uc694 \uc5c6\uc5b4 SFT \ub2e8\uacc4\ub97c \uac74\ub108\ub6f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ub2e8 7\ucc9c \uac1c\uc758 \uc120\ud638\ub3c4 \uc608\uc81c\ub9cc\uc73c\ub85c 1.3\uc2dc\uac04 \ub0b4\uc5d0 \ub300\uaddc\ubaa8 \ubaa8\ub378(141B) alignment\uc774 \uac00\ub2a5\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#_2","title":"\uc131\ub2a5 \ubc0f \uc601\ud5a5","text":""},{"location":"case_studies/zephyr/#_3","title":"\uc8fc\uc694 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc","text":"<ul> <li>Zephyr-7B beta: MT-Bench\uc5d0\uc11c 7.34\uc810\uc744 \uae30\ub85d\ud558\uba70 \ub2f9\uc2dc (2023) 7B \ubaa8\ub378 \uc911 \ucd5c\uace0 \uc131\ub2a5\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.</li> <li>Zephyr-141B ORPO: MT-Bench\uc5d0\uc11c 8.17\uc810\uc744 \uae30\ub85d\ud558\uba70 \ucd5c\uace0 \uc218\uc900\uc758 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\ub4e4\uacfc \uacbd\uc7c1\ub825\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#dpo","title":"DPO\uc758 \uc7a5\uc810","text":"<ul> <li>\ub2e8\uc21c\uc131: \ubcf5\uc7a1\ud55c RL \ub8e8\ud504 \uc5c6\uc774 \uc815\uc801 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc9c1\uc811 \uc120\ud638\ub3c4\ub97c \ud559\uc2b5\ud569\ub2c8\ub2e4.</li> <li>\ud6a8\uc728\uc131: \ubcf4\uc0c1 \ubaa8\ub378 \ud6c8\ub828\uc774\ub098 \uc628\ub77c\uc778 \uc0d8\ud50c\ub9c1\uc774 \ud544\uc694 \uc5c6\uc5b4 \ud6c8\ub828 \uc2dc\uac04\uacfc \ube44\uc6a9\uc774 \ud06c\uac8c \uc808\uac10\ub429\ub2c8\ub2e4.</li> <li>\uc548\uc815\uc131: RL \ud6c8\ub828\uc5d0\uc11c \ud754\ud788 \ubc1c\uc0dd\ud558\ub294 \ubd88\uc548\uc815\uc131 \ubb38\uc81c\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.</li> <li>AI \ud53c\ub4dc\ubc31 \ud65c\uc6a9: Human Labeling \uc5c6\uc774 GPT-4 \uac19\uc740 \uac15\ub825\ud55c \ubaa8\ub378\uc758 \ud3c9\uac00\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#_4","title":"\ub77c\uc774\uc120\uc2a4 \ubc0f \uc811\uadfc\uc131","text":"<ul> <li>\uc624\ud508\uc18c\uc2a4 \uacf5\uac1c: Zephyr-7B \ubaa8\ub378\ub4e4\uc740 MIT \ub77c\uc774\uc120\uc2a4\ub85c, Zephyr-141B\ub294 Apache 2.0 \ub77c\uc774\uc120\uc2a4\ub85c \uacf5\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\uc0c1\uc5c5\uc801 \ud65c\uc6a9: \ub450 \ub77c\uc774\uc120\uc2a4 \ubaa8\ub450 \uc0c1\uc5c5\uc801 \uc0ac\uc6a9\uacfc \uc5f0\uad6c\ub97c \ucd5c\uc18c\ud55c\uc758 \uc81c\uc57d\uc73c\ub85c \ud5c8\uc6a9\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#_5","title":"\uc81c\ud55c \uc0ac\ud56d","text":"<ul> <li>\uc548\uc804\uc131 alignment \ubd80\uc7ac: Zephyr \ubaa8\ub378\ub4e4\uc740 RLHF \uc548\uc804\uc131 \ud6c8\ub828\uc744 \uac70\uce58\uc9c0 \uc54a\uc544 \uc720\ud574\ud55c \ucf58\ud150\uce20\ub97c \uc0dd\uc131\ud560 \uac00\ub2a5\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud2b9\uc815 \ub3c4\uba54\uc778 \ud55c\uacc4: \ucf54\ub529\uc774\ub098 \uc218\ud559 \uac19\uc740 \ubcf5\uc7a1\ud55c \uc601\uc5ed\uc5d0\uc11c\ub294 \ub300\ud615 \ub3c5\uc810 \ubaa8\ub378\uc5d0 \ube44\ud574 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9d1\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#_6","title":"\uc758\uc758","text":""},{"location":"case_studies/zephyr/#alignment","title":"alignment \uae30\uc220 \ubc1c\uc804","text":"<ul> <li>DPO\uc640 ORPO \uac19\uc740 \ud6a8\uc728\uc801\uc778 alignment \uae30\ubc95\uc758 \uc2e4\uc6a9\uc131\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4.</li> <li>RLHF\uc758 \ubcf5\uc7a1\uc131\uacfc \ube44\uc6a9\uc744 \ud06c\uac8c \uc904\uc774\ub294 \ub300\uc548\uc744 \uc81c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"case_studies/zephyr/#_7","title":"\uc624\ud508\uc18c\uc2a4 \uc0dd\ud0dc\uacc4 \uae30\uc5ec","text":"<ul> <li>\uace0\uc131\ub2a5 alignment \ubaa8\ub378\uc744 \uc624\ud508\uc18c\uc2a4\ub85c \uacf5\uac1c\ud558\uc5ec \uc5f0\uad6c \ucee4\ubba4\ub2c8\ud2f0\uc5d0 \uae30\uc5ec\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\"Alignment Handbook\" \uc800\uc7a5\uc18c\ub97c \ud1b5\ud574 alignment \uae30\uc220\uc744 \ub300\uc911\ud654\ud588\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ucc38\uace0 \uc790\ub8cc: - Lewis Tunstall et al. \"Zephyr: Direct Distillation of LM Alignment.\" arXiv preprint 2310.16944 (2023) - HuggingFace H4 - Zephyr \ubaa8\ub378 \uce74\ub4dc (https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) - HuggingFace \ube14\ub85c\uadf8 - \"Preference Tuning LLMs with Direct Preference Optimization Methods\" (2023) </p>"},{"location":"concepts/fine_tuning_basics/","title":"Fine-Tuning \uae30\ubcf8 \uac1c\ub150","text":""},{"location":"concepts/fine_tuning_basics/#_1","title":"\uac1c\uc694","text":"<ul> <li> <p>Fine-Tuning\uc774\ub780?  </p> <ul> <li>Pre-Trained (\uc774\ubbf8 \ud29c\ub2dd\uc774 \ub418\uc5b4 \uc788\ub294 \uac83\ub3c4 \ud3ec\ud568) LLM\uc744 \ud2b9\uc815 \uc791\uc5c5/\ub3c4\uba54\uc778\uc5d0 \ub9de\uac8c \ucd94\uac00 \ud559\uc2b5\ud558\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4.</li> </ul> </li> <li> <p>\uae30\ub300 \ud6a8\uacfc </p> <ul> <li>\ub3c4\uba54\uc778 \uc801\ud569\uc131 \ud5a5\uc0c1: \uc77c\ubc18 LLM\uc774 \ub2e4\ub8e8\uc9c0 \ubabb\ud588\ub358 \uc804\ubb38 \ubd84\uc57c \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud568\uc73c\ub85c\uc368, \ud574\ub2f9 \ubd84\uc57c\uc5d0 \ub354 \uc815\ud655\ud558\uace0 \uc801\uc808\ud55c \uc751\ub2f5\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc608: \ubc94\uc6a9 \ubaa8\ub378\uc5d0 \uc758\ub8cc \uae30\ub85d \ub370\uc774\ud130\ub97c \ud30c\uc778\ud29c\ub2dd\ud558\uc5ec \uc758\ub8cc \uc0c1\ub2f4 \uc9c8\ubb38\uc5d0 \uc804\ubb38\uc801\uc778 \ub2f5\ubcc0\uc744 \uc0dd\uc131)</li> <li>\ud2b9\uc815 \uc791\uc5c5 \uc131\ub2a5 \ud5a5\uc0c1: \uc694\uc57d, \ubc88\uc5ed, \uc9c8\uc758\uc751\ub2f5 \ub4f1 \ud2b9\uc815 \ud0dc\uc2a4\ud06c\uc5d0 \ucd5c\uc801\ud654\ub41c \ub370\uc774\ud130\ub85c \ubbf8\uc138 \uc870\uc815\ud558\uba74 \uadf8 \uc791\uc5c5\uc5d0 \ub300\ud55c \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub429\ub2c8\ub2e4.</li> <li>\uc2a4\ud0c0\uc77c \ubc0f \uc77c\uad00\uc131 \ubd80\uc5ec: \uae30\uc5c5\uc774\ub098 \uc0ac\uc6a9\uc790\ub9cc\uc758 \ubb38\uccb4(style)\ub098 \ud3ec\ub9f7\uc744 \ubaa8\ub378\uc5d0\uac8c \ud559\uc2b5\uc2dc\ucf1c, \ucd9c\ub825 \uacb0\uacfc\uac00 \uc77c\uad00\ub41c \ud1a4\uacfc \ud615\uc2dd\uc744 \uc720\uc9c0\ud558\ub3c4\ub85d \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc5c5\ub370\uc774\ud2b8 \ubc0f \uac1c\uc120: \uc0c8\ub85c\uc6b4 \ud2b8\ub80c\ub4dc\ub098 \uc815\ubcf4\ub97c \ubc18\uc601\ud55c \ub370\uc774\ud130\ub85c \uc9c0\uc18d\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \uc7ac\ud6c8\ub828(Continual Tuning)\ud558\uba74, \ubaa8\ub378\uc774 \ucd5c\uc2e0 \uc815\ubcf4\uc5d0 \ub4a4\ucc98\uc9c0\uc9c0 \uc54a\uace0 \uacc4\uc18d \ubc1c\uc804\ub41c \uc131\ub2a5\uc744 \uc720\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> </ul> <p>\uc9c8\ubb38: \uacb0\uad6d \ubaa8\ub378\uc744 \ubcc0\uacbd (\uac1c\uc120) \ud574\uc11c \uc798 \ud65c\uc6a9\uc744 \ud574\ubcf4\uaca0\ub2e4\ub294 \uac83\uc778\ub370, \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c \ud558\uba74 \uc548\ub3fc\ub098\uc694? \"\ub3c4\uba54\uc778 \uc801\ud569\uc131\" \uac19\uc740 \uacbd\uc6b0\ub294 RAG\uac00 \ub354 \ud6a8\uacfc\uc801\uc774\uc9c0 \uc54a\ub098\uc694?</p> <p>\ub2f5\ubcc0: RAG\uc640 Fine-Tuning\uc740 \uae30\ub300 \ud6a8\uacfc\uc758 \uc885\ub958\uac00 \ub2e4\ub985\ub2c8\ub2e4!</p>"},{"location":"concepts/fine_tuning_basics/#rag-fine-tuning","title":"RAG \uc640 Fine-Tuning\uc758 \ucc28\uc774\uc810","text":"<ul> <li>RAG vs. Fine-Tuning<ul> <li>RAG: \uc678\ubd80 \uc9c0\uc2dd \uc18c\uc2a4 \ud65c\uc6a9\ud569\ub2c8\ub2e4, \uc2e4\uc2dc\uac04\uc744 \ud3ec\ud568\ud55c \ub2e4\uc591\ud55c \uc815\ubcf4 \uc811\uadfc\uc774 \uac00\ub2a5\ud558\ub098, vectorDB \uc640 \uac19\uc740 \ubcc4\ub3c4 \uc778\ud504\ub77c\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. </li> <li>Fine-Tuning: RAG \ub9cc\uc73c\ub85c\ub294 \ud480\ub9ac\uc9c0 \uc54a\ub294 \ubb38\uc81c\ub4e4\uc744 \ud480 \uc218 \uc788\uc2b5\ub2c8\ub2e4, \ubc18\ub300\ub85c \ud30c\uc778\ud29c\ub2dd\uc73c\ub85c \ubd88\uac00\ub2a5\ud55c \ubb38\uc81c\ub4e4\ub3c4 \uc788\uc8e0. \uc774\ub294 \ubcf4\ub2e4 \uc790\uc138\ud55c \uc608\uc2dc\ub97c \ud1b5\ud574 \uc124\uba85\ud558\ub294 \uac83\uc774 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.</li> </ul> </li> </ul> <p>\ud55c \ud68c\uc0ac\uc5d0\uc11c \uc9c1\uc6d0\ub4e4\uc758 \uc2ac\ub799 \uba54\uc138\uc9c0\ub85c \ud30c\uc778 \ud29c\ub2dd\uc744 \ud558\uace0 \uc77c\uc744 \uc2dc\ucf1c\ubd24\ub2e4\uace0 \ud569\ub2c8\ub2e4. </p>"},{"location":"concepts/fine_tuning_basics/#_2","title":"\uc2e4\uc81c \uc0ac\ub840: \ud68c\uc0ac \uc2ac\ub799 \uba54\uc2dc\uc9c0\ub85c \ud30c\uc778\ud29c\ub2dd\ud55c \uacbd\uc6b0","text":"<p>\ub2e4\uc74c\uc740 \ud55c \ud68c\uc0ac\uc5d0\uc11c \uc9c1\uc6d0\ub4e4\uc758 \uc2ac\ub799 \uba54\uc2dc\uc9c0\ub85c \ubaa8\ub378\uc744 \ud30c\uc778\ud29c\ub2dd\ud55c \ud6c4 \ubc1c\uc0dd\ud55c \uc0c1\ud669\uc785\ub2c8\ub2e4:</p> <p>User: \ud504\ub86c\ud504\ud2b8 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc5d0 \ub300\ud574 \ube14\ub85c\uadf8 \uae00 \uc368\uc918! Assistant: \ub124, \ub0b4\uc77c \uc544\uce68\uc5d0 \ud560\uac8c\uc694 User: \uc9c0\uae08 \ub2f9\uc7a5 \uc368\uc918 Assistant: \ub124 (\uc548\ud568)  </p> <p>\uc9dc\uc794! \uae30\ub300\ud55c \uac83\uc740 \ud68c\uc0ac\uc758 \ub0b4\ubd80 \uc9c0\uc2dd\ub4e4\uc744 \uc775\ud600\uc11c \uc9c4\uc9dc \uc9c1\uc6d0\ub4e4 \ucc98\ub7fc \uc77c\uc744 \ud558\uae30\ub97c \ubc14\ub7ac\uc73c\ub098, \uc77c\uc744 \ubbf8\ub8e8\ub294 \uc9c1\uc6d0\ub4e4\uc758 \ud0dc\ub3c4\ub97c \ubc30\uc6e0\uc2b5\ub2c8\ub2e4.  \ub2f5\ubcc0 \uc2a4\ud0c0\uc77c\uc744 \ud559\uc2b5\ud55c \uac83\uc774\uc8e0. \ubb3c\ub860 \uc2ac\ub799 \uba54\uc138\uc9c0\uc5d0 \ub179\uc544\uc788\ub358 \uc0ac\ub0b4 \uc9c0\uc2dd\ub4e4\ub3c4 \ubc30\uc6e0\uc744 \ud14c\uc9c0\ub9cc \uadf8 \uae30\ub2a5\uc774 \ubc14\ub85c \ubf51\ud600\ub098\uc624\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \ud504\ub86c\ud504\ud305\uc744 \ucd94\uac00\uc801\uc73c\ub85c \ub354 \uc798\ud574\uc918\uc57c \ud569\ub2c8\ub2e4.   </p> <p>\uc0ac\ub0b4 \uc9c0\uc2dd\uc744 \uae30\ubc18\uc73c\ub85c \uc989\uac01\uc801\uc778 \uc791\uc5c5 \uc218\ud589\uc774 \ud544\uc694\ud558\ub2e4\uba74, \ud30c\uc778\ud29c\ub2dd\ubcf4\ub2e4\ub294 RAG\uac00 \ub354 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4. \ucd5c\uc801\uc758 \uacb0\uacfc\ub97c \uc704\ud574\uc11c\ub294 \ub450 \ubc29\ubc95\uc744 \ubcd1\ud589\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.</p> <p>RAG \ub294 Context Optimization, \ud30c\uc778\ud29c\ub2dd\uc740 Model Optimization \uc785\ub2c8\ub2e4. \ub450 \ud589\uc704\ub294 \uc5b4\ub290 \uc815\ub3c4 orthogonal \ud558\uba70, \uc0c1\ud669\uc5d0 \ub9de\uac8c  \uc798 \ud65c\uc6a9\ud558\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4.  </p> <p></p> <p>\uc544\ub798 \uc601\uc0c1\uc740 OpenAI \uc758 devday \uc5d0\uc11c \uac1c\ubc1c\uc790\ub4e4\uc744 \ub300\uc0c1\uc73c\ub85c \ubcf8\uc778\ub4e4\uc758 RAG/\ud30c\uc778\ud29c\ub2dd \uacbd\ud5d8\uc744 \ubc14\ud0d5\uc73c\ub85c\ud55c \uac00\uc774\ub4dc \ub0b4\uc6a9\uc785\ub2c8\ub2e4. \ucc38\uace0\ud558\uc2dc\uba74 \ub3c4\uc6c0\uc774 \ub9cc\uc774 \ub418\uc2e4 \uac70\uc5d0\uc694. </p> <ul> <li>(\ucc38\uace0) Prompting vs. Fine-Tuning<ul> <li>Prompting: \ubaa8\ub378 \uc218\uc815 \uc5c6\uc774 \uc785\ub825\uc744 \ucd5c\uc801\ud654. \ube60\ub974\uace0 \uac04\ub2e8\ud558\ub098 \uae30\uc874 \uc9c0\uc2dd \ubc94\uc704\ub97c \ub298\ub9b4 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4, \ubaa8\ub378\uc758 \uc7a0\uc7ac\ub2a5\ub825\uc744 \ucd5c\ub300\ud55c \ub04c\uc5b4\ub0b4\ub294 \uac83\uc774 \ucd5c\uc120\uc774\uc8e0.</li> <li>Fine-Tuning: \ubaa8\ub378 weight \ub97c \ubcc0\uacbd. \ub9ac\uc18c\uc2a4\uac00 \ud544\uc694\ud558\ub098 \ubaa8\ub378 \uc790\uccb4\uc758 \uc7a0\uc7ac\ub41c \ub2a5\ub825\uc744 \ubc14\uafd4\uc8fc\ub2c8 \ub354 \ud3ec\ud150\uc15c\uc774 \ub192\uc2b5\ub2c8\ub2e4, \ud604\uc2e4\uc5d0\uc11c\ub294 \ud30c\uc778\ud29c\ub2dd\uc744 \ud574\ub3c4 \ud504\ub86c\ud504\ud305\uc740 \ub2f9\uc5f0\ud788 \uac19\uc774 \ub354 \ud569\ub2c8\ub2e4.</li> <li>\uc0ac\ub78c\uc5d0 \ube44\uc720\ud558\uc790\uba74, \uc9c1\uc6d0\uc5d0\uac8c \uba54\ub274\uc5bc\uc744 \uc798 \uc8fc\ub294\uac8c \ud504\ub86c\ud504\ud305\uc774\uace0, \uc9c1\uc6d0\uc744 \uad50\uc721\uc2dc\ud0a4\ub294 \uac83\uc774 \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4. </li> </ul> </li> </ul>"},{"location":"concepts/fine_tuning_basics/#fine-tuning_1","title":"Fine-Tuning\uc758 \uc885\ub958","text":"<ul> <li> <p>\uae30\ubc95\uc5d0 \ub530\ub978 \ubd84\ub958  </p> <ul> <li>Full Fine-Tuning: \ubaa8\ub378\uc758 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130 \uc5c5\ub370\uc774\ud2b8 \ud569\ub2c8\ub2e4.  </li> <li>Parameter-Efficient Fine-Tuning: \uc5b4\ub311\ud130 \ub4f1 \uc77c\ubd80 \ud30c\ub77c\ubbf8\ud130\ub9cc \uc870\uc815\ud569\ub2c8\ub2e4. \ub300\ud45c\uc801\uc73c\ub85c LoRA \uac00 \uc788\uc2b5\ub2c8\ub2e4. </li> </ul> </li> <li> <p>\ubaa9\uc801\uc5d0 \ub530\ub978 \ubd84\ub958</p> <ul> <li>Continuous Pretraining: Pretrain \ubaa8\ub378\uc5d0 \uc0c8\ub85c\uc6b4 \ucf54\ud37c\uc2a4(ex. \uc9c0\uc2dd)\uc744 \ud559\uc2b5\uc2dc\ud0a4\uae30 \uc704\ud55c \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4.</li> <li>Instruction Tuning: Pretrain \ubaa8\ub378\uc5d0 \ucc44\ud305 \uae30\ub2a5 (\uc9c0\uc2dc\ub97c \ub530\ub974\uac8c \ud558\uae30) \uc744 \ucd94\uac00\ud558\uae30 \uc704\ud55c \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4.</li> <li>Domain QA Tuning: \ud2b9\uc815 \ub3c4\uba54\uc778(\uc758\ub8cc, \ubc95\ub960 \ub4f1)\uc758 \ub370\uc774\ud130\ub97c \ud559\uc2b5\uc2dc\ucf1c \uc804\ubb38\uc801\uc778 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uac8c \ud558\ub294 \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4.</li> <li>Refusal Training: \ube44\uc724\ub9ac\uc801\uc778 \uc751\ub2f5\uc744 \uae08\uc9c0\uc2dc\ud0a4\uae30 \uc704\ud55c \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4.</li> <li>Style Tuning: \ub2f5\ubcc0\uc758 \ud615\uc2dd \ubcc0\ud654\ub97c \uc704\ud55c \ud30c\uc778\ud29c\ub2dd\uc785\ub2c8\ub2e4 (\uae38\uc774, \ub9d0\ud22c \ub4f1).</li> </ul> </li> </ul>"},{"location":"concepts/llm_background/","title":"LLM Background \uc9c0\uc2dd","text":""},{"location":"concepts/llm_background/#llm","title":"LLM\uc758 \ud6c8\ub828 \uacfc\uc815","text":"<p>LLM\uc744 \ub9cc\ub4e4 \ub54c\ub294 \ud06c\uac8c \uc138 \uac00\uc9c0 \uc8fc\uc694 \ub2e8\uacc4\ub97c \uac70\uce69\ub2c8\ub2e4:</p> <p>(\ucc38\uace0) \uc774 \uadf8\ub9bc\uc740 \uc544\uc8fc \uac04\ub2e8\ud558\uac8c \ub3c4\uc2dd\ud654\ud55c \uc608\uc2dc\ub77c\uc11c, \uad6c\uccb4\uc801\uc778 \uc0ac\ub840\uc5d0\uc11c\ub294 \ub9ce\uc774 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ub4a4\uc5d0\uc11c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"concepts/llm_background/#1-pre-training","title":"1. \uc0ac\uc804 \ud559\uc2b5(Pre-Training)","text":"<p>\uc0ac\uc804 \ud559\uc2b5\uc740 LLM\uc774 \uae30\ubcf8\uc801\uc778 \uc5b8\uc5b4 \uc774\ud574\uc640 \uc0dd\uc131 \ub2a5\ub825\uc744 \uc2b5\ub4dd\ud558\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4.</p> <ul> <li>\ub370\uc774\ud130 \uc218\uc9d1: \uc778\ud130\ub137 \ud14d\uc2a4\ud2b8(\uc6f9 \ud398\uc774\uc9c0, \uc704\ud0a4, \ucc45 \ub4f1)\uc5d0\uc11c \ub300\uaddc\ubaa8 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud569\ub2c8\ub2e4.</li> <li>\ud1a0\ud070\ud654(Tokenization): \ud14d\uc2a4\ud2b8\ub97c \ud1a0\ud070\uc774\ub77c\ub294 \uc791\uc740 \ub2e8\uc704\ub85c \ubd84\ud560\ud569\ub2c8\ub2e4. \ubaa8\ub378\ub9c8\ub2e4 \uace0\uc720\ud55c \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud558\uba70, GPT-4\uc758 \uacbd\uc6b0 \uc57d 10\ub9cc \uac1c\uc758 \ud1a0\ud070 \uc0ac\uc804\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud559\uc2b5 \ubc29\uc2dd: \"Next Token Prediction\"\uc774\ub77c\ub294 \ubc29\uc2dd\uc73c\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4. \uc8fc\uc5b4\uc9c4 \ud14d\uc2a4\ud2b8 \uc2dc\ud000\uc2a4\uc5d0\uc11c \ub2e4\uc74c\uc5d0 \uc62c \ud1a0\ud070\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4.</li> <li>\uc544\ud0a4\ud14d\ucc98: \ud604\ub300 LLM\uc740 \ub300\ubd80\ubd84 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer)\uc758 \ub514\ucf54\ub354(Decoder) \ubd80\ubd84\ub9cc \uc0ac\uc6a9\ud558\ub294 \"decoder-only\" \uad6c\uc870\ub97c \ucc44\ud0dd\ud569\ub2c8\ub2e4.</li> <li>\uacb0\uacfc\ubb3c: \uc778\ud130\ub137 \ud14d\uc2a4\ud2b8\uc758 \ud328\ud134\uc744 \ud655\ub960\uc801\uc73c\ub85c \ubaa8\ubc29\ud560 \uc218 \uc788\ub294 \ubca0\uc774\uc2a4(Base) \ubaa8\ub378\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.</li> <li>\ud2b9\uc9d5: \ubca0\uc774\uc2a4 \ubaa8\ub378\uc740 \ub300\ud654\ud615\uc73c\ub85c \uc0ac\uc6a9\ud558\uae30\uc5d0\ub294 \uc801\ud569\ud558\uc9c0 \uc54a\uc73c\uba70, \ub2e8\uc21c\ud788 \ud14d\uc2a4\ud2b8\ub97c \ud1b5\uacc4\uc801\uc73c\ub85c \uc774\uc5b4\uc4f0\ub294 \ub2a5\ub825\ub9cc \uac16\ucd98 \uc0c1\ud0dc\uc785\ub2c8\ub2e4.</li> </ul>"},{"location":"concepts/llm_background/#2-supervised-fine-tuning-sft","title":"2. \uc9c0\ub3c4 \ubbf8\uc138 \uc870\uc815(Supervised Fine-Tuning, SFT)","text":"<p>SFT\ub294 \ubca0\uc774\uc2a4 \ubaa8\ub378\uc774 \ub300\ud654\ud615 \uc5b4\uc2dc\uc2a4\ud134\ud2b8\ucc98\ub7fc \uc791\ub3d9\ud558\ub3c4\ub85d \ucd94\uac00 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4.</p> <ul> <li>\ub370\uc774\ud130: \uc0ac\uc6a9\uc790-\uc5b4\uc2dc\uc2a4\ud134\ud2b8 \ud615\ud0dc\uc758 \uace0\ud488\uc9c8 \ub300\ud654 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucd08\uae30\uc5d0\ub294 \uc0ac\ub78c\uc774 \uc9c1\uc811 \uc791\uc131\ud588\uc73c\ub098, \ucd5c\uadfc\uc5d0\ub294 AI\uac00 \uc0dd\uc131\ud558\uace0 \uc0ac\ub78c\uc774 \ud3b8\uc9d1\ud558\uac70\ub098 \uc644\uc804\ud788 AI\uac00 \uc0dd\uc131\ud55c \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uae30\ub3c4 \ud569\ub2c8\ub2e4.</li> <li>\ubaa9\uc801: \ubaa8\ub378\uc774 \uc9c8\ubb38\uc5d0 \ub300\ud574 \ub3c4\uc6c0\uc774 \ub418\uace0, \uc0ac\uc2e4\uc5d0 \uae30\ubc18\ud558\uba70, \ud574\ub86d\uc9c0 \uc54a\uc740 \ub300\ub2f5\uc744 \ud558\ub3c4\ub85d \uac00\ub974\uce69\ub2c8\ub2e4.</li> <li>\uacfc\uc815: \ubca0\uc774\uc2a4 \ubaa8\ub378\uc5d0 \ube44\ud574 \ud6e8\uc52c \uc801\uc740 \uc591\uc758 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ud558\uc9c0\ub9cc, \uc2e4\uc81c \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc911\uc694\ud55c \ub2e8\uacc4\uc785\ub2c8\ub2e4.</li> <li>Hallucination \ud574\uacb0: \ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uac83\uc5d0 \ub300\ud574 \"\ubaa8\ub978\ub2e4\"\uace0 \ub300\ub2f5\ud558\uac70\ub098, \uac80\uc0c9\uacfc \uac19\uc740 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4.</li> <li>\uacb0\uacfc\ubb3c: \uc2e4\uc81c\ub85c \uc9c8\ubb38\uc5d0 \ub300\ud654\ud615\uc73c\ub85c \ub2f5\ud560 \uc218 \uc788\ub294 Instruct \ubaa8\ub378\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.</li> </ul>"},{"location":"concepts/llm_background/#3-reinforcement-learning-rl","title":"3. \uac15\ud654 \ud559\uc2b5(Reinforcement Learning, RL)","text":"<p>\uac15\ud654 \ud559\uc2b5\uc740 \ubaa8\ub378\uc758 \uc751\ub2f5 \ud488\uc9c8\uc744 \ub354\uc6b1 \uac1c\uc120\ud558\ub294 \ub2e8\uacc4\uc785\ub2c8\ub2e4.</p> <ul> <li>\uae30\ubcf8 \uc6d0\ub9ac: \ubaa8\ub378\uc774 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc73c\ub85c \ubb38\uc81c\ub97c \ud480\uc5b4\ubcf4\uace0, \uc88b\uc740 \uacb0\uacfc\uc5d0 \ubcf4\uc0c1\uc744 \uc8fc\uc5b4 \uc2a4\uc2a4\ub85c \uac1c\uc120\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.</li> <li>\ubc29\ubc95\ub860:</li> <li>\uc815\ub2f5\uc774 \uba85\ud655\ud55c \uacbd\uc6b0(\uc608: \uc218\ud559, \ucf54\ub529): \uc815\ub2f5\uc744 \uae30\uc900\uc73c\ub85c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uace0 \ubcf4\uc0c1\uc744 \uc90d\ub2c8\ub2e4.</li> <li>\uc815\ub2f5\uc774 \uc8fc\uad00\uc801\uc778 \uacbd\uc6b0(\uc608: \uc720\uba38, \uc2dc): RLHF(Reinforcement Learning from Human Feedback)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc0ac\ub78c\uc758 \ud3c9\uac00\ub97c \ubaa8\ubc29\ud558\ub294 \ubcf4\uc0c1 \ubaa8\ub378(Reward Model)\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0, \uc774\ub97c \uae30\uc900\uc73c\ub85c RL\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.</li> <li>\uc7a5\uc810: RL\uc744 \ud1b5\ud574 \ubaa8\ub378\uc740 \uc778\uac04\uc774 \uc0dd\uac01\ud558\uc9c0 \ubabb\ud55c \ud480\uc774 \ubc29\ubc95\uc774\ub098 \uc811\uadfc\ubc95\uc744 \ubc1c\uacac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. DeepSeek-R1\uc774\ub098 \uc54c\ud30c\uace0\uc640 \uac19\uc740 \uc0ac\ub840\uc5d0\uc11c \uc774\ub7ec\ud55c \ud604\uc0c1\uc774 \uad00\ucc30\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ud2b9\uc9d5: \uac15\ud654 \ud559\uc2b5\uc744 \uac70\uce5c \ubaa8\ub378\ub4e4\uc740 \uc885\uc885 \"\uc0dd\uac01 \uc2dc\uac04\"\uc744 \ub354 \uae38\uac8c \uac00\uc9c0\uba70, \ub2e8\uacc4\uc801\uc778 \ucd94\ub860 \uacfc\uc815\uc744 \uac70\uccd0 \ub354 \uc815\ud655\ud55c \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> <li>RLHF\uc758 \ud55c\uacc4: \ubcf4\uc0c1 \ubaa8\ub378\uc774 \uc644\ubcbd\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0, \uacfc\ub3c4\ud55c RL \ud559\uc2b5\uc740 \uc624\ud788\ub824 \ubaa8\ub378 \uc131\ub2a5\uc744 \uc800\ud558\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"concepts/llm_background/#llm_1","title":"\ucd5c\uc2e0 LLM \ubaa8\ub378\uc758 \ud2b9\uc9d5","text":"<p>\ud604\ub300 LLM\uc740 \uc704 \uc138 \ub2e8\uacc4\ub97c \uac70\uccd0 \uac1c\ubc1c\ub418\uba70, \ub2e4\uc74c\uacfc \uac19\uc740 \ud2b9\uc9d5\uc744 \uac00\uc9d1\ub2c8\ub2e4:</p> <ul> <li>\ub300\uaddc\ubaa8 \ud559\uc2b5 \ub370\uc774\ud130: \ucd5c\uc2e0 \ubaa8\ub378\ub4e4(\uc608: Llama 3.1)\uc740 15\uc870 \uc774\uc0c1\uc758 \ud1a0\ud070\uc73c\ub85c \ud559\uc2b5\ub429\ub2c8\ub2e4.</li> <li>\ub3c4\uad6c \uc0ac\uc6a9 \ub2a5\ub825: \uc6f9 \uac80\uc0c9, \ucf54\ub4dc \uc2e4\ud589, API \ud638\ucd9c \ub4f1 \uc678\ubd80 \ub3c4\uad6c\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uba40\ud2f0\ubaa8\ub2ec \ub2a5\ub825: \ud14d\uc2a4\ud2b8\ubfd0\ub9cc \uc544\ub2c8\ub77c \uc774\ubbf8\uc9c0, \uc74c\uc131 \ub4f1 \ub2e4\uc591\ud55c \ud615\ud0dc\uc758 \uc785\ub825\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ucd94\ub860 \ub2a5\ub825: \ub2e8\uacc4\uc801 \uc0ac\uace0 \uacfc\uc815\uc744 \ud1b5\ud574 \ubcf5\uc7a1\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"concepts/llm_background/#_1","title":"\ub354 \uc54c\uc544\ubcf4\uae30","text":"<p>\ud639\uc2dc LLM \uc5d0 \ub300\ud55c \ubc30\uacbd\uc9c0\uc2dd\uc774 \ubd80\uc871\ud558\uc2dc\uac70\ub098 \ubcf4\ub2e4 \uc790\uc138\ud55c \ub0b4\uc6a9\uc774 \uad81\uae08\ud558\uc2dc\uba74, \uc544\ub798 LLM \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694. \uc791\uae08\uc758 LLM \uc5d0 \ub300\ud55c \uac1c\uad04\uc801\uc778 \ub0b4\uc6a9\uc744 \uc815\ub9ac\ud55c \ubb38\uc11c\uc785\ub2c8\ub2e4.  </p> <p>\ud544\uc790\uac00 \uc791\uc131\ud55c LLM Deep Dive \uc2dc\ub9ac\uc988 \ud398\uc774\uc9c0 \uc785\ub2c8\ub2e4. </p>"},{"location":"concepts/llm_paradigm/","title":"LLM \ud328\ub7ec\ub2e4\uc784","text":""},{"location":"concepts/llm_paradigm/#scaling-law","title":"\uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59 (Scaling Law)","text":"\ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac Machine Learning \uc2dc\ub300\ubd80\ud130 Deep Learning, LLM, \ucd5c\uadfc\uc758 Reasoning \ubaa8\ub378\uae4c\uc9c0 \ubaa8\ub4e0 \ud750\ub984\uc744 \ud558\ub098\ub85c \uad00\ud1b5\ud558\ub294 \ud0a4\uc6cc\ub4dc\ub97c \ubf51\uc73c\ub77c\uba74 \uc800\ub294 \ub2e8\uc5f0\ucf54 \"Scaling Law\" \ub97c \ubf51\uaca0\uc2b5\ub2c8\ub2e4."},{"location":"concepts/llm_paradigm/#_1","title":"\uc2a4\ucf00\uc77c\ub9c1 \ubc95\uce59\uc758 \uc758\ubbf8\uc640 \uc911\uc694\uc131","text":"<ul> <li>\ubaa8\ub378 \uc0ac\uc774\uc988, \ub370\uc774\ud130 \uc591, \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \ud655\uc7a5\ud568\uc5d0 \ub530\ub77c \ubaa8\ub378 \uc131\ub2a5\uc774 \uccb4\uacc4\uc801\uc73c\ub85c \ud5a5\uc0c1\ub418\ub294 \uacbd\ud5d8\uc801 \ubc95\uce59\uc785\ub2c8\ub2e4.</li> <li>LLM \uc758 \uc131\ub2a5 (\uc5ec\uae30\uc11c\ub294 test loss) \uc740 \uc0ac\uc774\uc988\ub97c \ud0a4\uc6b0\uba74 \uc88b\uc544\uc9c4\ub2e4\ub294 \uacbd\ud5a5\uc744 \uc2e4\ud5d8\uc801\uc73c\ub85c \ubc1c\uacac\ud55c \ud398\uc774\ud37c\uc785\ub2c8\ub2e4. </li> <li>2020\ub144\ub300\uc5d0 \ub4e4\uc5b4 LLM\uacfc \ub370\uc774\ud130 \uaddc\ubaa8\ub97c \ubaa8\ub450 \ud0a4\uc6b0\ub294 \uc5f0\uad6c \ubc29\ud5a5\uc774 \uc9c0\uc18d\ub418\uc5c8\ub294\ub370\uc694, \uc774 \ubc29\ud5a5\uc744 \uae09\uaca9\ud558\uac8c \uac00\uc18d\ud654\ud558\ub294 \uacc4\uae30\uac00 Scaling Law \uc600\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.</li> <li>\ucc38\uc870 (\uaf2d \ubcf4\uc2dc\ub294 \uac83\uc744 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4!) : Scaling Laws for Neural Language Models</li> </ul> <p>\ud558\ub098\uc758 \uadf8\ub798\ud504\ub85c \uc694\uc57d\ud55c \ub0b4\uc6a9\uc785\ub2c8\ub2e4. \uac01 \uc694\uc18c\ub4e4\uc5d0 \ub300\ud574 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc5bc\ub9c8\ub098 \uc131\ub2a5 \uac1c\uc120\uc744 \uc77c\uc73c\ud0a4\ub294\uc9c0 \uc2e4\ud5d8\ub4e4\uc774 \ub2e4 \uc788\uc73c\ub2c8 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ucc38\uc870 \ud398\uc774\ud37c\ub97c \ubcf4\uc138\uc694.  </p> <ul> <li>\ubaa8\ub378 \ud06c\uae30, \ub370\uc774\ud130 \uc591, \ucef4\ud4e8\ud305 \uc790\uc6d0 \uc774 3\uac00\uc9c0\ub97c \ub298\ub9ac\uba74 \uc131\ub2a5\uc774 \uc88b\uc544\uc9c4\ub2e4! \uac04\ub2e8\ud569\ub2c8\ub2e4. </li> <li>\ucef4\ud4e8\ud305 \uc790\uc6d0\uc774 \uc2dc\uac04\uc774 \uc9c0\ub0a0\uc218\ub85d \uc800\ub834\ud574\uc9c0\ub2c8, \uc2a4\ucf00\uc77c\ub9c1\uc740 \uc9c0\uc18d\uc801\uc73c\ub85c \uc9c4\ud589\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130\uac00 \uace0\uac08\ub098\ub294 \uac83\uc774 \ucd5c\uadfc\uc758 \uace0\ubbfc \uac70\ub9ac \uc785\ub2c8\ub2e4. </li> </ul>"},{"location":"concepts/llm_paradigm/#train-time-vs-test-time-scaling","title":"Train-Time vs. Test-Time Scaling","text":"<p>\uc55e\uc11c \uc124\uba85\ud55c \ub0b4\uc6a9\uc740 Train-Time Scaling \uc785\ub2c8\ub2e4, \ud559\uc2b5 \uc2dc\uac04\uc744 \ub298\ub9b0 \uac83\uc774\uc8e0. \ucd5c\uadfc\uc5d0\ub294 \ub2e4\ub978 \ubc29\ud5a5\uc73c\ub85c \uc2a4\ucf00\uc77c\ub9c1\uc774 \ub098\uc624\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \uc2a4\ud0c0\ud2b8\ub97c \ub04a\uc740 \ubaa8\ub378\uc740 OpenAI o1 \uc774\uace0, \ucd5c\uadfc\uc5d0\ub294 DeepSeek R1, QwQ, Sonnet Thinking, Gemini Thinking \ub4f1 \uc3df\uc544\uc838 \ub098\uc624\uace0 \uc788\uc8e0.  </p> <p>\ucc38\uc870 - Learning to Reason with LLMs</p>"},{"location":"concepts/llm_paradigm/#test-time-scaling","title":"\ucd94\ub860 \ub2e8\uacc4 \uc2a4\ucf00\uc77c\ub9c1 (Test-Time Scaling)","text":"<p>Test-Time = Inference-Time = \ucd94\ub860 \uc2dc\uac04 \uc785\ub2c8\ub2e4. \ubaa8\ub378\uc774 \ud1a0\ud070\uc744 \ub0b4\ubc49\ub294 \uc5f0\uc0b0 \uc2dc\uac04 (=\uc5f0\uc0b0\ub7c9) \uc744 \ub298\ub9ac\ub294 \uac83 \uc785\ub2c8\ub2e4. \uae30\uc874\uc5d0\ub294 \ud559\uc2b5 \uc2dc\uac04\uc744 \ub298\ub838\ub2e4\uba74, \uc774\uc81c\ub294 \uc77c\ud558\ub294 \uc2dc\uac04\uc744 \ub298\ub9bd\ub2c8\ub2e4. \uc0ac\ub78c\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc5d0\uc694, \uacf5\ubd80\ub97c \uc624\ub798\ud574\ub3c4 \ub611\ub611\ud574\uc9c0\uc9c0\ub9cc, \uc2dc\ud5d8 \uc2dc\uac04\uc774 \ub109\ub109\ud558\uba74 \uc2dc\ud5d8\uc810\uc218\uac00 \uc62c\ub77c\uac00\uc8e0.</p> <ul> <li> <p>Chain-of-Thought</p> <ul> <li>\uc774\ub7f0 \ubc29\ubc95\ub860\uc740 Prompting \uc73c\ub85c \uad6c\ud604\ud558\uace0 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.  </li> <li>\ub2e8\uc21c \ub300\ub2f5\uc774 \uc544\ub2c8\ub77c \ud480\uc774\ub97c \uc4f0\uba74\uc11c \ub300\ub2f5\ud558\ub3c4\ub85d \uc720\ub3c4\ud558\uba74 \ub611\ub611\ud574 \uc9d1\ub2c8\ub2e4.  </li> </ul> </li> <li> <p>Reasoning Models</p> <ul> <li>\ucd5c\uadfc\uc758 reasoning \ubaa8\ub378\ub4e4\uc740 CoT \ud504\ub86c\ud504\ud305\uc774 \uc5c6\uc774\ub3c4, \uc624\ub798 \uc0dd\uac01\ud558\uba74\uc11c \ub2f5\ubcc0\ud558\ub294 \ubaa8\ub378\ub4e4 \uc785\ub2c8\ub2e4. </li> <li>\ucc38\uc870 \ud398\uc774\ud37c\uc778 Learning to Reason \uc5d0 \ub530\ub974\uba74, OpenAI o1\uc740 \uac15\ud654\ud559\uc2b5\uc744 \ud1b5\ud574 reasoning \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5c8\ub2e4\uace0 \ud569\ub2c8\ub2e4, \uc790\uc138\ud55c \ubc29\ubc95\uc740 \ube44\uacf5\uac1c\uace0\uc694. </li> <li>\ubcf5\uc7a1\ud55c \uc0ac\uace0\ub97c \uc694\ud558\ub294 \ubb38\uc81c\ub4e4, \uc218\ud559\uc774\ub098 \ucf54\ub529\uacfc \uac19\uc740 \ubb38\uc81c\ub4e4\uc5d0 \ub300\ud574\uc11c \ube44\uc57d\uc801\uc778 \uc131\ub2a5\ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.  </li> </ul> </li> <li> <p>Workflow \ub610\ub294 Agent \ub294?</p> <ul> <li>\uadf8\ub807\ub2e4\uba74, Self Correction\uc774\ub098 Multi Agent \uac04\uc758 \ub300\ud654\ub4e4\uc744 \ud1b5\ud574 LLM \ub4e4\uc744 \ub2e4\uc591\ud558\uac8c \ud65c\uc6a9\ud574\uc11c \ubc29\ubc95\ub860\ub4e4\ub3c4 Test-Time Scaling \uc744 \ud560 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c\uc694?</li> <li>LLM \uc790\uccb4\uc758 test-time scaling \uc740 \uc544\ub2c8\uc9c0\ub9cc, \uc2dc\uc2a4\ud15c \uad00\uc810\uc5d0\uc11c \ubcf4\uba74 test-time scaling \uc774\ub77c\uace0 \ud574\ub3c4 (\uc870\uae08 \uc5b5\uc9c0 \ubcf4\ud0dc\uba74) \ub9d0\uc774 \ub418\uae34 \ud569\ub2c8\ub2e4. \ubcf4\ud1b5 \uc774\uac83\uc744 test-time scaling \uc774\ub77c\uace0 \ubd80\ub974\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. </li> </ul> </li> </ul>"},{"location":"concepts/llm_paradigm/#incentivization-and-reward-design","title":"\uc778\uc13c\ud2f0\ube0c \uc124\uacc4 (Incentivization and Reward Design)","text":"<p>OpenAI o1\uc740 \uc5b4\ub5bb\uac8c \ub9cc\ub4e4\uc5c8\uc744\uae4c\uc694? \uc774 \ud78c\ud2b8\ub294 DeepSeek \uac00 R1\uc744 \ub9cc\ub4e4\uace0 \uadf8 \ubc29\ubc95\uc744 \uc0c1\uc138\ud788 \uacf5\uac1c\ud558\uba74\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  </p>"},{"location":"concepts/llm_paradigm/#rule-based-incentive","title":"Rule-based Incentive","text":"<p>\uc27d\uac8c \uc694\uc57d\ud558\uba74, \ubaa8\ub378\uc5d0\uac8c \uc790\uc720\ub3c4\ub97c \uc8fc\uace0 \uc218\ud559 \ubb38\uc81c\ub97c \ud480\ub9bd\ub2c8\ub2e4. \ud480\uc774 \ubc29\ubc95\uc740 \uc54c\uc544\uc11c \ucc3e\uc73c\ub77c \ud558\uace0 \uc218\ud559 \ubb38\uc81c\uc758 \ub2f5\ubcc0\uc744 \ub9de\ucd94\ub294 \uac83\uc744 \ubcf4\uc0c1\uc73c\ub85c \uc90d\ub2c8\ub2e4. Supervised Fine Tuning \ucc98\ub7fc \ud480\uc774 \ubc29\ubc95\uc744 \uac00\uc774\ub4dc \ud558\uc9c0 \uc54a\uc8e0.  \"\uadf8\ub7ac\ub354\ub2c8 \uc54c\uc544\uc11c \uae38\uac8c \ud480\uc774\ub97c \uc4f0\uba70 \ud559\uc2b5\uc744 \ud558\ub354\ub77c\" \ub77c\ub294 \uac83\uc774 \uc8fc\uc694\ud55c \uace8\uc790\uc785\ub2c8\ub2e4.  </p> <p>\uc774 \ub0b4\uc6a9\uc740 DeepSeek\uc758 \uc131\uacf5\uc0ac\ub840 \ud0d0\uad6c \ud30c\ud2b8\uc5d0\uc11c \ub354 \uc790\uc138\ud788 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"concepts/llm_paradigm/#reinforcement-learning","title":"Reinforcement Learning (\uac15\ud654 \ud559\uc2b5)","text":"<p>\uc704 \ubc29\ubc95\uc740 \uac15\ud654 \ud559\uc2b5\uc785\ub2c8\ub2e4. \uac15\ud654\ud559\uc2b5\uc740 GPT-3.5 \uc5d0\uc11c \ubd80\ud130\ub3c4 \uc0ac\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. </p> <ul> <li>\uc778\uac04 \ud53c\ub4dc\ubc31 \uac15\ud654\ud559\uc2b5(RLHF):</li> <li>\uc778\uac04 \uc120\ud638\ub3c4 \uae30\ubc18 Reward Model (\ubcf4\uc0c1 \ubaa8\ub378) \uc744 \ub9cc\ub4e4\uc5b4\uc11c \ubcf4\uc0c1\uc744 \uc90d\ub2c8\ub2e4. </li> <li>\uc0ac\ub78c\ub4e4\uc774 \uc88b\uc544\ud558\ub294 \ub2f5\ubcc0\uc744 \uc720\ub3c4\ud569\ub2c8\ub2e4. </li> <li>RL \uc5d0\ub294 \ubaa8\ub378\uc774 \ud3b8\ubc95\uc744 \uc775\ud788\uac8c \ub418\ub294 Reward Hacking \ubb38\uc81c\uac00 \uc788\uc5b4\uc11c \uc131\uacf5\uc2dc\ud0a4\uae30\uac00 \uc5b4\ub835\uc9c0\ub9cc, \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\ub294 \ud504\ub860\ud2f0\uc5b4 \ubaa8\ub378 (ex. GPT \uc2dc\ub9ac\uc988) \ub4e4\uc740 \ub300\ubd80\ubd84 RLHF \ub97c \uc801\uc6a9\uc744 \ud574\uc654\uc2b5\ub2c8\ub2e4. </li> </ul> <p>Incentivize \ub97c \uae30\ubc18\uc73c\ub85c \ud55c \uac15\ud654\ud559\uc2b5 \ubc29\ubc95\uc740 \uc0c8\ub85c\uc6b4 \ubc29\ubc95\uc758 Scaling \uc785\ub2c8\ub2e4.</p> <p>\ubaa8\ub378\uc5d0\uac8c \uae00\uc744 \uc77d\ud788\uace0, \ub530\ub77c\ud558\ub3c4\ub85d \ud558\ub294 \ud559\uc2b5 \ubc29\ubc95\uc5d0\uc11c \ubc97\uc5b4\ub098 \ubaa8\ub378\uc5d0\uac8c \uc790\uc720\ub3c4\ub97c \uc8fc\uace0 \uc9c1\uc811 \ucc3d\uc758\uc801\uc73c\ub85c \ubc29\ubc95\uc744 \ucc3e\uac8c \uc720\ub3c4\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc0ac\ub78c\uc744 \uad50\uc721\ud558\ub294 \uac83\uc5d0 \ube44\uc720\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.   - \uc0ac\ub78c\uc5d0\uac8c \uba54\ub274\uc5bc\uc744 \uc8fc\uace0, \ub530\ub77c\ud558\ub3c4\ub85d \ud558\ub294 \ubc29\ubc95\uc744 \ubc97\uc5b4\ub098,   - \uc0ac\ub78c\uc5d0\uac8c \ubb38\uc81c\ub97c \uc8fc\uace0 \ucc3d\uc758\uc801\uc73c\ub85c \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc744 \ucc3e\uac8c \ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. </p> <p>\"Inductive Bias\" (\uc778\uac04\uc774 \ubaa8\ub378\uc5d0\uac8c \uc81c\uc2dc\ud558\ub294 \uac00\uc774\ub4dc) \ub97c \ud480\uc5b4\uc92c\ub2e4, \ub77c\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p> <p>OpenAI \uc758 \uc815\ud615\uc6d0 \ubc15\uc0ac\uac00 MIT \uc5d0\uc11c \uc9c4\ud589\ud55c \uc138\ubbf8\ub098 \uc778\ub370, \ubcf4\uc2dc\ub294 \uac83\uc744 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4.  </p>"},{"location":"concepts/model_types/","title":"LLM \ubaa8\ub378 \uc720\ud615 \ubc0f \ud2b9\uc9d5","text":""},{"location":"concepts/model_types/#llm-status","title":"LLM Status \uc5d0 \ub530\ub978 \ubd84\ub958","text":""},{"location":"concepts/model_types/#base-model-vs-instruct-model","title":"Base Model vs Instruct Model","text":"<p>\ub9ce\uc740 \uc624\ud508 \uc6e8\uc774\ud2b8 \ubaa8\ub378\ub4e4\uc740 Base Model \uacfc Instruct Model \ub450 \uac00\uc9c0 \ubc84\uc804\uc73c\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4. \ubb3c\ub860 Instruct Model \ub9cc \uacf5\uac1c\ud558\ub294 \uacbd\uc6b0\ub3c4 \ub9ce\uc2b5\ub2c8\ub2e4. \uc544\ub798 \uadf8\ub9bc\uc740 25\ub144 3\uc6d4 \uacf5\uac1c\ub41c Gemma3 \uc2dc\ub9ac\uc988\uc758 HuggingFace \ud398\uc774\uc9c0 \uc785\ub098\ub2e4. pt (pretrained, base model \uc784) \ubc84\uc804\uacfc it (instruct, \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378 \uc784) \ubc84\uc804\uc774 \uc788\uc2b5\ub2c8\ub2e4.  </p> <p>Gemma3 \uc2dc\ub9ac\uc988\uc758 HuggingFace \ud398\uc774\uc9c0</p> <p>\ud604\uc2dc\uc810 \uae30\uc900\uc73c\ub85c \ub9ce\uc740 \ud55c\uad6d\uc5b4 \ubaa8\ub378\ub4e4 (Exaone, Kanana, ...) \ub4f1\uc740 Base Model \uc744 \uacf5\uac1c\ud558\uc9c0 \uc54a\uace0 \uc788\uc5b4\uc11c \uc544\uc27d\uc2b5\ub2c8\ub2e4.  </p>"},{"location":"concepts/model_types/#base-model","title":"Base Model","text":"<ul> <li>Base Model \uc740 pretrained \ub9cc \uc644\ub8cc\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc0ac\ub78c\ucc98\ub7fc \ub300\ud654\ud558\uc9c0 \ubabb \ud569\ub2c8\ub2e4. \ud559\uc2b5\ub2e8\uacc4\uc5d0\uc11c \ub2e8\uc21c\ud788 \uc218\ub9ce\uc740 \uae00\uc744 \uc77d\uc5c8\uae30 \ub584\ubb38\uc5d0, \uc5b4\ub5a0\ud55c input\ub3c4 \uae00\uc758 \uc77c\ubd80\ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.</li> <li>\uc544\ub798\uc640 \uac19\uc774 \uc704\ud0a4 \uc2a4\ud0c0\uc77c\uc758 \uae00\uc744 \uc77c\ubd80 \uc778\ud48b\uc73c\ub85c \uc8fc\uba74 \uc790\uc5f0\uc2a4\ub7fd\uac8c \uc774\uc5b4\uc11c \uc791\uc131\ud569\ub2c8\ub2e4. </li> </ul> <p>Base Model \uc758 \uc608\uc2dc - \ud76c\ub9dd\ud3b8</p> <ul> <li>\ubc18\uba74, \uc544\ub798\uc640 \uac19\uc774 \ub300\ud654\ud615\uc2dd\uc73c\ub85c \uc9c8\ubb38\ud558\uba74 \uc644\uc804\ud788 \ub9dd\uac00\uc9c0\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc785\ub2c8\ub2e4. </li> </ul> <p>Base Model \uc758 \uc608\uc2dc - \uc808\ub9dd\ud3b8</p> <ul> <li>\uadf8\ub807\ub2e4\uba74, Base Model\uc740 \uc5b4\ub514\uc5d0 \uc368\uc57c\ud560\uae4c\uc694? \uc0ac\uc2e4\uc0c1 \ubc14\ub85c \uc0ac\uc6a9\ud558\uae30\ub294 \uc5b4\ub835\uc2b5\ub2c8\ub2e4. \ub300\ubd80\ubd84 \ud30c\uc778\ud29c\ub2dd\uc758 \uc7ac\ub8cc\ub85c \ud65c\uc6a9\ub429\ub2c8\ub2e4. \uc54c\uc544\uc11c \ud29c\ub2dd\ud574\uc11c \uac00\uc838\ub2e4 \uc4f0\ub77c\ub294 \uc6a9\ub3c4\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.  </li> </ul>"},{"location":"concepts/model_types/#instruct-model","title":"Instruct Model","text":"<ul> <li>\ub300\ud654 \ud615\uc2dd\uc73c\ub85c \ud29c\ub2dd\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4. \ud604\uc7ac \uc6b0\ub9ac\uac00 \ubcf4\ud1b5 \uc0ac\uc6a9\ud558\ub294 ChatGPT \uc640 \uac19\uc740 \ud615\ud0dc\ub294 \ubaa8\ub450 Instrcut \ud29c\ub2dd\uc774 \uc644\ub8cc\ub41c \ubaa8\ub378\ub4e4 \uc785\ub2c8\ub2e4. </li> <li>\uc704 Base \ubaa8\ub378\uc5d0\uc11c \ub9dd\uac00\uc84c\ub358 \uc9c8\ubb38\uc744 \ub611\uac19\uc774 \ud55c\ubc88 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</li> </ul> <p>Instruct Model \uc758 \uc608\uc2dc</p> <ul> <li>\ub300\ud654 \ud615\uc2dd\uc73c\ub85c \uc544\uc8fc \uc798 \ub300\ub2f5\ud558\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </li> <li>Instruct Model\uc740 \ub300\ud654 \ud615\uc2dd\uc758 \ub300\ub2f5 \ubfd0\ub9cc \uc544\ub2c8\ub77c, tool use \uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uae30\ub2a5\ub4e4\uc774 \ucd94\uac00\ub418\ub294 \uac83\uc774 \uc77c\ubc18\uc801\uc785\ub2c8\ub2e4.</li> <li>\uc0ac\ub78c\uc73c\ub85c \ube44\uc720\ud558\uc790\uba74, \uba54\ub274\uc5bc\uc744 \ub2e4 \ud559\uc2b5\ud574\uc11c \uc77c\ud560 \uc900\ube44\uac00 \ub41c \uacbd\ub825\uc9c1\uc774\ub77c\uace0 \ubcf4\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4, \ud68c\uc0ac\ub4e4\uc774 \uba54\ub274\uc5bc\uc744 \uac01\uac01 \ub2e4\ub974\uac8c \ub9cc\ub4e4\ud14c\ub2c8, \ubaa8\ub378\ub9c8\ub2e4 \uc9c0\uc6d0\ud558\ub294 \uc77c\uc758 \ubc94\uc704\uac00 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4. </li> <li>Llama 3.1 \uc758 \uc608\uc2dc\ub97c \ubcf4\uba74, Instruct \ubaa8\ub378\uc774 \uc5b4\ub5a4 \uae30\ub2a5\uc744 \ucd94\uac00\ub85c \uc9c0\uc6d0\ud558\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Herd of Models \ud398\uc774\ud37c\ub97c \ubcf4\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4.)</li> </ul> <p>Llama 3.1 \uc758 Base Model \uacfc Instruct Model \uc758 \ucc28\uc774</p>"},{"location":"concepts/model_types/#llm_1","title":"LLM \uc6a9\ub3c4\ubcc4 \ubd84\ub958","text":""},{"location":"concepts/model_types/#llm_2","title":"\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378 (LLM)","text":"<ul> <li>\ubc29\ub300\ud55c \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud574 \uc790\uc5f0\uc5b4 \uc0dd\uc131 \ubc0f \uc774\ud574\uc5d0 \ud2b9\ud654\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4, \uc790\uc138\ud55c \uc124\uba85\uc740 \uc5ec\uae30\uc11c\ub294 \uc548\ud574\ub3c4 \ub418\uaca0\uc8e0?</li> </ul>"},{"location":"concepts/model_types/#_1","title":"\uc784\ubca0\ub529 \ubaa8\ub378 (\ubca1\ud130 \uc0dd\uc131)","text":"<ul> <li>\ubb38\uc7a5\uc774\ub098 \uae00\uc744 \uace0\ucc28\uc6d0 \uc22b\uc790 \ubca1\ud130 \ud45c\ud604\uc73c\ub85c \ubcc0\ud658\ud558\uc5ec \uc758\ubbf8\ub97c \ud30c\uc545\ud558\uace0 \ubca1\ud130 \uac80\uc0c9\uc5d0 \ud65c\uc6a9\ub418\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>\ud14d\uc2a4\ud2b8\uc758 \uc758\ubbf8\uc640 \uad00\uacc4\ub97c \ud45c\ud604\ud558\ub294 \ubca1\ud130\ub97c \uc0dd\uc131\ud558\uc5ec, \uc720\uc0ac\ud55c \uc758\ubbf8\uc758 \uae00\ub4e4\ub07c\ub9ac \uac00\uae4c\uc6b4 \ubca1\ud130\ub85c \ub9e4\ud551\ud569\ub2c8\ub2e4.</li> <li>\ud2b9\uc9d5:</li> <li>\uc77c\ubc18\uc801\uc778 LLM\ubcf4\ub2e4 \uacbd\ub7c9\ud654\uac00 \ub418\uc5b4 \uc788\uace0, \ud6e8\uc52c \uc800\ub834\ud558\uace0 \ube60\ub985\ub2c8\ub2e4.   </li> <li>\ubca1\ud130 \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc640 \uacb0\ud569\ud574 \ub300\uaddc\ubaa8 \ubb38\uc11c \uc9d1\ud569\uc5d0\uc11c \uc758\ubbf8 \uae30\ubc18 \uac80\uc0c9\uc744 \uc218\ud589\ud569\ub2c8\ub2e4, RAG \uc758 \ud544\uc218\uc801\uc778 \ub2e8\uacc4\uc778 Semantic Search \uc5d0 \ub9ce\uc774 \uc4f0\uc785\ub2c8\ub2e4.</li> <li>ex. Cohere \uc758 Embed \ubaa8\ub378\uc774\ub098 OpenAI\uc758 text-embedding-3-* \ubaa8\ub378 \ub4f1\uc774 \uc5ec\uae30\uc5d0 \uc18d\ud558\uba70, \uac80\uc0c9\uc774\ub098 \ucd94\ucc9c \uc2dc\uc2a4\ud15c\uc5d0\uc11c \ucffc\ub9ac\uc640 \ubb38\uc11c\uc758 \uc758\ubbf8\uc801 \uc720\uc0ac\ub3c4\ub97c \uacc4\uc0b0\ud558\ub294 \ub370 \uc4f0\uc785\ub2c8\ub2e4.</li> <li>\ucc38\uace0: Openai Embeddings</li> </ul>"},{"location":"concepts/model_types/#_2","title":"\uba40\ud2f0\ubaa8\ub2ec \ubaa8\ub378 (\ud14d\uc2a4\ud2b8 + \uc774\ubbf8\uc9c0/\uc74c\uc131)","text":"<ul> <li>\ud14d\uc2a4\ud2b8\uc640 \uc774\ubbf8\uc9c0, \uc74c\uc131 \ub4f1 \ubcf5\uc218\uc758 \ub370\uc774\ud130 \ud615\ud0dc\ub97c \ub3d9\uc2dc\uc5d0 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>\uc774\ubbf8\uc9c0 \ucea1\uc158 \uc0dd\uc131, \uc74c\uc131 \uc778\uc2dd \ubc0f \ud569\uc131, \ube44\ub514\uc624 \uc774\ud574 \ub4f1 \ub2e4\uc591\ud55c modality\ub97c \uc5b8\uc5b4\uc640 \ud568\uaed8 \ub2e4\ub8f9\ub2c8\ub2e4.</li> <li>ex.<ul> <li>LLaVA: Vision Encoder\ub97c LLM \uc5d0 \uacb0\ud569\ud574 \uc774\ubbf8\uc9c0\ub97c \uc774\ud574\ud558\uace0 \uc124\uba85\ud558\ub294 \uc624\ud508\uc18c\uc2a4 \ucc57\ubd07 \ubaa8\ub378\ub85c, \ud14d\uc2a4\ud2b8-\uc774\ubbf8\uc9c0 \uc870\ud569 \uc9c0\uc2dc\uc5d0\ub3c4 \uc751\ub2f5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>OpenAI\uc758 GPT-4o: \ud14d\uc2a4\ud2b8 \ubfd0\ub9cc \uc544\ub2c8\ub77c \uc774\ubbf8\uc9c0 \uc785\ub825\uc744 \ubc1b\uc544 \ucc98\ub9ac\ud560 \uc218 \uc788\uace0, \uc74c\uc131\uc740 In/Out\uc774 \ubaa8\ub450 \ub429\ub2c8\ub2e4, o1 \ub3c4 \uc774\ubbf8\uc9c0 \uc778\ud48b\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. </li> <li>\uad6c\uae00\uc758 Gemini: \ud14d\uc2a4\ud2b8, \uc774\ubbf8\uc9c0, \uc624\ub514\uc624, \ube44\ub514\uc624 \uae4c\uc9c0 \ub2e4\uc591\ud55c \uc785\ub825\uc744 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> </ul>"},{"location":"concepts/model_types/#reasoning-model","title":"\ucd94\ub860 \ud2b9\ud654 \ubaa8\ub378 (Reasoning Model)","text":"<ul> <li>\ubcf5\uc7a1\ud55c \ubb38\uc81c \ud574\uacb0\uacfc \ub17c\ub9ac\uc801 \ucd94\ub860\uc5d0 \uc911\uc810\uc744 \ub454 LLM\ub4e4\uc785\ub2c8\ub2e4.</li> <li>LLM\uc5d0 Chain-of-Thought \uc640 \ube44\uc2b7\ud55c \ud615\uc2dd\uc758 reasnong \ucd94\ub860\uc744 \ud558\ub3c4\ub85d \ub9cc\ub4e0 \ubaa8\ub378\uc785\ub2c8\ub2e4. \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c\ub294 \ub4a4\uc5d0\uc11c \uc124\uba85\ud569\ub2c8\ub2e4.</li> <li>\ud2b9\uc9d5:</li> <li>\uc218\ud559 \uacc4\uc0b0, \ud37c\uc990 \ud480\uc774, \ubcf5\uc7a1\ud55c \uc758\uc0ac\uacb0\uc815 \ub4f1 \uc624\ub79c \uc0ac\uc0c9\uc774 \ud544\uc694\ud55c \ud0dc\uc2a4\ud06c\uc5d0 \uac15\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc0dd\uac01\uc744 \uc624\ub798\ud558\uae30 \ub584\ubb38\uc5d0 \ub9ce\uc774 \ub290\ub9bd\ub2c8\ub2e4.</li> <li>Ex. <ul> <li>OpenAI\uc758 o1, o3: \ubcf5\uc7a1\ud55c \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c GPT-4, GPT-4.5\ubcf4\ub2e4 \uc6d4\ub4f1\ud55c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.</li> <li>Anthropic\uc758 Claude-3.7-sonent-thinking, Google\uc758 Gemini 2.0 Flash Thinking</li> <li>DeepSeek-R1, Alibaba\uc758 QWQ, ... \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> </ul>"},{"location":"concepts/model_types/#diffusion-llm-vla","title":"\uae30\ud0c0 \ud2b9\uc218 \ubaa8\ub378 (Diffusion LLM, VLA \ub4f1)","text":"<ul> <li>\ud2b9\uc218 \ubaa9\uc801\uc73c\ub85c \uc124\uacc4\ub41c \ubcc0\ud615 LLM \uc720\ud615\ub4e4\ub3c4 \ub4f1\uc7a5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc720\ud615:<ul> <li>Diffusion LLM: \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ub418\ub358 \ud655\uc0b0 \ubaa8\ub378(diffusion) \uac1c\ub150\uc744 \ud14d\uc2a4\ud2b8 \uc0dd\uc131\uc5d0 \uc801\uc6a9\ud55c \uc0c8\ub85c\uc6b4 \uc720\ud615\uc73c\ub85c, Auto Regressive \ubc29\uc2dd\uc758 \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\ub824\ub294 \uc2dc\ub3c4\uc785\ub2c8\ub2e4.<ul> <li>ex. Inception Labs\uc758 Mercury Coder\ub294 \uc138\uacc4 \ucd5c\ucd08 \uc0c1\uc6a9 \uaddc\ubaa8 \ud655\uc0b0 LLM\uc73c\ub85c, \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 5~10\ubc30 \ube60\ub978 \ucd08\ub2f9 1000 \ud1a0\ud070 \uc774\uc0c1\uc758 \uc0dd\uc131 \uc18d\ub3c4\ub97c \uc2e4\ud604\ud588\ub2e4\uace0 \ubc1c\ud45c\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> </ul> </li> <li>VLA(Vision-Language-Action) \ubaa8\ub378: \ub85c\ubd07 \uc81c\uc5b4\ub97c \ubaa9\ud45c\ub85c \uc2dc\uac01\uacfc \uc5b8\uc5b4 \uc815\ubcf4\ub97c \ud589\ub3d9 \uc9c0\uc2dc\ub85c \ubcc0\ud658\ud558\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>ex. Physical Intelligence\uc758 Pi0, FigureAI\uc758 Helix</li> <li>\ucf54\ub4dc \uc0dd\uc131 \uc804\uc6a9 LLM: \ud504\ub85c\uadf8\ub798\ubc0d \ucf54\ub4dc \uc0dd\uc131\uc5d0 \ud2b9\ud654\ub41c \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> </ul> </li> </ul>"},{"location":"concepts/model_types/#_3","title":"\uc624\ud508 \uc6e8\uc774\ud2b8 \ubaa8\ub378\uacfc \uc0c1\uc5c5\uc6a9 \ubaa8\ub378","text":""},{"location":"concepts/model_types/#llm_3","title":"\uc8fc\uc694 \uacf5\uac1c LLM (\uae00\ub85c\ubc8c)","text":"<ul> <li> <p>Meta\uc758 LLaMA: \uba54\ud0c0 AI\uac00 \uacf5\uac1c\ud55c \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378 \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4.</p> <ul> <li>\ucd5c\uc2e0 \ubc84\uc804\uc778 LLaMA 3.3\uc740 2024\ub144 9\uc6d4 \ucd9c\uc2dc, 3.X \ubc84\uc804\uc5d0\ub294 1b \ubd80\ud130 405b \uae4c\uc9c0 \ub2e4\uc591\ud558\uac8c \uc788\uc2b5\ub2c8\ub2e4. <sup>1</sup></li> <li>\"The Llama 3 Herd of Models\" \ub17c\ubb38\uc744 \uacf5\uac1c\ud574\uc11c LLaMA 3 \uc2dc\ub9ac\uc988\uc758 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uacfc \uadf8 \ud2b9\uc131\uc744 \uc0c1\uc138\ud788 \uc124\uba85\ud569\ub2c8\ub2e4, \uadf8\ub7f0\ub370 \ub370\uc774\ud130\ub294 \uacf5\uac1c \uc548\ud588\uc2b5\ub2c8\ub2e4. <sup>2</sup></li> <li>LLaMA 3.3\uc740 \uc5ec\ub7ec \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c GPT-4o, Claude 3.5 Sonnet\uacfc \uacbd\uc7c1\ud558\ub294 \uc218\uc900\uc785\ub2c8\ub2e4. deepseek \ubc1c\ud45c \uc804\uae4c\uc9c0\ub294 \uc624\ud508 \uc6e8\uc774\ud2b8 \ubaa8\ub378\uc758 \ud76c\ub9dd\uc774\uc5c8\uc8e0. <sup>3</sup></li> <li>\ub77c\uc774\uc120\uc2a4: LLaMA 3.3\uc740 \uc5f0\uad6c \ubaa9\uc801\uc73c\ub85c \uc790\uc720\ub86d\uac8c \uc0ac\uc6a9 \uac00\ub2a5\ud558\uba70, \uc0c1\uc5c5\uc801 \uc6a9\ub3c4\ub294 \uc81c\ud55c\uc774 \uc788\uc2b5\ub2c8\ub2e4 (8B \ubaa8\ub378\uc740 \uc6d4 7000\ub9cc \uc0ac\uc6a9\uc790 \uc774\ud558 \uc11c\ube44\uc2a4\uc5d0 \ud5c8\uc6a9).<sup>4</sup></li> </ul> </li> <li> <p>Mistral AI: \ud504\ub791\uc2a4\uc758 AI \uc2a4\ud0c0\ud2b8\uc5c5\uc774 \uac1c\ubc1c\ud55c \uace0\uc131\ub2a5 \uc624\ud508 \uc18c\uc2a4 \ubaa8\ub378 \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4.</p> <ul> <li>Mistral 7B: 2023\ub144 \ucd9c\uc2dc\ub41c 7B \ubaa8\ub378\ub85c, \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc758 \uc0c8\ub85c\uc6b4 \uae30\uc900\uc744 \uc81c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.<sup>5</sup></li> <li>Mixtral 8x7B, Mixtral 8x22B: 2023, 2024\ub144 \ucd9c\uc2dc\ub41c MoE \uc544\ud0a4\ud14d\ucc98 \uae30\ubc18 \ubaa8\ub378\ub85c, 560\uc5b5 \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uacfc \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uba74\uc11c \ucd94\ub860 \ube44\uc6a9\uc740 \uc800\ub834\ud569\ub2c8\ub2e4.<sup>6</sup></li> <li>Mistral Large 2: 2024\ub144 7\uc6d4 \ucd9c\uc2dc\ub41c 123B \ubaa8\ub378\ub85c GPT-4o \uc218\uc900\uc758 \uc131\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4, \uc778\uae30\ub294 \uadf8\ub2e5  \uc778\uac83 \uac19\uc544\uc694. <sup>7</sup></li> <li>\ub77c\uc774\uc120\uc2a4: Apache 2.0 \ub77c\uc774\uc120\uc2a4(7B \ubc0f 8x7B \ubaa8\ub378)\ub85c \uc5f0\uad6c \ubc0f \uc0c1\uc5c5\uc801 \ud65c\uc6a9\uc774 \uc790\uc720\ub86d\uac8c \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>DeepSeek:</p> <ul> <li>DeepSeek-V3: 2024\ub144 12\uc6d4 \ubc1c\ud45c\ub41c MoE \uad6c\uc870\uc758 \ucd5c\uc2e0 \ubaa8\ub378\ub85c \ucd1d 6710\uc5b5 \ud30c\ub77c\ubbf8\ud130 \uc804\ubb38\uac00 \ubaa8\ub4c8\uc744 \ud3ec\ud568\ud558\uba70 \uc9c8\uc758\ub2f9 \uc57d 370\uc5b5 \ud30c\ub77c\ubbf8\ud130\ub9cc \ud65c\uc131\ud654\ub429\ub2c8\ub2e4.<sup>8</sup></li> <li>DeepSeek-R1: \ucd94\ub860 \ud2b9\ud654 \ubaa8\ub378\ub85c \uc2ec\uce35\uc801 \uc0ac\uace0\ub97c \uc694\ud558\ub294 \ubb38\uc81c\uc5d0 \ud6a8\uacfc\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4. 2024\ub144 10\uc6d4 \uc5c5\ub370\uc774\ud2b8\ub41c \ubc84\uc804\uc740 \uc218\ud559 \ubc0f \ubcf5\uc7a1\ud55c \ucd94\ub860 \uc791\uc5c5\uc5d0\uc11c \ub354\uc6b1 \ud5a5\uc0c1\ub41c \uc131\ub2a5\uc744 \ubcf4\uc785\ub2c8\ub2e4.<sup>9</sup></li> <li>\ub77c\uc774\uc120\uc2a4: \ub300\ubd80\ubd84 \ubaa8\ub378\uc774 Apache 2.0 \ub77c\uc774\uc120\uc2a4\ub85c \uc0c1\uc5c5\uc801 \uc774\uc6a9\uc774 \uc790\uc720\ub86d\uc2b5\ub2c8\ub2e4.</li> </ul> </li> <li> <p>HuggingFace Zephyr: Hugging Face \uc8fc\ub3c4\ub85c \ubc1c\ud45c\ub41c \uc624\ud508\uc18c\uc2a4 \ub300\ud654\ud615 \ucc57\ubd07 \uacc4\uc5f4 \ubaa8\ub378\uc785\ub2c8\ub2e4.</p> <ul> <li>7B \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378(Zephyr-7B-\u03b2)\uc774 \uacf5\uac1c\ub418\uc5b4 \uc788\uc73c\uba70, Mistral-7B\ub97c \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4.<sup>10</sup></li> <li>\uc9c1\uc811 \uc120\ud638 \ucd5c\uc801\ud654(Direct Preference Optimization, DPO) \uae30\uc220\uc744 \ud65c\uc6a9\ud574 \uc0ac\uc6a9\uc790 \uc9c0\uc2dc \ub530\ub77c\uac00\uae30 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.<sup>11</sup></li> <li>2023\ub144 10\uc6d4 \ucd9c\uc2dc \ub2f9\uc2dc \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378 \uc911 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \uc9c0\uae08\ub3c4 \uacc4\uc18d \uc5f0\uad6c\uc911\uc785\ub2c8\ub2e4. \ucd5c\uadfc\uc5d4 ORPO \ub97c \uc801\uc6a9\ud55c \ubaa8\ub378\uc774 \ub098\uc654\ub124\uc694.  </li> <li>\uc65c \uc5ec\uae30\uc5d0 \ub123\uc5c8\ub098\uba74\uc694, \ud559\uc2b5 \ub370\uc774\ud130(UltraFeedback)\ub97c \ud22c\uba85\ud558\uac8c \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4. <sup>12</sup></li> <li>\ub77c\uc774\uc120\uc2a4: MIT \ub77c\uc774\uc120\uc2a4\ub85c \uc81c\uacf5\ub418\uc5b4 \ud559\uc220 \ubc0f \uc0c1\uc5c5\uc801 \uc6a9\ub3c4\ub85c \uc790\uc720\ub86d\uac8c \ud65c\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>Cohere Aya Expanse:</p> <ul> <li>Aya: 2024\ub144 \uacf5\uac1c\ub41c \ub2e4\uad6d\uc5b4 \ud2b9\ud654 \uc624\ud508 LLM\uc73c\ub85c, \ub2e4\uad6d\uc5b4 \uc9c0\uc6d0\uc744 \uc798 \ud558\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4. \uc2e4\uc81c\ub85c \uc81c \uccb4\uac10 \uc131\ub2a5\uc0c1 \ud55c\uad6d\uc5b4\ub97c \uc815\ub9d0 \uc798 \ud569\ub2c8\ub2e4. <sup>13</sup></li> <li>24\ub144 \ub9d0 SK \uadf8\ub8f9\uc5d0\uc11c \uc9c4\ud589\ub41c \ud30c\uc778\ud29c\ub2dd \uac15\uc758 (\ubcf8 \uc790\ub8cc\ub97c \ud65c\uc6a9\ud588\uc74c) \uc5d0\uc11c, 30B \uc774\ud558 \uc218\uc900 \ubaa8\ub378\ub4e4 \uc911 \uac00\uc7a5 \uc88b\uc740 \ud55c\uad6d\uc5b4 \uc131\ub2a5\uc744 \ubcf4\uc600\uc5c8\uc2b5\ub2c8\ub2e4. </li> <li>\ucc38\uace0\ub85c Cohere \ub294 LLM \ubcf4\ub2e4\ub3c4 \uc784\ubca0\ub529 \ubaa8\ub378\uc774\ub098 \ud2b9\ud788 Rerank \ubaa8\ub378\ub85c \uc720\uba85\ud569\ub2c8\ub2e4. \uc800\ub3c4 \uac1c\uc778\uc801\uc73c\ub85c \ub9cc\ub4e0 \uc11c\ube44\uc2a4\uc5d0 Cohere Reranker \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. </li> </ul> </li> <li> <p>\uc54c\ub9ac\ubc14\ubc14 Qwen: \uc54c\ub9ac\ubc14\ubc14\uc758 \ud1b5\uc6a9\uc5b8\uc5b4\uc778\uacf5\uc9c0\ub2a5\uae30\uc220\uc5f0\uad6c\uc6d0(DAMO Academy)\uc774 \uac1c\ubc1c\ud55c \ub2e4\uad6d\uc5b4 \ubaa8\ub378\uc785\ub2c8\ub2e4.<sup>14</sup></p> <ul> <li>Qwen2.5: 2024\ub144 8\uc6d4 \ucd9c\uc2dc\ub41c \ucd5c\uc2e0 \ubc84\uc804\uc73c\ub85c 0.5B\ubd80\ud130 72B\uae4c\uc9c0 \ub2e4\uc591\ud55c \ud06c\uae30\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4.</li> <li>Qwen2.5-VL: \ube44\uc804-\uc5b8\uc5b4 \uba40\ud2f0\ubaa8\ub2ec \ubaa8\ub378\ub85c \uc774\ubbf8\uc9c0 \uc774\ud574 \ubc0f \ubd84\uc11d \ub2a5\ub825\uc774 \ub6f0\uc5b4\ub0a9\ub2c8\ub2e4.</li> <li>Qwen-Coder: \ucf54\ub529\uc5d0 \ud2b9\ud654\ub41c \ubaa8\ub378\ub85c \ub2e4\uc591\ud55c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4 \uc9c0\uc6d0 \ubc0f \ub192\uc740 \ucf54\ub4dc \ud488\uc9c8\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> <li>QWQ: 2024\ub144 9\uc6d4 \ucd9c\uc2dc\ub41c \ucd94\ub860 \ud2b9\ud654 \ubaa8\ub378\ub85c \ubcf5\uc7a1\ud55c \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc774 \uac15\ud654\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ud2b9\ud788 \uc911\uad6d\uc5b4\uc640 \uc601\uc5b4\uc5d0 \uac15\uc810\uc774 \uc788\uc73c\uba70, 7B \ubaa8\ub378\ub3c4 GPT-3.5 \uc218\uc900\uc758 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</li> <li>\ub77c\uc774\uc120\uc2a4: Qwen2.5 \ubaa8\ub378\ub4e4\uc740 \uc624\ud508\uc18c\uc2a4(Tongyi Qianwen License)\ub85c \uc81c\uacf5\ub418\uc5b4 \uc0c1\uc5c5\uc801 \uc774\uc6a9\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>\ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8 Phi: \ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8\uac00 \uac1c\ubc1c\ud55c \uc18c\ud615 \uace0\ud6a8\uc728 \ubaa8\ub378 \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4.<sup>15</sup></p> <ul> <li>Phi-4: 2024\ub144 12\uc6d4 \ucd9c\uc2dc\ub41c \ucd5c\uc2e0 \ubc84\uc804\uc73c\ub85c 4B(Mini), 14B \ud06c\uae30\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4.<sup>16</sup></li> <li>Phi-4 Multimodal: \ud14d\uc2a4\ud2b8\uc640 \uc774\ubbf8\uc9c0\ub97c \ud568\uaed8 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294 \uba40\ud2f0\ubaa8\ub2ec \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>\ub77c\uc774\uc120\uc2a4: MIT \ub77c\uc774\uc120\uc2a4\ub85c \uc5f0\uad6c \ubc0f \uc0c1\uc5c5\uc801 \ubaa9\uc801 \ubaa8\ub450\uc5d0 \uc790\uc720\ub86d\uac8c \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>\uae30\ud0c0 Yi-V2, KIMI \ub4f1 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc774 \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4. <sup>17</sup></p> </li> </ul>"},{"location":"concepts/model_types/#_4","title":"\ud55c\uad6d\uc5b4 \ud2b9\ud654 \uacf5\uac1c \ubaa8\ub378","text":"<ul> <li> <p>LG AI Research Exaone: LG AI\uc5f0\uad6c\uc6d0\uc774 \uac1c\ubc1c\ud55c \ud55c\uad6d\uc5b4/\uc601\uc5b4 \ubaa8\ub378\uc785\ub2c8\ub2e4. <sup>18</sup></p> <ul> <li>2024\ub144 Exaone 3.5 \ubc84\uc804\uc744 \uc624\ud508 \uc6e8\uc774\ud2b8\ub85c \uacf5\uac1c\ub418\uc5c8\uace0\uc694, \ud3c9\uc774 \uaf64 \uc88b\uc2b5\ub2c8\ub2e4. </li> <li>2.4B, 7.8B, 32B 3\uac00\uc9c0 \ubc84\uc804\uc73c\ub85c \uc81c\uacf5\ub418\uba70, \ucd5c\ub300 100\ud398\uc774\uc9c0 \ubd84\ub7c9\uc758 \uae34 \ubb38\uc11c\ub3c4 \ud55c \ubc88\uc5d0 \ucc98\ub9ac\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>\uc5c5\uc2a4\ud14c\uc774\uc9c0 SOLAR: \uc5c5\uc2a4\ud14c\uc774\uc9c0\uac00 \uac1c\ubc1c\ud55c \ud55c\uad6d \uc2a4\ud0c0\ud2b8\uc5c5 \ucd5c\ucd08(?)\uc758 \uae00\ub85c\ubc8c \uacbd\uc7c1\ub825\uc774 \uc788\ub294 \ubaa8\ub378\uc785\ub2c8\ub2e4. <sup>19</sup></p> <ul> <li>2023\ub144 8\uc6d4 \ub77c\ub9c82 70B\ub97c \uae30\ubc18\uc73c\ub85c \ud55c SOLAR 70B\ub97c \uacf5\uac1c\ud574 HuggingFace Open LLM Leaderboard 1\uc704\ub97c \uae30\ub85d\ud588\uc2b5\ub2c8\ub2e4.</li> <li>2023\ub144 12\uc6d4\uc5d0\ub294 \uc790\uccb4 \uac1c\ubc1c\ud55c SOLAR 10.7B\ub97c \uacf5\uac1c\ud574 \ub2e4\uc2dc \ud55c \ubc88 \uc624\ud508\uc18c\uc2a4 LLM \uc138\uacc4 1\uc704\uc5d0 \uc62c\ub790\uc2b5\ub2c8\ub2e4.<ul> <li>Benchmark \uc21c\uc704\uc5d0 \ub300\ud55c \ud6a8\uc6a9\uc740 \ub17c\ub780\uc758 \uc5ec\uc9c0\uac00 \uc788\uc9c0\ub9cc, \uc5b4\uca0c\ub4e0 1\ub4f1\uc744 \ud588\uace0 \uaf64 \ub6f0\uc5b4\ub09c \uac74 \uc774\uacac\uc758 \uc5ec\uc9c0\uac00 \uc5c6\ub294 \uac83 \uac19\uc544\uc694.</li> </ul> </li> <li>2024\ub144 \ub9d0\uc5d0\ub294 pro preview \uac00 \ub098\uc654\ub294\ub370, (2025\ub144 3\uc6d4 \uc791\uc131\uc77c \uae30\uc900) \uace7 \ub098\uc62c \uac83\uc778\uac00 \ubd05\ub2c8\ub2e4.  </li> </ul> </li> <li> <p>\uce74\uce74\uc624 Kanana: \uce74\uce74\uc624\uac00 2024\ub144 \uacf5\uac1c\ud55c \ud55c\uad6d\uc5b4-\uc601\uc5b4 \uc774\uc911\uc5b8\uc5b4 LLM \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4. <sup>20</sup></p> <ul> <li>\ub0ae\uc740 \uacc4\uc0b0 \ube44\uc6a9\uc73c\ub85c \ud55c\uad6d\uc5b4\uc5d0\uc11c \ucd5c\uace0 \uc131\ub2a5\uc744 \ub0b4\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4.</li> <li>\uace0\ud488\uc9c8 \ub370\uc774\ud130 \uc120\ubcc4, \uacc4\ub2e8\uc2dd \uc0ac\uc804\ud559\uc2b5, Depth up-scaling, \uac00\uc9c0\uce58\uae30 \ubc0f \uc99d\ub958 \ub4f1\uc758 \uae30\ubc95\uc73c\ub85c \ud6a8\uc728\uc131\uacfc \uc131\ub2a5\uc744 \ubaa8\ub450 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc800 \uac1c\uc778\uc801\uc73c\ub85c\ub294 \uac1c\ubc1c\uae30\ub97c \uaf64\ub098 \uc790\uc138\ud558\uac8c \uc798 \uacf5\uac1c\ud574\uc11c \ubcf4\uae30 \ucc38 \uc88b\uc558\uace0, \uc778\uc0c1 \uae4a\uc5c8\uc2b5\ub2c8\ub2e4. \ub530\ub77c\ud574\ubcf4\uace0 \uc2f6\ub354\ub77c\uace0\uc694.  <sup>22</sup></li> </ul> </li> </ul>"},{"location":"concepts/model_types/#_5","title":"\uc0c1\uc5c5\uc6a9 \ubaa8\ub378","text":"<ul> <li> <p>OpenAI GPT, o1 \uc2dc\ub9ac\uc988: OpenAI\uc758 \ub300\ud45c\uc801\uc778 \uc0c1\uc5c5\uc6a9 LLM \uad70\uc73c\ub85c, \ud604\uc7ac \uac00\uc7a5 \ub110\ub9ac \uc4f0\uc774\ub294 ChatGPT\uc758 \uae30\ubc18\uc785\ub2c8\ub2e4.</p> <ul> <li>GPT-3.5(Turbo)\ub294 1750\uc5b5 \ud30c\ub77c\ubbf8\ud130\uae09 \ubaa8\ub378\ub85c \ub300\ud654\ud615 AI\uc5d0 \ud65c\uc6a9\ub429\ub2c8\ub2e4.</li> <li>GPT-4\ub294 \uba40\ud2f0\ubaa8\ub2ec \uc785\ub825(\uc774\ubbf8\uc9c0+\ud14d\uc2a4\ud2b8)\uc744 \ucc98\ub9ac\ud558\uace0 \ucd9c\ub825\ud560 \uc218 \uc788\uc73c\uba70, \uc0ac\ub840 \uae30\ubc18 \ucd94\ub860 \ub2a5\ub825\uacfc \uc0ac\uc2e4\uc801 \uc751\ub2f5 \uba74\uc5d0\uc11c GPT-3.5 \ub300\ube44 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ubaa8\uc758 \uc0ac\ubc95\uc2dc\ud5d8(bar exam)\uc5d0\uc11c GPT-4\ub294 \uc0c1\uc704 10% \uc218\uc900 \uc810\uc218\ub85c \ud569\uaca9\uad8c\uc5d0 \ub4e0 \ubc18\uba74, GPT-3.5\ub294 \ud558\uc704 10%\uc5d0 \uadf8\ucce4\uc2b5\ub2c8\ub2e4.</li> <li>\ud3d0\uc1c4\ud615 \ubaa8\ub378\ub85c\uc11c OpenAI API\ub098 ChatGPT \uc11c\ube44\uc2a4\ub97c \ud1b5\ud574\uc11c\ub9cc \uc811\uadfc \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> </ul> </li> <li> <p>Google Gemini \uc2dc\ub9ac\uc988: Google DeepMind\uac00 2023\ub144 \ubc1c\ud45c\ud55c \ucd5c\uc2e0 \uac70\ub300 \ubaa8\ub378 \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4.</p> <ul> <li>Ultra, Pro, Nano\uc758 3\uac00\uc9c0 \ud06c\uae30\ub85c \ucd5c\uc801\ud654\ub418\uc5b4 \uc81c\uacf5\ub429\ub2c8\ub2e4.</li> <li>Ultra\ub294 \uac00\uc7a5 \ud06c\uace0 \ubcf5\uc7a1\ud55c \uc791\uc5c5\uc6a9, Pro\ub294 \ubc94\uc6a9 \ubaa9\uc801\uc6a9, Nano\ub294 \ubaa8\ubc14\uc77c \uae30\uae30\uc5d0\uc11c\ub3c4 \ub3d9\uc791 \uac00\ub2a5\ud55c \ud6a8\uc728 \ubaa8\ub378\uc785\ub2c8\ub2e4.</li> <li>\ub300\uaddc\ubaa8 \uc774\ubbf8\uc9c0/\uc601\uc0c1 \uc774\ud574\uc640 \uc218\ud559 \ubb38\uc81c \ud480\uc774\uc5d0\uc11c Gemini Ultra\uac00 \uae30\uc874 SOTA\ub97c \ub118\uc5b4\uc11c\ub294 \uc131\ub2a5\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4.</li> <li>Gemini \ub294 \uc624\ud508 \ubc84\uc804 Gemma \ub97c \ud3ec\ud568\ud574\uc11c  Gemini 2.0 Flash, Gemini 2.0 Flash Thinking \ub4f1 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ub610\ud55c \uad6c\uae00 \ub525\ub9c8\uc778\ub4dc\uc5d0\ub294 AlphaGeometry, AlphaProof, AlphaFold \ub4f1 \ub2e4\uc591\ud55c \ud2b9\uc218 \ubaa8\ub378\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> <li> <p>Anthropic Claude \uc2dc\ub9ac\uc988: OpenAI \ucd9c\uc2e0\ub4e4\uc774 \uc124\ub9bd\ud55c Anthropic\uc774 \uac1c\ubc1c\ud55c LLM \uc2dc\ub9ac\uc988\uc785\ub2c8\ub2e4. <sup>21</sup></p> <ul> <li>\ucc3d\uc758\uc801 \uae00\uc4f0\uae30, \ucf54\ub529, \ubcf5\uc7a1\ud55c \uc9c8\uc758 \uc751\ub2f5\uc5d0\uc11c GPT \uc2dc\ub9ac\uc988\uc5d0 \uc900\ud558\ub294 \uc131\ub2a5\uc744 \ubaa9\ud45c\ub85c \ud569\ub2c8\ub2e4.</li> <li>Claude 3.7 sonnet \uc774 2025\ub144 2\uc6d4 \ucd9c\uc2dc\ub418\uc5c8\uace0, thinking \ubc84\uc804\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.</li> <li>\uac1c\ubc1c\uc790 \ubd84\ub4e4\uc740 GPT \ubcf4\ub2e4 Claude \ub97c \uc120\ud638\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc800\ub3c4 \uadf8\ub807\uace0, \uac1c\uc778\uc801\uc778 \ud3c9\uc73c\ub85c\ub294 \ud655\uc2e4\ud788 \ucf54\ub4dc\ub97c \ub354 \uc798 \uc9dc\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4. </li> </ul> </li> </ul> <ol> <li> <p>Meta AI. (2024). \"LLaMA 3.3\". Meta \u21a9</p> </li> <li> <p>Meta AI. (2024). \"The Llama 3 Herd of Models\". arXiv:2407.21783 \u21a9</p> </li> <li> <p>Meta AI. (2024). \"LLaMA 3.3 Performance Benchmarks\". Meta AI \u21a9</p> </li> <li> <p>Meta AI. (2024). \"LLaMA 3.3 License\". Meta AI \u21a9</p> </li> <li> <p>Mistral AI. (2023). \"Mistral 7B\". Mistral AI Blog \u21a9</p> </li> <li> <p>Mistral AI. (2024). \"Mixtral of Experts\". Mistral AI Blog \u21a9</p> </li> <li> <p>Mistral AI. (2024). \"Mistral Large 2\". Mistral AI Blog \u21a9</p> </li> <li> <p>DeepSeek. (2024). \"DeepSeek-V3\". DeepSeek github \u21a9</p> </li> <li> <p>DeepSeek. (2024). \"DeepSeek-R1\". DeepSeek github \u21a9</p> </li> <li> <p>HuggingFace. (2023). \"Zephyr-7B-\u03b2\". HuggingFace Models \u21a9</p> </li> <li> <p>Tunstall, L. et al. (2023). \"Zephyr: Direct Distillation of LM Alignment\". arXiv:2310.16944 \u21a9</p> </li> <li> <p>HuggingFace. (2023). \"UltraFeedback Dataset\". HuggingFace Datasets \u21a9</p> </li> <li> <p>Cohere For AI. (2024). \"Introducing Aya\". Cohere For AI Blog \u21a9</p> </li> <li> <p>Alibaba DAMO Academy. (2024). \"Qwen2.5\". GitHub \u21a9</p> </li> <li> <p>Microsoft. (2024). \"Phi Series\". Microsoft Research \u21a9</p> </li> <li> <p>Microsoft. (2024). \"Introducing Phi-4\". Microsoft HuggingFace \u21a9</p> </li> <li> <p>01.AI. (2024). \"Yi-V2\". GitHub \u21a9</p> </li> <li> <p>LG AI Research. (2024). \"EXAONE-3.5\". GitHub \u21a9</p> </li> <li> <p>Upstage. (2023). \"SOLAR 10.7B Emerges as World's Top Pre-trained LLM\". Upstage News \u21a9</p> </li> <li> <p>Kakao. (2024). \"Kanana\". GitHub \u21a9</p> </li> <li> <p>Anthropic. (2024). \"Claude 3.7 Sonnet\". Anthropic News \u21a9</p> </li> <li> <p>Kakao. (2024). \"Kanana\". Kakao Tech \u21a9</p> </li> </ol>"},{"location":"data_preparation/dataset_creation/","title":"\ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130\uc14b \uc900\ube44","text":""},{"location":"data_preparation/dataset_creation/#_2","title":"\ud30c\uc778\ud29c\ub2dd\uc744 \uc704\ud55c \uc720\ud615\ubcc4 \uacf5\uac1c \ub370\uc774\ud130\uc14b","text":"<p>\ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130\uc14b\uc740 \ud06c\uac8c \ub450 \uac00\uc9c0 \uc720\ud615\uc73c\ub85c \ub098\ub20c \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p>"},{"location":"data_preparation/dataset_creation/#1-supervised-fine-tuning","title":"1. \uc9c0\ub3c4 \ud559\uc2b5(Supervised Fine-Tuning) \ub370\uc774\ud130\uc14b","text":"<p>\uc9c0\ub3c4 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc740 \ubca0\uc774\uc2a4 \ubaa8\ub378\uc774 \uc0ac\uc6a9\uc790 \uc9c0\uc2dc\uc5d0 \ub530\ub77c \uc751\ub2f5\ud558\ub294 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc8fc\ub85c \uc9c8\ubb38-\ub2f5\ubcc0 \uc30d\uc774\ub098 \ub300\ud654 \ud615\ud0dc\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.</p>"},{"location":"data_preparation/dataset_creation/#openassistant-conversations-oasst1","title":"OpenAssistant Conversations (OASST1)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: OpenAssistant \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c \ud06c\ub77c\uc6b0\ub4dc\uc18c\uc2f1\uc73c\ub85c \uc218\uc9d1\ud55c \ub300\ud654\ud615 \uc9c0\uc2dc \uc751\ub2f5 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uc804\uc138\uacc4 \uc790\uc6d0\ubd09\uc0ac\uc790\ub4e4\uc774 OpenAssistant \ud50c\ub7ab\ud3fc\uc5d0\uc11c \uc0ac\uc6a9\uc790 \uc9c8\ubb38\uacfc \uc774\uc5d0 \ub300\ud55c \uc5b4\uc2dc\uc2a4\ud134\ud2b8 \uc751\ub2f5\uc744 \uc9c1\uc811 \uc791\uc131\ud558\uace0 \ud3c9\uac00\ud558\uc5ec \uad6c\ucd95\ud588\uc2b5\ub2c8\ub2e4. </li> <li>\uaddc\ubaa8\uc640 \ud2b9\uc9d5: \uc57d 161,443\uac1c\uc758 \uba54\uc2dc\uc9c0(\uc0ac\uc6a9\uc790/\uc2dc\uc2a4\ud15c/\uc5b4\uc2dc\uc2a4\ud134\ud2b8 \ubc1c\ud654)\uc640 35\uac1c \uc5b8\uc5b4\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub300\ud654 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70, \uac01 \ubc1c\ud654\uc5d0\ub294 461,292\uac74\uc758 \ud488\uc9c8 \ud3c9\uac00\uac00 \ub2ec\ub824 \ucd1d 10,000\uac1c \uc774\uc0c1\uc758 \ub300\ud654 \ud2b8\ub9ac\ub97c \ud615\uc131\ud569\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: OASST1\uc740 ChatGPT \ub300\uc548\uc774 \ub420 \uc218 \uc788\ub294 \uacf5\uac1c \ucc57\ubd07\uc744 \ud6c8\ub828\ud558\ub294 \ub370 \ud3ed\ub113\uac8c \uc4f0\uc600\uc2b5\ub2c8\ub2e4. EleutherAI\uc758 Pythia-12B \ubaa8\ub378\uc744 OASST1\ub85c \ubbf8\uc138\uc870\uc815\ud55c oasst-sft-4-pythia-12b\uac00 \uacf5\uac1c\ub418\uc5c8\uace0, OpenAssistant \uc790\uccb4\ub3c4 \uc774 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud574 30\uc5b5~120\uc5b5 \uaddc\ubaa8 \ubaa8\ub378\uc744 \ud29c\ub2dd\ud588\uc2b5\ub2c8\ub2e4. \ud488\uc9c8\uc774 \ub192\uace0 \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc5b4, \uc774\ud6c4 \uc5ec\ub7ec \uc624\ud508\uc18c\uc2a4 ChatGPT\ub958 \ubaa8\ub378(\uc608: XLM, Falcon \ubbf8\uc138\ud29c\ub2dd \ub4f1)\uc758 \uc9c0\ub3c4\ud29c\ub2dd \ub2e8\uacc4\uc5d0 OASST \ub370\uc774\ud130\uac00 \uc0ac\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#stanford-alpaca-52k-instruction-corpus","title":"Stanford Alpaca (52K Instruction Corpus)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: Stanford Alpaca\ub294 LLaMA 7B\ub97c \uc9c0\uc2dc \ub530\ub77c\ud558\uae30 \ubaa8\ub378\ub85c \ub9cc\ub4e4\uba70 \uc18c\uac1c\ub41c 52,000\uac1c \uaddc\ubaa8\uc758 \uc601\uc5b4 \uc9c0\uc2dc-\uc751\ub2f5 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. Stanford \ud300\uc740 \uba3c\uc800 Self-Instruct \ubc29\ubc95\ub860\uc744 \ub530\ub77c 175\uac1c\uc758 \uc778\uac04 \uc791\uc131 \uc2dc\ub4dc \uc9c0\uc2dc\ubb38\uc744 \uc900\ube44\ud55c \ub4a4, OpenAI\uc758 text-davinci-003 (GPT-3.5) \ubaa8\ub378\uc744 \ud65c\uc6a9\ud574 \uc0c8\ub85c\uc6b4 \uc9c0\uc2dc\ubb38\uacfc \ub2f5\ubcc0\uc744 \uc790\ub3d9 \uc0dd\uc131\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc0dd\uc131 \ubc29\uc2dd: GPT-3.5\uc5d0 \uc2dc\ub4dc \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc8fc\uace0 \uc0c8\ub85c\uc6b4 \uc9c0\uc2dc\ub97c \ub9cc\ub4e4\uac8c \ud55c \ub2e4\uc74c, \uadf8 \uc9c0\uc2dc\uc5d0 \ub300\ud55c \ubaa8\ubc94 \ub2f5\ubcc0\uc744 \ub2e4\uc2dc GPT-3.5\ub85c \uc0dd\uc131\ud558\ub294 \uc2dd\uc73c\ub85c \ud30c\uc774\ud504\ub77c\uc778\uc744 \ub2e8\uc21c\ud654\ud558\uc5ec 52K\uc30d\uc758 \uace0\ud488\uc9c8 \uc2dc\uc5f0 \ub370\uc774\ud130\ub97c \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130 \uc0dd\uc131 \ube44\uc6a9\uc740 \uc57d $500 \uc0c1\ub2f9\uc758 OpenAI API \uc0ac\uc6a9\uc73c\ub85c \ucd94\uc0b0\ub429\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: \uc774 \ub370\uc774\ud130\ub85c LLaMA-7B\ub97c 3\uc2dc\uac04 \ub9cc\uc5d0 \ubbf8\uc138\uc870\uc815\ud55c \uacb0\uacfc, OpenAI\uc758 text-davinci-003\uc640 \uc720\uc0ac\ud55c \uac70\uc9d3\ub9d0 \ubc0f \ucd94\ub860 \ub2a5\ub825\uc744 \ubcf4\uc774\ub294 Alpaca 7B \ubaa8\ub378\uc774 \ud0c4\uc0dd\ud588\uc2b5\ub2c8\ub2e4. Alpaca\uc758 \uc131\uacf5\uc740 \uc800\ube44\uc6a9 \ud559\uc2b5\uc73c\ub85c\ub3c4 \uac15\ub825\ud55c \uc9c0\uc2dc \ub530\ub974\uae30 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5b4 \ud070 \ubc18\ud5a5\uc744 \uc77c\uc73c\ucf30\uc2b5\ub2c8\ub2e4. \uc774\ud6c4 Alpaca \ub370\uc774\ud130\ub294 Alpaca-LoRA \ub4f1 \uacbd\ub7c9\ud654 \ubaa8\ub378\uc774\ub098 \ub2e4\ub978 LLM (\uc608: GPT-J \ub4f1)\uc758 \uc9c0\uc2dc\ud29c\ub2dd\uc5d0 \uc0ac\uc6a9\ub418\uc5c8\uace0, \ub9ce\uc740 \ud6c4\uc18d \uc5f0\uad6c\ub4e4\uc774 Alpaca \ubc29\uc2dd\uc744 \ucc38\uace0\ud558\uc5ec \uc790\uccb4 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub294 \uacc4\uae30\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub2e4\ub9cc Alpaca \ub370\uc774\ud130\ub294 OpenAI \uc751\ub2f5\uc744 \ud3ec\ud568\ud558\ubbc0\ub85c \uc0c1\uc5c5\uc801 \uc0ac\uc6a9\uc5d0\ub294 \uc81c\uc57d\uc774 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#databricks-dolly-15k-human-generated-instructions","title":"Databricks Dolly 15K (Human-Generated Instructions)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: Dolly 2.0 \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud574 Databricks\uc0ac\uac00 \uacf5\uac1c\ud55c 15,000\uac1c \uaddc\ubaa8\uc758 \uc778\uac04 \uc0dd\uc131 \uc9c0\uc2dc-\uc751\ub2f5 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. 5\ucc9c \uba85 \uc774\uc0c1\uc758 Databricks \uc9c1\uc6d0\ub4e4\uc774 2023\ub144 3~4\uc6d4\uc5d0 \uac78\uccd0 \ub2e4\uc591\ud55c \ud615\uc2dd\uc758 \ud504\ub86c\ud504\ud2b8\uc640 \uc774\uc5d0 \ub300\ud55c \uc751\ub2f5\uc744 \uc9c1\uc811 \uc791\uc131\ud558\uc5ec \ubaa8\uc558\uc2b5\ub2c8\ub2e4.</li> <li>\ub0b4\uc6a9\uacfc \ud2b9\uc9d5: \ube0c\ub808\uc778\uc2a4\ud1a0\ubc0d, \ucf58\ud150\uce20 \uc0dd\uc131, \uc815\ubcf4 \uac80\uc0c9, \uc694\uc57d \ub4f1 \uad11\ubc94\uc704\ud55c \uacfc\uc81c \uc720\ud615\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b(databricks-dolly-15k)\uc740 CC BY-SA \ub77c\uc774\uc120\uc2a4\ub85c \ubc30\ud3ec\ub418\uc5b4 \uc5f0\uad6c \ubc0f \uc0c1\uc5c5\uc6a9\uc73c\ub85c \uc790\uc720\ub86d\uac8c \uc0ac\uc6a9 \uac00\ub2a5\ud558\uba70, \uc624\ud508\uc18c\uc2a4 LLM\uc758 ChatGPT\ub958 \uc778\ud130\ub799\ud2f0\ube0c \ud2b9\uc131\uc744 \uc774\ub04c\uc5b4\ub0b4\uae30 \uc704\ud55c \uccab \ub300\uaddc\ubaa8 \uc778\uac04 \ud589\ub3d9 \ub370\uc774\ud130\uc138\ud2b8\ub85c \uc54c\ub824\uc838 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: Dolly 15K\ub294 \uc0c1\uc5c5\uc801 \uc81c\uc57d\uc774 \uc5c6\ub294 \uc810 \ub54c\ubb38\uc5d0 \uc8fc\ubaa9\ubc1b\uc558\uc73c\uba70, \uc774 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud574 EleutherAI\uc758 Pythia-12B\ub97c \ubbf8\uc138\uc870\uc815\ud55c Dolly 2.0 (12B) \ubaa8\ub378\uc774 \uacf5\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Dolly 2.0\uc740 \uc644\uc804 \uacf5\uac1c \ub370\uc774\ud130\ub9cc\uc73c\ub85c \ud559\uc2b5\ub41c \uccab \uc0c1\uc5c5\uac00\ub2a5 ChatGPT \uc720\uc0ac \ubaa8\ub378\ub85c, \uc774\ud6c4 \uae30\uc5c5\ub4e4\uc774 \uc790\uccb4 \ub370\uc774\ud130\ub85c \ub9de\ucda4 LLM\uc744 \uad6c\ucd95\ud558\ub294 \uc608\uc2dc\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. Dolly 15k \uc790\uccb4\ub3c4 \uc5ec\ub7ec \uc624\ud508\uc18c\uc2a4 \ud29c\ub2dd \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c \ucc38\uace0 \ub370\uc774\ud130\ub85c \uc0ac\uc6a9\ub418\uace0 \uc788\uc73c\uba70, \uaddc\ubaa8\ub294 \uc791\uc9c0\ub9cc \ud488\uc9c8 \uac80\uc99d\ub41c \uc778\uac04 \uc0dd\uc131 \uc751\ub2f5\uc73c\ub85c\uc11c \uac00\uce58\uac00 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#sharegpt-vicuna","title":"ShareGPT (Vicuna) \ub300\ud654 \ub370\uc774\ud130","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: Vicuna \ubaa8\ub378\ub85c \uc720\uba85\ud574\uc9c4 ShareGPT \ub300\ud654 \ub370\uc774\ud130\uc14b\uc740, \uc2e4\uc81c ChatGPT \uc0ac\uc6a9\uc790\ub4e4\uc774 \uc6f9\uc0ac\uc774\ud2b8 ShareGPT\uc5d0 \uacf5\uc720\ud55c \ub300\ud654 \uae30\ub85d \uc57d 70,000\uac74\uc744 \ubaa8\uc740 \uac83\uc785\ub2c8\ub2e4. \uac01 \ub370\uc774\ud130\ub294 ChatGPT\uc640 \uc0ac\uc6a9\uc790 \uac04\uc758 \uba40\ud2f0\ud134 \ub300\ud654(\uc608: \uc0ac\uc6a9\uc790 \uc9c8\ubb38\uacfc \uc5f0\uc18d\uc801\uc778 \ucc57\ubd07 \ub2f5\ubcc0\ub4e4)\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \ucc98\ub9ac: \uc218\uc9d1\ud300(LMSYS)\uc740 \uacf5\uc720\ub41c HTML \ub300\ud654\ub97c \ubaa8\uc544 \ub9c8\ud06c\ub2e4\uc6b4 \ud615\uc2dd\uc73c\ub85c \uc815\uc81c\ud558\uace0, \uc74c\ub780\ud558\uac70\ub098 \ubd80\uc801\uc808\ud55c \ub300\ud654\ub97c \ud544\ud130\ub9c1\ud558\uc5ec \ud488\uc9c8\uc744 \ud655\ubcf4\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ub108\ubb34 \uae34 \ub300\ud654\ub294 \ubaa8\ub378 \ub9e5\ub77d\uae38\uc774\uc5d0 \ub9de\uac8c \ubd84\ud560\ud558\uc5ec \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub294 ChatGPT\uc758 \uc2e4\uc81c \ud65c\uc6a9 \uc0ac\ub840\ub97c \ub2f4\uace0 \uc788\uc5b4 \ub2e4\uc591\ud55c \ub3c4\uba54\uc778\uacfc \ud604\uc2e4\uc801 \uc9c8\ubb38\uc774 \ubc18\uc601\ub41c \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: ShareGPT \uae30\ubc18 \ub370\uc774\ud130\ub294 Vicuna-13B \ubaa8\ub378\uc758 \uc9c0\ub3c4\ud559\uc2b5\uc5d0 \ud65c\uc6a9\ub418\uc5b4 \ud070 \uc8fc\ubaa9\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. LLaMA-13B\uc5d0 \uc774 70k \ub300\ud654\ub97c 1\uc77c\uac04 \ubbf8\uc138\uc870\uc815\ud55c Vicuna-13B\ub294, GPT-4 \ud3c9\uac00\uc5d0\uc11c ChatGPT \ub300\ube44 90% \uc218\uc900\uc758 \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4\uace0 \ubcf4\uace0\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Vicuna\uc758 \ub4f1\uc7a5 \uc774\ud6c4 \ub9ce\uc740 \uc624\ud508\ucc57\ubd07 \ud504\ub85c\uc81d\ud2b8\ub4e4\uc774 ShareGPT \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uac70\ub098, \ube44\uc2b7\ud55c \uc0ac\uc6a9\uc790 \ub300\ud654 \ub370\uc774\ud130\ub97c \ubaa8\uc544 \ud559\uc2b5\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. \ub2e4\ub9cc ShareGPT \ub370\uc774\ud130\ub294 \uc0ac\uc6a9\uc790 \uac1c\uc778\uc815\ubcf4\ub098 OpenAI \uc751\ub2f5\uc744 \ud3ec\ud568\ud560 \uac00\ub2a5\uc131\uc774 \uc788\uc5b4 \ub77c\uc774\uc120\uc2a4 \uc774\uc288\uac00 \uc874\uc7ac\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#ultrachat-ai","title":"UltraChat (\ub2e4\uc911\ud134 AI \uc0dd\uc131 \ub300\ud654 \ub370\uc774\ud130)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: UltraChat\uc740 2023\ub144 4\uc6d4 \uacf5\uac1c\ub41c \ub300\uaddc\ubaa8 \uba40\ud2f0\ud134 \ub300\ud654 \ub370\uc774\ud130\uc14b\uc73c\ub85c, \ubaa8\ub4e0 \ub370\uc774\ud130\uac00 AI \ubaa8\ub378\uc5d0 \uc758\ud574 \uc790\ub3d9 \uc0dd\uc131\ub41c \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4. \uc911\uad6d Tsinghua \ub300\ud559 NLP \uadf8\ub8f9\uc5d0\uc11c \uacf5\uac1c\ud55c \uac83\uc73c\ub85c, OpenAI\uc758 GPT-3.5 Turbo API\ub97c \uc774\uc6a9\ud574 \uc0ac\uc6a9\uc790 \uc5ed\ud560\uacfc \uc5b4\uc2dc\uc2a4\ud134\ud2b8 \uc5ed\ud560\uc744 \uc2dc\ubbac\ub808\uc774\uc158\ud558\uc5ec \ub300\ud654\ub97c \ub9cc\ub4e4\uc5b4\ub0c8\uc2b5\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \uad6c\uc870: \uc9c0\uc2dd\uc9c8\ubb38(Q&amp;A), \uae00\uc4f0\uae30/\ucc3d\uc791, \uc8fc\uc5b4\uc9c4 \uc790\ub8cc\uc5d0 \ub300\ud55c \ub3c4\uc6c0\uc758 3\uac00\uc9c0 \uc2dc\ub098\ub9ac\uc624\ub85c \ub098\ub204\uc5b4, \uac01 \uc601\uc5ed\uc5d0\uc11c \uc218\uc2ed\ub9cc \uac74\uc758 \ub300\ud654\ub97c \uc0dd\uc131\ud588\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \"\uc138\uacc4\uc5d0 \ub300\ud55c \uc9c8\ubb38\" \uc139\ud130\uc5d0\uc11c\ub294 \uc77c\ubc18\uc0c1\uc2dd\ubd80\ud130 \uc804\ubb38\uc9c0\uc2dd\uae4c\uc9c0 \uad11\ubc94\uc704\ud55c \uc9c8\ubb38\uc744 \ub9cc\ub4e4\uace0 \uc774\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud588\uace0, \"\uae00\uc4f0\uae30/\ucc3d\uc791\" \uc139\ud130\uc5d0\uc11c\ub294 \uc774\uba54\uc77c \uc791\uc131, \uc2a4\ud1a0\ub9ac \ucc3d\uc791 \ub4f1 \ucc3d\uc758\uc801 \uc694\uccad\uacfc \uadf8 \ub300\uc751\uc744 \uc0dd\uc131\ud558\ub294 \uc2dd\uc785\ub2c8\ub2e4. \uc774\ub807\uac8c \ubaa8\uc778 \ub300\ud654\ub294 \ucd1d 70\ub9cc~80\ub9cc \uac74 \uaddc\ubaa8\ub85c, \ud3c9\uade0 \uba87 \ucc28\ub840\uc758 \uc9c8\uc758\uc751\ub2f5 \ud134\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: UltraChat \ub370\uc774\ud130\ub294 Supervised Fine-tuning(SFT) \ub2e8\uacc4\uc5d0\uc11c \ub300\uaddc\ubaa8 \ub300\ud654 \ub370\uc774\ud130\ub97c \uc81c\uacf5\ud568\uc73c\ub85c\uc368, \uac15\ub825\ud55c \uc624\ud508 \ucc57\ubd07 \uac1c\ubc1c\uc5d0 \uc4f0\uc600\uc2b5\ub2c8\ub2e4. UltraChat\uc744 \ud559\uc2b5\ud55c UltraLM \uc2dc\ub9ac\uc988(LLaMA-13B \uae30\ubc18)\ub294 AlpacaEval \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc624\ud508\ubaa8\ub378 1\uc704\ub97c \uae30\ub85d\ud558\ub294 \ub4f1 \uc131\ub2a5\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4. \ub610\ud55c UltraChat\uc740 \ub77c\uc774\uc120\uc2a4 \uc81c\uc57d\uc774 \uc5c6\ub294 \ubc29\ub300\ud55c \ub300\ud654 \uc790\ub8cc\ub85c\uc11c, \ub2e4\ub978 \uc5f0\uad6c\uc790\ub4e4\ub3c4 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \ub300\ud654\ub2a5\ub825\uc744 \ubaa8\ub378\uc5d0 \ubd80\uc5ec\ud558\uae30 \uc704\ud574 \ud65c\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ucee8\ub300 Allen AI\uc758 T\u00fclu-2 \ubaa8\ub378 \uac1c\ubc1c \uc2dc\uc5d0\ub3c4 OpenAssistant\ub098 UltraChat\uac19\uc740 \ucee4\ubba4\ub2c8\ud2f0 \uc0dd\uc131 \ub300\ud654 \ucf54\ud37c\uc2a4\ub97c \uc870\ud569\ud574 \uace0\ud488\uc9c8 SFT \ub370\uc774\ud130\ub97c \uad6c\ucd95\ud558\uc600\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#simplescaling-s1-1k","title":"SimpleScaling S1 (\uace0\ub09c\ub3c4 \ubb38\uc81c 1K \ub370\uc774\ud130\uc14b)","text":"\ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac 25\ub144 3\uc6d4 \ud604\uc7ac, \uc81c\uac00 \uad00\uc2ec\uc744 \uac00\uc9c0\uace0 \uc788\ub294 \uc2e4\ud5d8\uc744 \ud558\uace0 \uc788\ub294 \ub370\uc774\ud130\uc14b \uc785\ub2c8\ub2e4. DeepSeek-R1 \uc744 \ud1b5\ud574 Reasoning \ubaa8\ub378\uc5d0 \ub300\ud55c \ud78c\ud2b8\uac00 \uc138\uc0c1\uc5d0 \uc54c\ub824\uc84c\ub294\ub370\uc694. R1 \uc5d0\uc11c \uc0ac\uc6a9\ud588\ub358 distillation\uacfc \ube44\uc2b7\ud55c \ubc29\uc2dd\uc73c\ub85c \ubcf4\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4. <ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: SimpleScaling S1\uc740 Stanford Hashimoto \uadf8\ub8f9 \ub4f1\uc774 2024\ub144 \ucd08 \uc81c\uc548\ud55c \uc18c\uaddc\ubaa8 \uace0\ud488\uc9c8 \ub370\uc774\ud130\uc14b\uc73c\ub85c, \ub2e8 1,000\uac1c\uc758 \uc9c8\ubb38\uacfc \uac01 \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc815\uad50\ud55c \ucd94\ub860 \uacfc\uc815(COT) \ubc0f \ucd5c\uc885 \ub2f5\ubcc0\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub294 \uc8fc\ub85c \uc218\ud559 \uacbd\uc2dc\ub300\ud68c \ubb38\uc81c(AIME, MATH) \ub4f1 \ub09c\uc774\ub3c4 \ub192\uc740 \uacfc\uc81c\ub97c \ud3ec\ud568\ud558\uba70, \uac01 \uc9c8\ubb38\uc5d0 \ub300\ud574 GPT-4 \uae30\ubc18\uc758 \uc2ec\ud654 \ub2e8\uacc4 \ucd94\ub860(Gemini Thinking)\uc744 \uc2e4\ud5d8\ud558\uace0 \uc0ac\ub78c\uc774 \uac80\uc99d\ud55c \uc0ac\uace0\ud754\uc801(traces)\uacfc \uc815\ub2f5\uc744 \uc30d\uc73c\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \ud2b9\uc131: \ub370\uc774\ud130 \uad6c\ucd95\uc2dc \ubb38\ud56d\uc758 \ub2e4\uc591\uc131, \ub09c\uc774\ub3c4, \ud488\uc9c8 \uc138 \uac00\uc9c0 \uae30\uc900\uc744 \ucda9\uc871\ud558\ub3c4\ub85d \uc120\ubcc4\ud588\uc73c\uba70, \uc77c\uc885\uc758 \uccb4\uc778-\uc624\ube0c-\uc18c\ud2b8(Chain-of-Thought) \ud3ec\ud568 \ub370\uc774\ud130\uc14b\uc774\ub77c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: SimpleScaling \ud300\uc740 \uc774 S1K \ub370\uc774\ud130\ub85c Tencent\uc758 Qwen-7B \ubaa8\ub378\uc744 \ubbf8\uc138\uc870\uc815\ud558\uace0, \ucd94\ub860 \ub2e8\uacc4 \uc870\uc808(budget forcing) \uae30\ubc95\uc744 \ud568\uaed8 \uc801\uc6a9\ud558\uc5ec S1-32B \ubaa8\ub378\uc744 \uacf5\uac1c\ud588\uc2b5\ub2c8\ub2e4. \uc791\uc740 \ub370\uc774\ud130\uc14b\uc774\uc9c0\ub9cc \uace0\ud488\uc9c8 \ucd94\ub860 \ud559\uc2b5 \ub355\ubd84\uc5d0, S1-32B\ub294 OpenAI\uc758 o1-preview \ubaa8\ub378\uc744 \uc218\ud559 \ubb38\uc81c\uc5d0\uc11c \ucd5c\ub300 27%p \uc0c1\ud68c\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc600\uace0, \ucd94\ub860 \ud14c\uc2a4\ud2b8\ud0c0\uc784 \uc99d\uac15\uc774\ub77c\ub294 \uc0c8\ub85c\uc6b4 \ubc29\ud5a5\uc744 \uc2dc\uc5f0\ud588\uc2b5\ub2c8\ub2e4. S1 \ub370\uc774\ud130\uc14b\uc740 \uaddc\ubaa8\ub294 \uc791\uc73c\ub098 \ub17c\ub9ac\uc801 \uc0ac\uace0\ub825 \uac15\ud654\uc5d0 \ud2b9\ud654\ub418\uc5b4 \uc788\uc5b4, \ub2e4\ub978 LLM\uc758 \uace0\ub09c\ub3c4 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud55c \ubcf4\ucda9 \ud559\uc2b5\uc790\ub8cc\ub85c \ud65c\uc6a9 \uac00\ub2a5\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#2-rlhfdpo","title":"2. \uc120\ud638\ub3c4 \ud559\uc2b5(RLHF/DPO) \ub370\uc774\ud130\uc14b","text":"<p>RLHF(Reinforcement Learning from Human Feedback) \ub610\ub294 DPO(Direct Preference Optimization) \ub2e8\uacc4\uc5d0\uc11c\ub294, \ubaa8\ub378\uc758 \uc751\ub2f5 \ud488\uc9c8\uc744 \ub192\uc774\uae30 \uc704\ud574 \uc120\ud638\ub3c4 \uc815\ubcf4\uac00 \ud3ec\ud568\ub41c \ub370\uc774\ud130\uc14b\uc774 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ub370\uc774\ud130\ub294 \ub300\uac1c \ud55c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \ub450 \uac1c \uc774\uc0c1\uc758 \ubaa8\ub378 \uc751\ub2f5\uacfc \uc778\uac04\uc758 \uc120\ud638 \ud310\ub2e8(\uc88b\uc74c/\ub098\uc068)\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc120\ud638\ub3c4 \ub370\uc774\ud130\uc14b\uc740 \ubcf4\uc0c1\ubaa8\ub378 \ud6c8\ub828\uc774\ub098, \ucd5c\uadfc \uc81c\uc548\ub41c DPO \uc54c\uace0\ub9ac\uc998\uc744 \ud1b5\ud55c \uc9c1\uc811 \ubbf8\uc138\uc870\uc815\uc5d0 \ud65c\uc6a9\ub429\ub2c8\ub2e4.</p>"},{"location":"data_preparation/dataset_creation/#anthropic-hh-rlhf-helpfulharmless-preferences","title":"Anthropic HH-RLHF (Helpful/Harmless Preferences)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: Anthropic\uc774 \uc790\uc0ac \ud5cc\ubc95\ud615 AI \uc5f0\uad6c\uc5d0\uc11c \uacf5\uac1c\ud55c Helpful-Harmless (HH) \ub300\ud654 \uc120\ud638 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \ud55c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud574 AI \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uc758 \ub450 \uac00\uc9c0 \ub2f5\ubcc0(\uc608: \ud558\ub098\ub294 \ub3c4\uc6c0\uc774 \ub418\uc9c0\ub9cc \ub2e4\ub978 \ud558\ub098\ub294 \ubb34\ub840\ud558\uac70\ub098 \uc720\ud574\ud55c \ub2f5\ubcc0)\uc744 \uc81c\uc2dc\ud558\uace0, \uc778\uac04 \ud3c9\uac00\uc790\uac00 \uc5b4\ub290 \ucabd\uc744 \uc120\ud638\ud558\ub294\uc9c0\ub97c \ub77c\ubca8\ub9c1\ud55c \ud398\uc5b4 \ub370\uc774\ud130\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc804\uccb4 \uc57d 160k \uc30d\uc758 \uc751\ub2f5 \ube44\uad50 \uc0ac\ub840\uac00 \ud3ec\ud568\ub418\uba70, \uc5ec\ub7ec \ud134\uc758 \ub300\ud654 \ub9e5\ub77d\ub3c4 \ub2f4\uaca8 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ubaa9\uc801\uacfc \ud2b9\uc9d5: \uc774 \ub370\uc774\ud130\ub294 Anthropic\uc758 '\ud5ec\ud504\ud480-\ud558\ub984\ub9ac\uc2a4' \uc815\ucc45\uc5d0 \ub530\ub77c \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uac00 \uc720\uc6a9\ud558\uba74\uc11c\ub3c4 \uc548\uc804\ud558\uac8c \ub300\ud654\ud558\ub3c4\ub85d \ud559\uc2b5\uc2dc\ud0a4\uae30 \uc704\ud574 \uc218\uc9d1\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: HH-RLHF \ub370\uc774\ud130\ub294 Anthropic\uc758 Claude \ubaa8\ub378 \ud559\uc2b5\ubfd0 \uc544\ub2c8\ub77c, \uacf5\uac1c \uc5f0\uad6c\uc5d0\uc11c\ub3c4 \ud45c\uc900 \uc120\ud638\ub3c4 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ud65c\uc6a9\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 Stanford\uc758 DPO \ub808\ud37c\ub7f0\uc2a4 \uad6c\ud604\uc5d0\uc11c\ub294 Anthropic HH \ub370\uc774\ud130\ub97c \uc120\ud638\ub3c4 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc5ec\ub7ec \uc624\ud508 RLHF \uc2e4\ud5d8(\uc608: Redwood Research \ub4f1)\uc5d0\uc11c \ubaa8\ub378\uc758 \uc720\ud574\ubc1c\uc5b8 \uc5b5\uc81c\ub098 \ub3c4\uc6c0\ub418\ub294 \ub2f5\ubcc0 \uc120\ud638\ub97c \uac00\ub974\uce58\ub294 \uc6a9\ub3c4\ub85c \uc774 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud569\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b\uc758 \ub4f1\uc7a5\uc73c\ub85c, \ube44\uad50\uc801 \uc18c\uaddc\ubaa8 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc778\uac04 \uc120\ud638\ub3c4 \uc801\uc6a9\uc774 \uac00\ub2a5\ud574\uc84c\uc73c\uba70, \ub300\ud654 \uc548\uc804\uc131 \uad00\ub828 \uc5f0\uad6c\uc758 \uae30\ubc18\uc774 \ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#stanford-shp-human-preferences-from-reddit","title":"Stanford SHP (Human Preferences from Reddit)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: SHP (Stanford Human Preferences)\ub294 Stanford\ud300\uc774 Reddit\uc5d0\uc11c \uc790\uc5f0\ubc1c\uc0dd\uc801\uc73c\ub85c \ucd95\uc801\ub41c \uc778\uac04 \uc120\ud638\ub97c \ucd94\ucd9c\ud558\uc5ec \ub9cc\ub4e0 \ub300\uaddc\ubaa8 \ud14d\uc2a4\ud2b8 \uc120\ud638 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. Reddit\uc758 \uc5ec\ub7ec \uc139\uc158(\uc694\ub9ac, \ubc95\ub960 \uc870\uc5b8 \ub4f1 18\uac1c \uc8fc\uc81c)\uc5d0\uc11c \uc9c8\ubb38/\uc694\uccad \uac8c\uc2dc\ubb3c\uacfc \uadf8\uc5d0 \ub300\ud55c \uc5ec\ub7ec \ub2f5\uae00\uc744 \uc218\uc9d1\ud55c \ub4a4, \ucee4\ubba4\ub2c8\ud2f0 \ud22c\ud45c(upvote)\ub97c \ube44\uad50\ud568\uc73c\ub85c\uc368 \uc5b4\ub290 \ub2f5\ubcc0\uc774 \ub354 \uc120\ud638\ub418\ub294\uc9c0 \uc0c1\ub300\uc801 \ud310\ub2e8\uc744 \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \ud2b9\uc131: \uc774\ub807\uac8c \ubaa8\uc740 \uc57d 38.5\ub9cc \uac74\uc758 \uc120\ud638 \ube44\uad50\ub294 \ubaa8\ub450 \uc0ac\ub78c\uc774 \uc4f4 \uc751\ub2f5\ub4e4 \uac04\uc758 \uc6b0\uc5f4\uc774\ubbc0\ub85c, \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uc778\uac04 \uc120\ud638 \uacbd\ud5a5\uc744 \ubc18\uc601\ud569\ub2c8\ub2e4. (\uc608: \uac19\uc740 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub450 \uac1c\uc758 \ub313\uae00 \uc911 \ud558\ub098\uc5d0 \ub354 \ub9ce\uc740 \ucd94\ucc9c\uc774 \uc788\ub2e4\uba74, \uadf8 \ub2f5\ubcc0\uc744 \uc120\ud638\ud558\ub294 \uac83\uc73c\ub85c \uac04\uc8fc\ud568.)</li> <li>\ud65c\uc6a9 \uc0ac\ub840: SHP\ub294 RLHF \ubcf4\uc0c1\ubaa8\ub378 \ud559\uc2b5 \ucd08\uae30\ub2e8\uacc4\uc5d0 \ub110\ub9ac \uc4f0\uc774\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ucee8\ub300 OpenAI \uc694\uc57d \ub17c\ubb38\uc5d0\uc11c\ub3c4 TL;DR \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc778\uac04 \ud53c\ub4dc\ubc31 \uc678\uc5d0 \uc774\ub7f0 \uc6f9 \uc0c1\uc758 \uc120\ud638 \ub370\uc774\ud130\ub97c \ucc38\uace0\ud558\uae30\ub3c4 \ud588\uc2b5\ub2c8\ub2e4. Stanford DPO \uc5f0\uad6c\uc5d0\uc11c\ub3c4 SFT \ud6c4 SHP \ub370\uc774\ud130\ub85c DPO \ubbf8\uc138\ud29c\ub2dd\uc744 \uc218\ud589\ud558\uc5ec \ud6a8\uacfc\ub97c \uac80\uc99d\ud588\uace0, HuggingFace \ub4f1 \ucee4\ubba4\ub2c8\ud2f0\uc5d0\uc11c\ub3c4 \uc120\ud638 \ud559\uc2b5 \ubca4\uce58\ub9c8\ud06c\ub85c \ud65c\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. SHP\uc758 \uc7a5\uc810\uc740 \ubc29\ub300\ud55c \ud604\uc2e4 \uc751\ub2f5\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uc5ec \ud2b9\uc815 \ubaa8\ub378\uc5d0 \ud3b8\ud5a5\ub418\uc9c0 \uc54a\uc740 \uc778\uac04 \uc120\ud638 \uc2e0\ud638\ub97c \uc900\ub2e4\ub294 \uc810\uc774\uba70, \uc774\ub97c \ud1b5\ud574 LLM\uc774 \ub354 \ub3c4\uc6c0\uc774 \ub418\ub294 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \uc720\ub3c4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#ultrafeedback-ai","title":"UltraFeedback (AI \uc0dd\uc131 \ub300\uaddc\ubaa8 \uc120\ud638 \ub370\uc774\ud130)","text":"<ul> <li>\ub370\uc774\ud130 \uad6c\uc131 \ubc0f \uc0dd\uc131: UltraFeedback\uc740 OpenBMB \ud300\uc774 \ud655\uc7a5 \uac00\ub2a5\ud55c AI \ud53c\ub4dc\ubc31\uc744 \ud1b5\ud574 \ub9cc\ub4e0 \ub300\uaddc\ubaa8 \uc815\ubc00 \uc120\ud638\ub3c4 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uba3c\uc800 UltraChat, ShareGPT, Evol-Instruct, TruthfulQA \ub4f1 \ub2e4\uc591\ud55c \ucd9c\ucc98\uc758 64k \ud504\ub86c\ud504\ud2b8\ub97c \uc218\uc9d1\ud55c \ud6c4, \uac01 \ud504\ub86c\ud504\ud2b8\ub9c8\ub2e4 \uc5ec\ub7ec LLM\ub4e4(\uc5ec\ub7ec \uc624\ud508\uc18c\uc2a4 \ubc0f \uc0c1\uc6a9 \ubaa8\ub378)\uc744 \ud65c\uc6a9\ud574 4\uac1c\uc758 \uc0c1\uc774\ud55c \uc751\ub2f5\uc744 \uc0dd\uc131\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ud3c9\uac00 \ubc29\uc2dd: \uc774\ub807\uac8c \uc5bb\uc740 256k\uac1c\uc758 \ubaa8\ub378 \uc751\ub2f5\uc5d0 \ub300\ud574, OpenAI GPT-4 \ubaa8\ub378\uc5d0\uac8c \ubbf8\ub9ac \uc815\uc758\ub41c \uae30\uc900(\uc9c0\uc2dc \uc900\uc218, \uc815\ud655\uc131, \uc815\uc9c1\uc131, \uc720\uc6a9\uc131\uc758 4\uce21\uba74)\uc744 \ub530\ub77c \uc138\ubc00\ud55c \ud3c9\uac00\ub97c \uc218\ud589\ud558\uac8c \ud558\uc600\uc2b5\ub2c8\ub2e4. GPT-4\ub294 \uac01 \uc751\ub2f5\uc5d0 \ub300\ud574 4\uac00\uc9c0 \uce21\uba74\ubcc4 \uc810\uc218\uc640 \uc804\uccb4 \uc810\uc218\ub97c \ub9e4\uacbc\uc73c\uba70, \uadf8 \uacb0\uacfc \uc751\ub2f5\ubcc4 \uc138\ubd80 \ud53c\ub4dc\ubc31\uacfc \ub4f1\uae09\uc774 \ubd80\uc5ec\ub41c \ud53c\ub4dc\ubc31 \ub370\uc774\ud130\uc14b\uc774 \uad6c\ucd95\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \uc751\ub2f5 \uac04 \uc57d 34\ub9cc \uac74\uc758 \uc120\ud638 \ube44\uad50 \uc30d\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \ubaa8\ub4e0 \uc0d8\ud50c\uc5d0 \uc138\ubc00 \ud3c9\uac00 \ucf54\uba58\ud2b8\uae4c\uc9c0 \ud3ec\ud568\ub41c \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4.</li> <li>\ud65c\uc6a9 \uc0ac\ub840: UltraFeedback \ub370\uc774\ud130\uc14b\uc740 \ubcf4\uc0c1\ubaa8\ub378(RM)\uc774\ub098 \ube44\ud3c9\ubaa8\ub378(Critic) \ud559\uc2b5\uc5d0 \ubc14\ub85c \ud65c\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ud55c UltraRM \ubcf4\uc0c1\ubaa8\ub378\uc744 \ud1b5\ud574, UltraLM \uac19\uc740 \uc0dd\uc131\ubaa8\ub378\uc5d0 best-of-n \uc0dd\uc131\uc744 \uc2dc\ub3c4\ud55c \uacb0\uacfc text-davinci-003 \ub300\ube44 92% \uc815\ub2f5\ub960\ub85c \uc6b0\uc218\ud568\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ucd5c\uadfc\uc5d0\ub294 \uc774 \uc120\ud638\ub3c4 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud55c DPO \ubbf8\uc138\uc870\uc815\ub3c4 \uc2dc\ub3c4\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 open LLM\uc778 Zephyr-7B\ub294 Mistral-7B\ub97c \uae30\ubc18\uc73c\ub85c UltraFeedback \uc120\ud638\uc30d\uc73c\ub85c DPO \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uc5ec, AlpacaEval \ud3c9\uac00 \uc810\uc218\ub97c \ud5a5\uc0c1\uc2dc\ucf30\ub2e4\ub294 \ubcf4\uace0\uac00 \uc788\uc2b5\ub2c8\ub2e4. Allen AI\uc758 T\u00fclu-2 70B \ubaa8\ub378 \uc5ed\uc2dc UltraFeedback\uc744 \ud3ec\ud568\ud55c \ub300\ub7c9\uc758 \uc120\ud638 \ub370\uc774\ud130\ub85c RLHF/DPO\ub97c \uc218\ud589\ud574 \uc131\ub2a5\uc744 \ub192\uc600\uc2b5\ub2c8\ub2e4. UltraFeedback\uc740 \uc778\uac04 \ub300\uc2e0 AI\ub85c\ubd80\ud130 \uc5bb\uc740 \ud53c\ub4dc\ubc31\uc774\ubbc0\ub85c \uc800\ube44\uc6a9\uc73c\ub85c \ub300\uaddc\ubaa8 \uad6c\ucd95\uc774 \uac00\ub2a5\ud558\uba70, \uc774\ub97c \ud1b5\ud574 \uc624\ud508\uc18c\uc2a4 LLM\uc758 RLHF \ud488\uc9c8\uc744 \ud06c\uac8c \uac1c\uc120\ud560 \uc218 \uc788\uc74c\uc744 \uc785\uc99d\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#_3","title":"\ub370\uc774\ud130\uc14b \uc81c\uc791 \ubc29\ubc95\ub860","text":"<p>\ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc73c\ub85c \uc81c\uc791\ub418\uba70, \uac01 \ubc29\ubc95\ub860\uc740 \uace0\uc720\ud55c \uc7a5\ub2e8\uc810\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4:</p>"},{"location":"data_preparation/dataset_creation/#1","title":"1. \uc778\uac04 \uc791\uc131 \ub370\uc774\ud130","text":"<ul> <li>\uc804\ubb38\uac00 \uc81c\uc791: Dolly 15K\ucc98\ub7fc \uc804\ubb38\uac00\ub098 \uae30\uc5c5 \uc9c1\uc6d0\ub4e4\uc774 \uc9c1\uc811 \uc791\uc131\ud55c \uace0\ud488\uc9c8 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \ud488\uc9c8\uc774 \ub192\uace0 \uc758\ub3c4\uc5d0 \ub9de\ub294 \ub370\uc774\ud130\ub97c \uc5bb\uc744 \uc218 \uc788\uc9c0\ub9cc, \ube44\uc6a9\uacfc \uc2dc\uac04\uc774 \ub9ce\uc774 \uc18c\uc694\ub429\ub2c8\ub2e4.</li> <li>\ud06c\ub77c\uc6b0\ub4dc\uc18c\uc2f1: OASST1\ucc98\ub7fc \ub2e4\uc218\uc758 \ucc38\uc5ec\uc790\uac00 \ud611\ub825\ud558\uc5ec \uad6c\ucd95\ud55c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uad00\uc810\uacfc \uc2a4\ud0c0\uc77c\uc744 \ud3ec\ud568\ud560 \uc218 \uc788\uc73c\ub098, \ud488\uc9c8 \uad00\ub9ac\uac00 \uc5b4\ub824\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc0ac\uc6a9\uc790 \uacf5\uc720 \ub370\uc774\ud130: ShareGPT\ucc98\ub7fc \uc2e4\uc81c \uc0ac\uc6a9\uc790\ub4e4\uc758 \ub300\ud654\ub97c \uc218\uc9d1\ud55c \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4. \uc2e4\uc81c \uc0ac\uc6a9 \ud328\ud134\uc744 \ubc18\uc601\ud558\uc9c0\ub9cc, \uac1c\uc778\uc815\ubcf4\ub098 \uc800\uc791\uad8c \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#2-ai","title":"2. AI \uc0dd\uc131 \ub370\uc774\ud130","text":"<ul> <li>Self-Instruct: Alpaca\ucc98\ub7fc \uc18c\ub7c9\uc758 \uc778\uac04 \uc791\uc131 \uc2dc\ub4dc \ub370\uc774\ud130\ub85c AI\uac00 \ub300\ub7c9\uc758 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uc801\uc740 \ube44\uc6a9\uc73c\ub85c \ub300\ub7c9 \ub370\uc774\ud130\ub97c \uc5bb\uc744 \uc218 \uc788\uc9c0\ub9cc, \uc6d0\ubcf8 \ubaa8\ub378\uc758 \ud55c\uacc4\uc640 \ud3b8\ud5a5\uc774 \ubc18\uc601\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>AI \uc2dc\ubbac\ub808\uc774\uc158: UltraChat\ucc98\ub7fc AI\uac00 \uc0ac\uc6a9\uc790\uc640 \uc5b4\uc2dc\uc2a4\ud134\ud2b8 \uc5ed\ud560\uc744 \ubaa8\ub450 \uc218\ud589\ud558\uc5ec \ub300\ud654\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ub2e4\uc591\ud55c \uc2dc\ub098\ub9ac\uc624\ub97c \ube60\ub974\uac8c \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\ub098, \uc2e4\uc81c \uc778\uac04 \ub300\ud654\uc758 \uc790\uc5f0\uc2a4\ub7ec\uc6c0\uc774 \ubd80\uc871\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>AI \ud3c9\uac00: UltraFeedback\ucc98\ub7fc AI\uac00 \ub2e4\ub978 \ubaa8\ub378\uc758 \uc751\ub2f5\uc744 \ud3c9\uac00\ud558\uc5ec \uc120\ud638\ub3c4 \ub370\uc774\ud130\ub97c \uad6c\ucd95\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ub300\uaddc\ubaa8 \ud3c9\uac00\uac00 \uac00\ub2a5\ud558\uc9c0\ub9cc, \ud3c9\uac00 \ubaa8\ub378\uc758 \ud3b8\ud5a5\uc774 \uacb0\uacfc\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#3","title":"3. \ud63c\ud569 \uc811\uadfc\ubc95","text":"<ul> <li>\uc778\uac04-AI \ud611\uc5c5: SimpleScaling S1\ucc98\ub7fc AI\uac00 \ucd08\uc548\uc744 \uc0dd\uc131\ud558\uace0 \uc778\uac04\uc774 \uac80\uc99d\ud558\uac70\ub098, \uc778\uac04\uc774 \uc9c8\ubb38\uc744 \ub9cc\ub4e4\uace0 AI\uac00 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ud488\uc9c8\uacfc \uaddc\ubaa8\uc758 \uade0\ud615\uc744 \ub9de\ucd9c \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uc790\uc5f0\ubc1c\uc0dd \ub370\uc774\ud130 \ud65c\uc6a9: SHP\ucc98\ub7fc \uc628\ub77c\uc778 \ud50c\ub7ab\ud3fc\uc758 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uc0c1\ud638\uc791\uc6a9\uc5d0\uc11c \uc120\ud638\ub3c4\ub97c \ucd94\ucd9c\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uc2e4\uc81c \uc778\uac04 \uc120\ud638\ub3c4\ub97c \ubc18\uc601\ud558\uba74\uc11c\ub3c4 \ub300\uaddc\ubaa8 \ub370\uc774\ud130 \uc218\uc9d1\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \uc99d\uac15: \uae30\uc874 \ub370\uc774\ud130\uc14b\uc744 AI\ub85c \ubcc0\ud615\ud558\uac70\ub098 \ud655\uc7a5\ud558\uc5ec \ub354 \ub9ce\uc740 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ud655\ubcf4\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \uc6d0\ubcf8 \ub370\uc774\ud130\uc758 \ud488\uc9c8\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ub2e4\uc591\uc131\uc744 \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"data_preparation/dataset_creation/#_4","title":"\ub370\uc774\ud130\uc14b \uc81c\uc791 &amp; \ud30c\uc778\ud29c\ub2dd \uc0ac\ub840 \ud0d0\uad6c","text":""},{"location":"data_preparation/dataset_creation/#1-llama3-hallucination","title":"1. Llama3 \uc758 Hallucination \ubc29\uc9c0\ub97c \uc704\ud55c \ub370\uc774\ud130\uc14b","text":"<p>Llama3 Herd of Models \ud398\uc774\ud37c\uc5d0\uc11c \ubc1c\ucdcc\ud574 \uc654\uc2b5\ub2c8\ub2e4. </p> <pre><code>Align the model to \"Know what it knows\"\n\n1. Extract a data snippet from the pre-training data.\n2. Generate a factual question about these snippets (context) by prompting Llama 3.\n3. Sample responses from Llama 3 to the question.\n4. Score the correctness of the generations using the original context as a reference and Llama 3 as a judge.\n5. Score the informativeness of the generations using Llama 3 as a judge.\n6. Generate a refusal for responses which are consistently informative and incorrect across the generations, using Llama 3.\n</code></pre> <p>(\uc791\uc131 \uc911)</p>"},{"location":"data_preparation/templates_formats/","title":"\ud15c\ud50c\ub9bf\uacfc \ud3ec\ub9f7","text":""},{"location":"data_preparation/templates_formats/#_2","title":"\ub370\uc774\ud130 \ud15c\ud50c\ub9bf \uc124\uacc4","text":"<ul> <li>\ud6a8\uacfc\uc801\uc778 \ud15c\ud50c\ub9bf \uad6c\uc870</li> <li>\uc5ed\ud560 \uae30\ubc18 \ud3ec\ub9f7 (\uc0ac\uc6a9\uc790/\uc5b4\uc2dc\uc2a4\ud134\ud2b8)</li> <li>\uc2dc\uc2a4\ud15c \uc9c0\uc2dc \ud65c\uc6a9</li> </ul>"},{"location":"data_preparation/templates_formats/#jinja","title":"Jinja \ud15c\ud50c\ub9bf \ud65c\uc6a9","text":"<ul> <li>Jinja2 \uae30\ubcf8 \ubb38\ubc95</li> <li>\ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130\uc5d0 Jinja \uc801\uc6a9\ud558\uae30</li> <li>\uc870\uac74\ubd80 \ubc0f \ubc18\ubcf5 \uad6c\ubb38 \ud65c\uc6a9</li> </ul>"},{"location":"data_preparation/templates_formats/#vs","title":"\ucc44\ud305 \ud615\uc2dd vs \uba85\ub839\uc5b4 \ud615\uc2dd","text":"<ul> <li>\ucc44\ud305 \uae30\ubc18 \ud559\uc2b5 \ub370\uc774\ud130 \uad6c\uc131</li> <li>\uba85\ub839\uc5b4-\uc751\ub2f5 \ud615\ud0dc \ub370\uc774\ud130 \uad6c\uc131</li> <li>\uac01 \ud615\uc2dd\uc758 \uc801\ud569\ud55c \uc0ac\uc6a9 \uc0c1\ud669 </li> </ul>"},{"location":"evaluation/benchmarks/","title":"\ubca4\uce58\ub9c8\ud06c \ubc0f \ud3c9\uac00 \ubc29\ubc95\ub860","text":"<p>LLM\uc744 \uc798 \ub9cc\ub4e4\uc5c8\ub294\uc9c0 \uad81\uae08\ud558\ub2e4\uba74 \ud3c9\uac00\ub97c \ud574\uc57c\uaca0\uc8e0.  </p> <p>\uac00\uc7a5 \uc26c\uc6b4 \ubc29\ubc95\uc740, </p> <ul> <li>\uc0ac\ub78c\ub4e4\ud55c\ud14c \ub450 \uac00\uc9c0 \ubaa8\ub378\uc5d0\uc11c \ub098\uc628 \uc751\ub2f5\uc744 \ubcf4\uc5ec\uc8fc\uace0 \ubb50\uac00 \ub354 \ub098\uc740\uc9c0 \ud22c\ud45c\ub97c \ud558\uac8c \ud558\ub294 \uac83\uc785\ub2c8\ub2e4.</li> </ul> <p>\uc0ac\ub78c\uc774 \ud22c\ud45c\ub97c \ud558\uba74 \uc790\ub3d9 \ud3c9\uac00\uac00 \uc548\ub418\ub294 \uac83\uc774 \ubb38\uc81c\uc8e0. \ub2e4\uc74c \ubc29\ubc95\uc740 </p> <ul> <li>\uadf8\ub0e5 \ud034\uc988\ub0b4\uace0 \uc798 \ub9de\ucd94\ub294\uc9c0 \ubcf4\ub294 \uac83\uc785\ub2c8\ub2e4.</li> </ul> <p>\ub2e8\ub2f5\ud615\uc774\ub098 \uac1d\uad00\uc2dd \ub2f5\ubcc0\uc740 \ub2e8\uc21c\ud558\uac8c \ucc44\uc810\uc774 \uac00\ub2a5\ud55c\ub370\uc694, \ub2e4\uc74c \ubb38\uc81c\ub294 \uc11c\uc220\ud615 \ud3c9\uac00\uc8e0. \uc694\uc998\uc740 \uc11c\uc220\ud615 \ub2f5\ubcc0\uc5d0 \ub300\ud55c \ud3c9\uac00\ub3c4 LLM \uc5d0 \ub9e1\uae41\ub2c8\ub2e4. \uc774\ub97c LLM as Judge \ub77c\uace0 \ubd80\ub974\ub294\ub370\uc694, \uc774\uac74 \ub530\ub85c \ub2e4\uc74c \ubb38\uc11c\uc5d0\uc11c \ub2e4\ub8e8\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"evaluation/benchmarks/#1-lmsys-chatbot-arena","title":"1. LMSYS Chatbot Arena","text":"<p>Chatbot Arena Leaderboard \uc5d0 \uac00\uc2dc\uba74 \ud604 \uc2dc\uc810\uc5d0 \uac00\uc7a5 \ub9ce\uc740 \uc120\ud0dd\uc744 \ubc1b\uc740 \ubaa8\ub378\ub4e4\uc758 \uc21c\uc704\ub97c \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \uc774 \uae00\uc744 \uc791\uc131\ud558\uace0 \uc788\ub294 25.03.10 \uae30\uc900\uc73c\ub85c\ub294 Grok3 \uac00 1\ub4f1, 1\uc810\ucc28\uc774\ub85c GPT-4.5\uac00 2\ub4f1\uc774\ub124\uc694.</p> <p>\uc81c\uac00 \uac1c\uc778\uc801\uc73c\ub85c \uc5b4\ub5a4 \ubaa8\ub378\uc774 \ud604\uc7ac \uac00\uc7a5 \uc88b\uc744\uc9c0 \uad81\uae08\ud560 \ub54c \ud56d\uc0c1 \ubcf4\ub294 \uacf3\uc785\ub2c8\ub2e4. </p> <p>LMSYS Chatbot Arena Leaderboard (25.03.10)</p> <p>\ub204\uad6c\ub098 \ubc14\ub85c \ud14c\uc2a4\ud2b8\ud558\uace0 \ud22c\ud45c \ud574\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubc14\ub85c \ud55c\ubc88 \ud574\ubd24\uc2b5\ub2c8\ub2e4.  Model B \uac00 \uc90f\ub300\uac00 \uc788\uad70\uc694. \ub354 \ub9c8\uc74c\uc5d0 \ub4dc\ub294 \ubaa8\ub378\uc744 \ud22c\ud45c\ud558\uba74 \uac00\ub824\uc9c4 \uc774\ub984\uc774 \ub098\uc635\ub2c8\ub2e4. </p> <p>LMSYS Chatbot Arena Battle</p> <p>Model A \ub294 claude sonnet 3.7 thinking \uc774\uc5c8\uace0, Model B \ub294 Gemini 2.0 flash lite \uc774\uc5c8\uc2b5\ub2c8\ub2e4. </p> <p>LMSYS Arena \uac00 \uc804\ubc18\uc801\uc778 \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub300\ud574 \uac00\uc7a5 \uc0ac\ub78c\ub4e4\uc774 \uc2e0\ub8b0\ud558\ub294 \ud3c9\uac00 \ubc29\ubc95\uc778 \uac83 \uac19\uc544\uc694. \ubb3c\ub860, \ucf54\ub529 \ub2a5\ub825\uc774\ub098 \ubd84\uc57c\uc5d0 \ub530\ub77c\uc11c, \uc6a9\ub3c4\uc5d0 \ub530\ub77c\uc11c \ub2e4 \ub2e4\ub97c \uc218 \uc788\uacd8\uc9c0\ub9cc\uc694. \ube14\ub77c\uc778\ub4dc \ud398\uc5b4\uc640\uc774\uc988 \ud14c\uc2a4\ud2b8, Elo \ub808\uc774\ud305 \uc2dc\uc2a4\ud15c, \uc218\ub9ce\uc740 \ub370\uc774\ud130 \uc218\uc9d1 \ub4f1 \uc2e0\ub8b0\ud560 \ub9cc\ud55c \uc694\uc18c\uac00 \uc798 \uac16\ucdb0\uc9c4 \uac83 \uac19\uc2b5\ub2c8\ub2e4.  </p> <p>\ubb38\uc81c\ub294 \uc2dc\uc2a4\ud15c\uc5d0 \ub4f1\ub85d\ub418\uc5b4 \uc0ac\ub78c\ub4e4\uc758 \ud22c\ud45c\ub97c \uae30\ub2e4\ub824\uc57c \ud558\ub294\ub370\uc694, \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 Arena Hard Auto \ub77c\ub294 \uc544\ub808\ub098 \ubaa8\uc0ac(?) \ubca4\uce58\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00\ub294 \uc774\uc5b4\uc11c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"evaluation/benchmarks/#2","title":"2. \ubca4\uce58\ub9c8\ud06c \ud14c\uc2a4\ud2b8","text":"<p>LLM \uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc81c\uc77c \uc720\uba85\ud55c \uac83\uc740 \uc544\ubb34\ub798\ub3c4 MMLU (Massive Multitask Language Understanding) \uaca0\uc8e0. \ud55c \ubc88\ucbe4\uc740 \ub4e4\uc5b4\ubcf4\uc168\uc744 \ud150\ub370\uc694. 57\uac1c \uacfc\ubaa9\uc5d0 \uac78\uce5c \uc9c0\uc2dd \ud14c\uc2a4\ud2b8\ub85c, \ub300\ud559 \uc218\uc900\uc758 \ubb38\uc81c\ub4e4\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.  </p> <p>\uc218 \ub9ce\uc740 \ubb38\uc81c \uc911 \ud558\ub098\ub9cc \ubc1c\ucdcc\ud574 \uc654\uc2b5\ub2c8\ub2e4. </p> \ubb38\uc81c \uacfc\ubaa9 \ubcf4\uae30 \uc815\ub2f5 Which of these sets of logic gates are designated as universal gates? electrical_engineering A. NOR, NAND.B. XOR, NOR, NAND.C. OR, NOT, AND.D. NOR, NAND, XNOR. A <p>\uc774 \ucc98\ub7fc \ubcf4\uae30\uac00 4\uac1c\uac00 \uc788\ub294 \uac1d\uad00\uc2dd\uc774\ub77c \ud3c9\uac00\ud558\uae30\uac00 \uc544\uc8fc \uc27d\uc2b5\ub2c8\ub2e4. </p> <p>\ub610 \ub2e4\ub978 \uc720\uba85\ud55c \ubca4\uce58\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. </p> <ul> <li>HumanEval/MBPP: \ucf54\ub529 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ubca4\uce58\ub9c8\ud06c\ub85c, \ud568\uc218 \uad6c\ud604 \ubb38\uc81c\ub97c \uc81c\uc2dc\ud558\uace0 \uc2e4\ud589 \uac00\ub2a5\ud55c \ucf54\ub4dc\ub97c \uc0dd\uc131\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. HumanEval\uc740 OpenAI\uac00 \uac1c\ubc1c\ud55c 164\uac1c\uc758 \ud504\ub85c\uadf8\ub798\ubc0d \ubb38\uc81c\ub85c \uad6c\uc131\ub418\uba70, MBPP(Mostly Basic Python Problems)\ub294 974\uac1c\uc758 \uae30\ubcf8 \ud30c\uc774\uc36c \ud504\ub85c\uadf8\ub798\ubc0d \ubb38\uc81c\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.</li> <li>GSM8K/MATH: \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744 \ud3c9\uac00\ud558\ub294 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4. GSM8K\ub294 8,000\uac1c\uc758 \ucd08\uc911\ub4f1 \uc218\uc900 \uc218\ud559 \ubb38\uc81c\ub97c, MATH\ub294 \ub300\ud559 \uc785\uc2dc \uc218\uc900\uc758 12,500\uac1c \uace0\ub09c\ub3c4 \uc218\ud559 \ubb38\uc81c\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \ub2e8\uacc4\ubcc4 \ucd94\ub860 \ub2a5\ub825\uc744 \uce21\uc815\ud558\ub294 \ub370 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4.</li> <li>TruthfulQA: \ubaa8\ub378\uc774 \uc0ac\uc2e4\uc5d0 \uae30\ubc18\ud55c \uc815\ud655\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub294\uc9c0 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \uc624\ud574, \ubbf8\uc2e0, \uc798\ubabb\ub41c \ubbff\uc74c\uc5d0 \uad00\ud55c \uc9c8\ubb38\uc744 \ud1b5\ud574 \ubaa8\ub378\uc758 \uc0ac\uc2e4\uc131\uacfc \uc815\uc9c1\uc131\uc744 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4.</li> <li>ARC (AI2 Reasoning Challenge): \ucd08\ub4f1\ud559\uad50 \uc218\uc900\uc758 \uacfc\ud559 \ubb38\uc81c\ub97c \ud1b5\ud574 \ucd94\ub860 \ub2a5\ub825\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc26c\uc6b4 \uc138\ud2b8\uc640 \uc5b4\ub824\uc6b4 \uc138\ud2b8\ub85c \ub098\ub258\uba70, \ud2b9\ud788 \uc5b4\ub824\uc6b4 \uc138\ud2b8\ub294 \ub2e8\uc21c \ud328\ud134 \ub9e4\uce6d\uc73c\ub85c \ud574\uacb0\ud560 \uc218 \uc5c6\ub294 \ubb38\uc81c\ub4e4\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>MMLU-Pro: MMLU\uc758 \ud655\uc7a5 \ubc84\uc804\uc73c\ub85c, \ub354 \uc5b4\ub824\uc6b4 \uc804\ubb38 \uc9c0\uc2dd \ubb38\uc81c\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4.</li> <li>SWE-bench: \uc2e4\uc81c GitHub \uc774\uc288\ub97c \uae30\ubc18\uc73c\ub85c \ud55c \uc18c\ud504\ud2b8\uc6e8\uc5b4 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1 \ub2a5\ub825 \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\uc785\ub2c8\ub2e4.</li> <li>SimpleQA: \uc77c\ubc18 \uc9c0\uc2dd\uc744 \ud14c\uc2a4\ud2b8\ud558\ub294 \uc9c8\uc758\uc751\ub2f5 \ubca4\uce58\ub9c8\ud06c\ub85c, \ubaa8\ub378\uc758 \uae30\ubcf8\uc801\uc778 \uc0ac\uc2e4 \uc9c0\uc2dd\uc744 \ud3c9\uac00\ud569\ub2c8\ub2e4.</li> </ul> <p>\ub2e4\uc591\ud55c \ubca4\uce58\ub4e4\uc740 \uc6cc\ub099 \ub9ce\uae30 \ub54c\ubb38\uc5d0 \uc774\uc815\ub3c4\ub9cc \uc18c\uac1c\ud558\uace0 \ub118\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"evaluation/benchmarks/#_2","title":"\uc790\ub3d9 \ud3c9\uac00 \ub3c4\uad6c","text":""},{"location":"evaluation/benchmarks/#eleutherai-lm_eval","title":"EleutherAI\uc758 lm_eval","text":"<p>\uc544\ub798\uc640 \uac19\uc774 CLI\ub85c \uc544\uc8fc \uc27d\uac8c \uc790\ub3d9\ud654\ub41c \ud3c9\uac00\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4. </p> <pre><code>lm_eval --model hf \\\n    --model_args pretrained=EleutherAI/gpt-j-6B \\\n    --tasks hellaswag \\\n    --device cuda:0 \\\n    --batch_size 8\n</code></pre> <ul> <li>\ubaa8\ub4c8\uc2dd \ud0dc\uc2a4\ud06c \ud50c\ub7ec\uadf8\uc778: \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\ub97c \uc77c\uad00\ub41c \ubc29\uc2dd\uc73c\ub85c \ud3c9\uac00\ud560 \uc218 \uc788\ub294 \ud1b5\ud569 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4.</li> <li>\ub2e4\uc591\ud55c \ud3c9\uac00 \ubc29\uc2dd \uc9c0\uc6d0: \uc81c\ub85c\uc0f7, \uc6d0\uc0f7, \ud4e8\uc0f7 \ub4f1 \ub2e4\uc591\ud55c \ud3c9\uac00 \ubc29\uc2dd\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.</li> <li>\ub2e4\uc591\ud55c \uba54\ud2b8\ub9ad: \uc815\ud655\ub3c4, F1 \uc810\uc218, \ud37c\ud50c\ub809\uc2dc\ud2f0 \ub4f1 \ud0dc\uc2a4\ud06c\uc5d0 \uc801\ud569\ud55c \uba54\ud2b8\ub9ad\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> <li>\uc7ac\ud604\uc131: \ub3d9\uc77c\ud55c \ubaa8\ub378\uacfc \ud0dc\uc2a4\ud06c \uc124\uc815\uc73c\ub85c \ud56d\uc0c1 \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ud3c9\uac00 \ubc29\uc2dd\uc5d0 \ub300\ud574\uc120 \uc790\uc138\ud55c \uc124\uba85\uc774 \ub9ce\uc774 \ud544\uc694\ud574\uc11c \ub530\ub85c \ub2e4\ub8e8\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>lm_eval tasks \ub97c \ubcf4\uc2dc\uba74 \uc9c0\uc6d0\ud558\ub294 \ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c\ub4e4\uc744 \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\uc8fc \uc544\uc8fc \uc544\uc8fc \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uaf2d \uc4f0\uc138\uc694!</p>"},{"location":"evaluation/benchmarks/#llm","title":"\ud55c\uad6d\uc5b4 LLM \ud3c9\uac00 \ubc29\ubc95","text":"<p>\ud55c\uad6d\uc5b4\ub97c \uc704\ud55c \ubca4\uce58\ub9c8\ud06c\ub3c4 \ub2f9\uc5f0\ud788 \uaf64 \uc788\uc2b5\ub2c8\ub2e4. </p> <p>\uc55e\uc11c \uc774\uc57c\uae30\ud55c lm_eval tasks \uc5d0\ub3c4 \ud55c\uad6d\uc5b4 \ubca4\uce58\ub9c8\ud06c\ub4e4\uc774 \uc788\uc8e0. </p> <p>\ucd5c\uadfc \uce74\uce74\uc624\uc5d0\uc11c \ud55c\uad6d\uc5b4\ub97c \ud0c0\uac9f\uc73c\ub85c \ubc1c\ud45c\ud55c \ubaa8\ub378 Kanana\uc758 \ud3c9\uac00 \uacb0\uacfc\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> \ubaa8\ub378 MMLU KMMLU HAE-RAE HumanEval+ MBPP+ GSM8K MATH Kanana-Flag-32.5b 81.08 64.19 68.18 77.44 69.84 90.83 57.82 Qwen2.5-32b 84.40 59.37 48.30 82.32 71.96 95.30 81.90 Gemma-2-27b 78.01 49.98 46.02 70.12 70.90 91.05 53.80 EXAONE-3.5-32b 78.30 55.44 52.27 78.66 70.90 93.56 76.80 Aya-Expanse-32b 74.49 42.35 51.14 64.63 65.61 75.06 42.82 <p>\uc774 \ud45c\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774, Kanana \uac00 \uac00\uc7a5 \ub6f0\uc5b4\ub09c \ub450 \uac1c\uc758 \ubca4\uce58 KMMLU \uc640 HAE-RAE \uac00 \ud55c\uad6d\uc5b4 \uae30\ubc18 \ubca4\uce58\ub9c8\ud06c \uc785\ub2c8\ub2e4.  </p> <p>\uc81c\uac00 \ubf51\uc740 \ubca4\uce58\ub9c8\ud06c\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \ud55c\uad6d\uc5b4 \uc0ac\uc6a9\uc790 \ubd84\ub4e4\uc774 \uac00\uc7a5 \ub9ce\uc774 \uc0ac\uc6a9\ud558\uc2dc\ub358 \uac83\ub4e4\uc744 \uae30\uc900\uc73c\ub85c \uc120\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc601\uc5b4 \ubca4\uce58\ub9c8\ud06c\ub97c \ud55c\uad6d\uc5b4 \uae30\ubc18\uc73c\ub85c \uc801\uc6a9\uc2dc\ud0a8 \ubca4\uce58\ub4e4\uc774 \uac00\uc7a5 \uc77c\ubc18\uc801\uc774\uace0, HAE-RAE \uac19\uc740 \uacbd\uc6b0\ub294 \"\ud55c\uad6d\uc5b4\" \ub77c\uae30\ubcf4\ub2e4 \"\ud55c\uad6d\" \ubb38\ud654 \ubca4\uce58 \ub77c\uace0 \ubcf4\ub294 \uac83\uc774 \uc88b\uaca0\uc2b5\ub2c8\ub2e4.  </p> <ul> <li>KMMLU: \ud55c\uad6d\uc5b4\ub85c \uc7ac\uad6c\uc131\ub41c MMLU\ub85c, 45\uac1c \uce74\ud14c\uace0\ub9ac\uc758 \uc804\ubb38 \uc9c0\uc2dd\uc744 \ud3c9\uac00</li> <li>LogicKor: LMSYS\uc758 MT-Bench\ub97c \ud55c\uad6d\uc5b4\ub85c \uc801\uc751\ud55c \ub2e4\uc911 \ud134 \ub300\ud654 \ubca4\uce58\ub9c8\ud06c</li> <li>Open Ko-LLM Leaderboard: \ud55c\uad6d\uc5b4 LLM\uc744 \uc704\ud55c \uc885\ud569 \ub7ad\ud0b9 \uc2dc\uc2a4\ud15c</li> <li>HAE-RAE Bench: \ud55c\uad6d \ubb38\ud654 \uc9c0\uc2dd QA \ubca4\uce58\ub9c8\ud06c</li> </ul>"},{"location":"evaluation/benchmarks/#3","title":"3. \ubca4\uce58\ub9c8\ud06c\uc758 \uc2e4\ud6a8\uc131\uc5d0 \ub300\ud55c \ub17c\uc758","text":"<p>\ubca4\uce58\ub9c8\ud06c \uc131\uc801\uc774 \uc88b\uc740 \uac83\uc774 \uc815\ub9d0 \uc88b\uc740 \ubaa8\ub378\uc77c\uae4c\uc694 ?? </p> <p>\uc774 \ubd80\ubd84\uc5d0 \ub300\ud574 \ub9ce\uc740 \ubd84\ub4e4\uc774 \uc758\ubb38\uc744 \uac00\uc9d1\ub2c8\ub2e4. \ub9ce\uc774\ub4e4 \ub290\ub07c\uc168\uc744 \uac70\uc5d0\uc694. \ubd84\uba85 \ubca4\uce58 \uc22b\uc790\ub294 \uc88b\uc740\ub370 \uc65c \uc2e4 \uccb4\uac10\uc740 \ubcc4\ub85c\uc9c0...? \uc81c \uc758\uacac\uc73c\ub85c\ub294 \ubca4\uce58\uc5d0 overfit \ub418\uc5b4 \uc788\uc5b4\uc11c \ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.   </p> <p>MMLU \uc640 \uac19\uc740 \uc9c0\uc2dd\uc740 \ubb3b\ub294 \ubca4\uce58\ub294 \ub2f9\uc5f0\ud788 \uc138\uc0c1\uc758 \ubaa8\ub4e0 \uc9c8\ubb38\uc744 \ucee4\ubc84\ud558\uc9c0 \ubabb \ud569\ub2c8\ub2e4, \uc774\ubbf8 \ubb38\uc81c \uc815\ub2f5\uc9c0\uac00 \uacf5\uac1c\ub41c \uc774\uc0c1 \uae30\ucd9c\uc744 \ub9ce\uc774 \ud480\uc5b4\ubcf8\ub2e4\uba74 \uc131\ub2a5\uc744 \uc62c\ub9b4 \uc218 \uc788\uaca0\uc8e0. \uc218\ud559\uc774\ub098 \ubb50 \ub2e4\ub978 \ubca4\uce58\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc5d0\uc694. LLM \uc774 \ud55c\ubc88\ub3c4 \uc548 \ud480\uc5b4\ubcf8 \ubca4\uce58 \ubb38\uc81c\ub85c \ud14c\uc2a4\ud2b8\ud558\uba74 \uc131\uc801\ud45c\uac00 \ub9ce\uc774 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4. </p>"},{"location":"evaluation/benchmarks/#open-ko-llm-leaderboard-logickor","title":"Open Ko-LLM Leaderboard \uc640 LogicKor","text":"<p>\uc704 \uc758\ubb38\uc744 \uc798 \ubcf4\uc5ec\uc8fc\ub294 \uc0ac\ub840\uac00 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. </p> <ul> <li>\ud55c\uad6d\uc5b4 \ub9ac\ub354\ubcf4\ub4dc\ub294 \uc8fd\uc5c8\ub2e4 (24.03)</li> </ul> <p>\uc704 \ub9c1\ud06c\ub97c \ud55c\ubc88 \uc77d\uc5b4\ubcf4\uc2dc\ub294 \uac78 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4. \uc694\uc57d\ud558\uba74, \uc2e4\uccb4\uac10\uacfc \ubca4\uce58 \uc810\uc218 (\uc5ec\uae30\uc11c\ub294 Open Ko-LLM Leaderboard) \uc758 \uad34\ub9ac\uac00 \ud06c\ub2e4\ub294 \uc810\uc774\uace0. \uadf8\ub798\uc11c \uc0c8\ub85c \ubca4\uce58 (\uc5ec\uae30\uc11c\ub294 LogicKor)\ub97c \ub9cc\ub4e4\uc5b4 \ub3cc\ub838\ub354\ub2c8 \uc2e4\uccb4\uac10\uacfc \ube44\uc2b7\ud558\ub354\ub77c\ub294 \uc0ac\ub840\uc785\ub2c8\ub2e4. \uc601\uc5b4\uad8c\uc5d0\uc11c\ub3c4 \ub2f9\uc5f0\ud788 \ube44\uc2b7\ud55c \ubb38\uc81c\uac00 \uc81c\uae30\uac00 \ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. Pretraining on the Test Set Is All You Need \ud398\uc774\ud37c\ub294 \ud14c\uc2a4\ud2b8 \ubca4\uce58\ub97c \uc9c1\uc811 \ud559\uc2b5\ud558\ub294 \uac83 (\uae30\ucd9c \ubb38\uc81c\ub97c \ud480\uc5b4\ubcf4\uace0 \uc2dc\ud5d8 \ubcf4\ub294 \uac83) \uc5d0 \ub300\ud55c \ube44\ud310\uc785\ub2c8\ub2e4.   </p> <ul> <li>LogicKor \uc6b4\uc601 \uc911\uc9c0</li> </ul> <p>\uc0c8\ub86d\uac8c \ub9cc\ub4e4\uc5b4\uc9c4 \ubca4\uce58\ub3c4 \ub2f9\uc5f0\ud788 \uacf5\uac1c\uac00 \ub418\uace0 \ub098\uba74 \uc774\ub97c \ud559\uc2b5\ud558\uace0 \uc0c8\ub85c\uc6b4 \ubaa8\ub378\ub4e4\uc5d0\uac8c \ub6ab\ub9ac\uae30 \ub9c8\ub828\uc785\ub2c8\ub2e4. \ube44\uc2b7\ud55c \ub9e5\ub77d\uc73c\ub85c \uc6b4\uc601\uc740 \uc911\uc9c0\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. </p> <ul> <li>2025 \uc218\ub2a5 \uc5b8\uc5b4\uc601\uc5ed \ubca4\uce58</li> </ul> <p>\ube44\uc2b7\ud55c \ub9e5\ub77d\uc73c\ub85c Marker AI \uc5d0\uc11c \ub9cc\ub4e4\uc5b4\uc8fc\uc2e0 24\ub144 \ub9d0 \uc2e4\uc2dc\ub41c \uc218\ub2a5 \uc5b8\uc5b4\uc601\uc5ed\uc5d0 \ub300\ud55c \ubca4\uce58\uc640 \uadf8 \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc0c8\ub85c \ub098\uc628 \ubb38\uc81c\ub4e4\uc740 \uc544\uc9c1 \ud559\uc2b5\ud55c LLM \uc774 \uc5c6\uae30 \ub584\ubb38\uc5d0 \uc815\ub2f9\ud55c \ud3c9\uac00 \uc694\uc18c\ub85c \uc0ac\uc6a9\ub420 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.   </p> <p>\ucc38\uace0\ub85c LogicKor \ub97c \ub9cc\ub4dc\uc2e0 maywell \uc120\uc0dd\ub2d8\uc740 \uace0\ub4f1\ud559\uc0dd \uc774\uc2dc\ub124\uc694?! \ub300\ub2e8\ud558\uc2dc\uad70\uc694 \ud06c\ud760...</p>"},{"location":"evaluation/benchmarks/#_3","title":"\ucd5c\uadfc \ubc1c\uc804 \ubc0f \ucd94\uac00 \ubca4\uce58\ub9c8\ud06c","text":"<p>\ucd5c\uadfc\uc5d0\ub294 \ub2e8\uc21c \uc9c0\uc2dd\uacfc \ub2a5\ub825\uc744 \ub118\uc5b4\uc11c \ubcf4\ub2e4 \ub2e4\uc591\ud55c \ud3c9\uac00\uc694\uc18c\ub4e4\uc774 \ub5a0\uc624\ub974\uace0 \uc788\uc2b5\ub2c8\ub2e4. Reasoning \uc774\ub098 AGI, \uba40\ud2f0\ubaa8\ub2ec\ub9ac\ud2f0, \uc774\ub7f0 \uc694\uc18c\ub4e4\uc744 \ub2e4\ub8e8\uace0 \uc788\uc2b5\ub2c8\ub2e4. GPT4.5\ub294 EQ \ub97c \uac15\uc870\ud558\uae30\ub3c4 \ud588\uc8e0...</p> <ul> <li>HELM (Holistic Evaluation of Language Models): \uc815\ud655\uc131, \uacac\uace0\uc131, \uacf5\uc815\uc131 \ub4f1 \ub2e4\uc591\ud55c \ucc28\uc6d0\uc5d0\uc11c LLM\uc744 \ud3c9\uac00</li> <li>\ub3c4\uba54\uc778\ubcc4 \ubca4\uce58\ub9c8\ud06c:</li> <li>\ucf54\ub4dc \ubc0f Reasoning: HumanEval-X, APPS, LeetCode \ud3c9\uac00</li> <li>\uba40\ud2f0\ubaa8\ub2ec: MME, VQAv2</li> <li>\uc0c1\ud638\uc791\uc6a9 \uc5d0\uc774\uc804\ud2b8: WebArena, MATH-GPT</li> <li>\uc548\uc804\uc131 \ubc0f \uc815\ub82c: toxicity \ud3c9\uac00, \ud0c8\uc625 \ud14c\uc2a4\ud2b8</li> </ul>"},{"location":"evaluation/benchmarks/#_4","title":"\ucc38\uace0\ubb38\ud5cc","text":"<ol> <li>LMSYS Org, \"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings\", 2023.</li> <li>LMSYS Org, \"From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline\", 2024.</li> <li>EleutherAI, \"Language Model Evaluation Harness\", 2023.</li> <li>Stanford CRFM, \"Holistic Evaluation of Language Models (HELM)\", 2023.</li> <li>Kim et al., \"KMMLU: Korean Massive Multitask Language Understanding\", 2023.</li> <li>Lee et al., \"Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark\", 2024.</li> </ol>"},{"location":"evaluation/llm_as_judge/","title":"LLM as Judge \uc640 \ud3c9\uac00 \ud504\ub808\uc784\uc6cc\ud06c","text":""},{"location":"evaluation/llm_as_judge/#llm-as-judge_1","title":"LLM as Judge","text":"<p>LLM \uc774 \ucc44\uc810\uc744 \ud569\ub2c8\ub2e4. \ud3c9\uac00\uc790\ub85c\uc11c LLM \uc744 \uc0ac\uc6a9\ud558\uace0, \uae30\uc900\ub3c4 \ub123\uc5b4\uc8fc\uace0 LLM \uc774 \ud3c9\uac00 \uc810\uc218\uc640 \uc774\uc720\uae4c\uc9c0 \ub0b4\uc90d\ub2c8\ub2e4. (\uc2e4\uc81c \uc0ac\ub78c) \uc804\ubb38\uac00\ubcf4\ub2e4 \ub354 \uc798 \ud3c9\uac00\ud55c\ub2e4\uba74 \uac15\ub825\ud558\uace0, \uc2dc\uac04\ub3c4 \ub2e8\ucd95\ub418\uace0, \ube44\uc6a9\ub3c4 \uc904\uace0, \uc88b\uaca0\uc8e0. \ud3c9\uac00\ub97c \uc798 \ud558\uac8c \ub9cc\ub4dc\ub294 \uac83 \ub610\ud55c \ud558\ub098\uc758 LLM application \uc774\ub77c, \ubc30\ubcf4\ub2e4 \ubc30\uaf3d\uc774 \ub354 \ucee4\uc9c8 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. (\uc694\uc998\uc5d4 GPT4.5 \ub97c \ud544\ub450\ub85c LLM \uc774 \ube44\uc2f8\uc838\uc11c \uc0ac\ub78c\uc774 \ub354 \uc2fc \uacbd\uc6b0\ub3c4 \uc788\ub294 \uac83 \uac19\ub124\uc694 ... ;;)</p>"},{"location":"evaluation/llm_as_judge/#_1","title":"\ud3c9\uac00 \uc608\uc2dc","text":""},{"location":"evaluation/llm_as_judge/#reference-free","title":"reference-free \ud55c \ud3c9\uac00","text":"<p>\ub2f5\ubcc0\uc774 \uacf5\uaca9\uc131\uc774 \uc788\ub294 \uc9c0, \ubd80\uc815\uc801\uc778\uc9c0, \uac04\uacb0\ud55c\uc9c0, \ub4f1 \uc815\ub2f5\uc9c0\uc640 \ubb34\uad00\ud55c \ud3c9\uac00\ub97c \uc9c4\ud589\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc8fc\uad00\uc801\uc73c\ub85c \ud3c9\uac00\ud574\uc57c\ud558\ub294 \uac83\ub4e4\uc774\ub77c LLM \uc774 \ud3c9\uac00\ud558\ub294 \uac83\uc774 \uc88b\uc544\uc694.  </p> <p>LangSmith \uc640 \uac19\uc740 \ub3c4\uad6c\uc5d0\uc11c\ub294 conciseness, harmfulness, maliciousness \ub4f1 \uc5ec\ub7ec \uae30\uc900\uc5d0 \ub300\ud574 \ud3c9\uac00\ud558\ub294 LLM judge \ub97c \ud15c\ud50c\ub9bf \ucc98\ub7fc \ub9cc\ub4e4\uc5b4\uc11c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub9e4\uc6b0 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc6d0\ubc84\ud2bc\uc5d0 \ub2e4\uc591\ud55c \ud3c9\uac00\ub97c \ubc14\ub85c \uc218\ud589\uac00\ub2a5\ud558\ub2c8\uae4c\uc694.</p> <p>LangSmith \uc5d0\uc11c \ud504\ub86c\ud504\ud305\ud55c reference-free \ud3c9\uac00 \uc608\uc2dc\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> <p>\ucd9c\ucc98 - Answer Helpfulness</p> <p><pre><code>system\n\nYou are a teacher grading a quiz. \nYou will be given a QUESTION and a STUDENT ANSWER. \nHere is the grade criteria to follow:\n(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\n(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\n\nScore:\nA score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \nA score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n\nAvoid simply stating the correct answer at the outset.\n</code></pre> <pre><code>human\n\nSTUDENT ANSWER: {{student_answer}}\nQUESTION: {{question}}\n</code></pre></p> <p>\uc774 \ud15c\ud50c\ub9bf\uc5d0 \uc9c8\ubb38\uacfc LLM\uc758 \uc751\ub2f5\uc744 \ub123\uc5b4\uc8fc\uba74, LLM \uc740 \ub2f5\ubcc0\uc774 helpful \ud588\ub294\uc9c0 \ud3c9\uac00\ub97c \ud574\uc90d\ub2c8\ub2e4.</p>"},{"location":"evaluation/llm_as_judge/#reference","title":"reference \uac00 \ud544\uc694\ud55c \ud3c9\uac00","text":"<p>\ub370\uc774\ud130\uc14b\uc758 \uc801\ud600\uc788\ub294 \uc608\uc0c1 \uacb0\uacfc\uac12\uacfc \ube44\uad50\ud558\uc5ec \ud3c9\uac00\ub97c \ub0b4\ub9b4 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. LLM application \uc5d0 \ub530\ub77c \ud3c9\uac00 \ub85c\uc9c1 \uac1c\ubc1c\uc774 \uc5b4\ub824\uc6b8 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>LangSmith \uc640 \uac19\uc740 \ub3c4\uad6c\uc5d0\uc11c\ub294  QA correctness, Context QA, Chain of Thought QA \uc640 \uac19\uc774 \uc5bc\ub9c8\ub098 \ub300\ub2f5\uc774 \uc815\ud655\ud55c\uc9c0 reference \uc640 \ube44\uad50\ud558\uc5ec \uacb0\uacfc\ub97c \ub0b4\ub9ac\uc8fc\uae30\ub3c4 \ud569\ub2c8\ub2e4.  </p> <p>\uc704 \uc608\uc2dc\uc640 \ube44\uc2b7\ud558\uac8c LangSmith \uc5d0\uc11c \ud504\ub86c\ud504\ud305\ud55c reference-related \ud3c9\uac00 \uc608\uc2dc\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ucd9c\ucc98 - Answer Correctness</p> <p><pre><code>system\n\nYou are a teacher grading a quiz. \n\nYou will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n\nHere is the grade criteria to follow:\n(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n(2) Ensure that the student answer does not contain any conflicting statements.\n(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n\nScore:\nA score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \nA score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n\nAvoid simply stating the correct answer at the outset.\n</code></pre> <pre><code>human\nQUESTION: {{question}}\nGROUND TRUTH ANSWER: {{correct_answer}}\nSTUDENT ANSWER: {{student_answer}}\n</code></pre></p> <p>\uc774 \ud15c\ud50c\ub9bf\uc5d0 \uc9c8\ubb38\uacfc \ub098\uc640\uc57c\ud560 \uc815\ub2f5, \uadf8\ub9ac\uace0 LLM\uc758 \uc751\ub2f5\uc744 \ub123\uc5b4\uc8fc\uba74, LLM \uc740 \ub2f5\ubcc0\uc774 \ub9de\uc558\ub294\uc9c0 \ud3c9\uac00\ud574\uc90d\ub2c8\ub2e4.</p>"},{"location":"evaluation/llm_as_judge/#-llm-llm","title":"\ucca8\uc5b8 - LLM \ub9cc\uc744 \uc704\ud55c \uac83\uc774 \uc544\ub2cc LLM \uc2dc\uc2a4\ud15c\uc744 \uc704\ud55c \uac83","text":"<p>\uc704 \ud3c9\uac00 \uc608\uc2dc\ub098 \ub2f5\ubcc0\uc744 \ubcf4\uc2dc\uba74 \uc544\uc2dc\uaca0\uc9c0\ub9cc, LLM \ubaa8\ub378 \uadf8 \uc790\uccb4\ub9cc\uc744 \uc704\ud55c \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. Embedding \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c \uac80\uc0c9\ud55c \uacb0\uacfc\uac00 \uad1c\ucc2e\uc740\uc9c0 \ud3c9\uac00\ub97c \ud560 \uc218\ub3c4 \uc788\uace0\uc694, RAG \uc2dc\uc2a4\ud15c\uc758 \uc804\ubc18\uc801\uc778 \uc131\ub2a5 \ud3c9\uac00\uc77c \uc218\ub3c4 \uc788\uace0, LLM Application \uc758 \uc2e4\ud589 \uc911\uc5d0 \uc911\uac04 \ud3c9\uac00\ub85c \uc0ac\uc6a9\ub420 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. LLM \uc774\ub791 \uc5f0\uad00\ub418\uc9c0 \uc54a\uc740 In/Out \uc744 LLM \uc73c\ub85c \ud3c9\uac00\ud558\ub294 \uac83\ub3c4 \uac00\ub2a5\ud558\uc8e0. \ud559\uc0dd\ub4e4\uc758 \ub17c\uc220 \ucc44\uc810\uc73c\ub85c\ub3c4 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uaca0\ub124\uc694.  </p> <p>\ubcf8 \ubb38\uc11c\ub294 LLM \ud29c\ub2dd\uc744 \uc704\ud55c \ubb38\uc11c\uc774\uc9c0\ub9cc, \ud3c9\uac00 \uae30\ubc95 \uc790\uccb4\ub294 \ud6e8\uc52c \ubc94\uc6a9\uc131\uc774 \ub6f0\uc5b4\ub098\ub2e4\ub294 \uc810\uc744 \uc9da\uace0 \ub118\uc5b4\uac11\ub2c8\ub2e4. </p>"},{"location":"evaluation/llm_as_judge/#frameworks","title":"Frameworks","text":"<p>\uc774\ub97c \uc704\ud55c \uc5ec\ub7ec \ud504\ub808\uc784\uc6cc\ud06c\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc800\ub294 \uac1c\uc778\uc801\uc73c\ub85c LangSmith\ub97c \uc120\ud638\ud569\ub2c8\ub2e4. \uac1c\ubc1c \uacbd\ud5d8\uc774 \uc88b\uc558\uace0, \uc2e0\uae30\ub2a5\uc774 \uac70\uc758 \uc81c\uc77c \ube68\ub9ac \ub4e4\uc5b4\uc624\uace0, \ud3c9\uac00 \ubfd0 \uc544\ub2c8\ub77c \ub370\uc774\ud130 \uad00\ub9ac \uce21\uba74\uc5d0\uc11c\ub3c4 \uc88b\uc740 \uba74\uc774 \uc788\uc5b4\uc11c\uc694. \ub2e8\uc810\uc740 trial \uc744 \uc81c\uc678\ud558\uba74 \uc720\ub8cc\uc785\ub2c8\ub2e4... </p>"},{"location":"evaluation/llm_as_judge/#langsmith","title":"LangSmith","text":"<p>LangSmith\ub294 LangChain \ud300\uc774 \uac1c\ubc1c\ud55c \ud3c9\uac00 \ubc0f \uad00\uce21(Observability) \ud50c\ub7ab\ud3fc\uc73c\ub85c, LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \uc791\ub3d9\uc744 \ucd94\uc801\ud558\uace0 \ud488\uc9c8\uc744 \ubaa8\ub2c8\ud130\ub9c1\ud558\ub294 \ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4. \ucc38\uace0\ub85c LangSmith\ub294 \uc81c\uac00 \ub530\ub85c \uc791\uc131\ud55c \uc0ac\uc6a9\ubc95 \ubb38\uc11c\ub3c4 \uc788\uc73c\ub2c8 \ud544\uc694\ud558\uc2e0 \ubd84\ub4e4\uc740 \ucc38\uc870\ud558\uc138\uc694.</p> <ul> <li>\ubcf5\uc7a1\ud55c \ud504\ub86c\ud504\ud2b8 \uc2dc\ud000\uc2a4 \ucd94\uc801 \ubc0f \ud488\uc9c8 \ud3c9\uac00\uac00 \uc798 \uc900\ube44\ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4. </li> <li>\ud3b8\ud5a5\uc131(Bias) \ud0d0\uc9c0\ub098 \uc548\uc804\uc131 \uac80\ud1a0 \uac19\uc740 \ud2b9\ud654\ub41c \ud3c9\uac00 \uae30\ub2a5 \ud3ec\ud568</li> <li>Hosted \uc11c\ube44\uc2a4 \ud615\ud0dc\ub85c \uc81c\uacf5, Closed source, \uc720\ub8cc\ub85c \uc0ac\uc6a9\ud558\uba74 \uc124\uce58\ud615\uc73c\ub85c \uc9c1\uc811 \ud638\uc2a4\ud305 \ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. (\ube44\uc2b7\ud55c \uc624\ud508\uc18c\uc2a4\ub4e4\ub3c4 \ub9ce\uc774 \uc788\uc5b4\uc694. LangFuse \ub77c\ub358\uac00...)</li> <li>LangChain \uae30\ubc18 workflow\uc640 \uc6d0\ud65c\ud558\uac8c \uc5f0\uacc4</li> <li>LLM \uc0c1\ud638\uc791\uc6a9\uacfc \ud504\ub86c\ud504\ud2b8 \uccb4\uc778\uc744 \ub85c\uadf8\ub85c \uae30\ub85d\ud558\uace0 \ubd84\uc11d\ud558\ub294 \ub3c4\uad6c \uc81c\uacf5</li> </ul> <p>LangSmith\ub294 \uc8fc\ub85c LangChain\uc73c\ub85c \ub2e4\ub2e8\uacc4 \uccb4\uc778\uc744 \uad6c\uc131\ud55c LLM \uc571 \uac1c\ubc1c \uc2dc \ub514\ubc84\uae45\uacfc \ud488\uc9c8 \ud3c9\uac00\uc5d0 \ud65c\uc6a9\ub429\ub2c8\ub2e4. Fine-Tuning \uc758 \uad00\uc810\uc5d0\uc11c \ubcf4\uba74, \ubaa8\ub378\uc744 \ud29c\ub2dd\ud574\uc11c \"\ubaa8\ub378\ub9cc \ubc30\ud3ec\ud558\uace0 \ub098\ub294 \uc774\uc81c \ub05d!\" \ud558\ub294 \uacbd\uc6b0\ub294 \ubcc4\ub85c \uc5c6\uaca0\uc8e0.  \uacb0\uad6d \ubaa8\ub378\uc740 \uc2dc\uc2a4\ud15c \uc18d\uc73c\ub85c \ub4e4\uc5b4\uac00\uc11c \uc77c\uc744 \ud558\uac8c \ub420\ud150\ub370\uc694, \uadf8 \uc2dc\uc2a4\ud15c \uad00\uc810\uc5d0\uc11c \ud3c9\uac00\ub97c \ud558\uac8c \ub418\uae30 \ub584\ubb38\uc5d0 \uc720\uc6a9\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub77c\uace0 \ubcfc \uc218 \uc788\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"evaluation/llm_as_judge/#openai-evals","title":"OpenAI Evals","text":"<p>OpenAI Evals\ub294 OpenAI\uac00 \uc790\uccb4 \ubaa8\ub378 \ud3c9\uac00\ub97c \uc704\ud574 \uac1c\ubc1c\ud558\uc5ec \uc624\ud508\uc18c\uc2a4\ub85c \uacf5\uac1c\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. LangSmith \ucc98\ub7fc \uc2dc\uc2a4\ud15c\uc744 \ud3c9\uac00\ud55c\ub2e4\uae30 \ubcf4\ub2e4\ub294 \ubaa8\ub378 \uadf8 \uc790\uccb4\uc758 \ud3c9\uac00\uc5d0 \ub354 \uc9d1\uc911\ud55c \ud504\ub808\uc784\uc6cc\ud06c\ub77c\uace0 \ubcf4\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4.  </p> <p>\ucc38\uc870 - GitHub \uc800\uc7a5\uc18c</p> <ul> <li>\ub2e4\uc591\ud55c \ubca4\uce58\ub9c8\ud06c \ud3c9\uac00 \uc138\ud2b8\uc758 \ub808\uc9c0\uc2a4\ud2b8\ub9ac \uc81c\uacf5</li> <li>GPT \uc2dc\ub9ac\uc988\ub97c \ube44\ub86f\ud55c LLM\ub4e4\uc758 \ub2e4\uc591\ud55c \uce21\uba74\uc758 \uc131\ub2a5 \ud14c\uc2a4\ud2b8 \uac00\ub2a5</li> <li>\uc0ac\uc6a9\uc790\uac00 \uc790\uc2e0\ub9cc\uc758 \ud3c9\uac00(Eval)\ub97c \uc791\uc131\ud558\uc5ec \ud2b9\uc815 \ud65c\uc6a9 \uc0ac\ub840\uc5d0 \ub300\ud55c \ub9de\ucda4 \ud14c\uc2a4\ud2b8 \uc0dd\uc131 \uac00\ub2a5</li> <li>\ucee4\uc2a4\ud140 \ud3c9\uac00\ub3c4 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\uc5d0 \ud1b5\ud569\ud558\uc5ec \ud65c\uc6a9 \uac00\ub2a5</li> </ul> <p>\uac1c\ubc1c\uc790\ub4e4\uc740 OpenAI Evals\ub97c \ud65c\uc6a9\ud574 \ud2b9\uc815 \uc791\uc5c5(\uc608: \ub3c4\uba54\uc778\ubcc4 Q&amp;A)\uc5d0 \ub300\ud574 \ub2e4\uc591\ud55c \ubaa8\ub378\uc744 \ub3d9\uc77c\ud55c \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub85c \ud3c9\uac00\ud558\uace0 \ube44\uad50\ud569\ub2c8\ub2e4. \ubaa8\ub378 \uc2e0\uaddc \ubc84\uc804 \ucd9c\uc2dc \uc2dc \uc774\uc804 \ubc84\uc804\uacfc\uc758 \uc131\ub2a5 \ucc28\uc774\ub97c \uac80\uc99d\ud558\uac70\ub098, \ubbf8\uc138\uc870\uc815\ub41c \ubaa8\ub378\uc774 \uae30\uc874 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\ub294\uc9c0 \ud655\uc778\ud558\ub294 \ub4f1 \ud68c\uadc0 \uac80\uc0ac \uc6a9\ub3c4\ub85c\ub3c4 \ud65c\uc6a9\ub429\ub2c8\ub2e4.</p>"},{"location":"evaluation/llm_as_judge/#ragas","title":"Ragas","text":"<p>Ragas\ub294 Retrieval-Augmented Generation(RAG) \ud30c\uc774\ud504\ub77c\uc778 \ud3c9\uac00\uc5d0 \ud2b9\ud654\ub41c \uc624\ud508\uc18c\uc2a4 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4. \uc774\ub984 \uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774, \ubaa8\ub378 \uadf8 \uc790\uccb4 \ubcf4\ub2e4\ub294 RAG \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\ub294\ub370 \ud2b9\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  </p> <ul> <li>\ubb38\uc11c \uac80\uc0c9 \ud6c4 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub294 QA \uc2dc\uc2a4\ud15c\uc758 \uc131\ub2a5\uc744 \uc815\ub7c9\ud654\ud558\uae30 \uc704\ud574 \uc124\uacc4</li> <li>\ub2e4\uc12f \uac00\uc9c0 \ud575\uc2ec \uc9c0\ud45c \uc81c\uacf5:</li> <li>Faithfulness (\ucda9\uc2e4\ub3c4)</li> <li>Contextual Relevancy (\ubb38\ub9e5 \uad00\ub828\uc131)</li> <li>Answer Relevancy (\ub2f5\ubcc0 \uc801\ud569\uc131)</li> <li>Contextual Recall (\ubb38\ub9e5 \uc7ac\ud604\uc728)</li> <li>Contextual Precision (\ubb38\ub9e5 \uc815\ud655\ub3c4)</li> <li>\ucd5c\uc2e0 RAG \uc5f0\uad6c\uc5d0 \uae30\ubc18\ud55c \uc885\ud569\uc801\uc778 \ud3c9\uac00 \uc9c0\ud45c \uc81c\uacf5</li> </ul> <p>Ragas\ub294 RAG \uae30\ubc18\uc758 \uc9c0\uc2dd \uac80\uc0c9 \ucc57\ubd07\uc774\ub098 \ubb38\uc11c QA \uc2dc\uc2a4\ud15c\uc5d0\uc11c \ub9ce\uc774 \ud65c\uc6a9\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc0ac\ub0b4 \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc9c8\uc758\uc751\ub2f5 \uc2dc\uc2a4\ud15c\uc5d0 \uc801\uc6a9\ud558\uba74 \ubaa8\ub378\uc758 \ub2f5\ubcc0\uc774 \uc8fc\uc5b4\uc9c4 \ubb38\uc11c\uc5d0 \uc5bc\ub9c8\ub098 \ucda9\uc2e4\ud55c\uc9c0, \ubb38\uc11c \uac80\uc0c9\uc774 \uc5bc\ub9c8\ub098 \uc801\uc808\ud588\ub294\uc9c0\ub97c \uc218\uce58\ub85c \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\ub9cc, Ragas\uc758 \uc9c0\ud45c \uc774\ub984\ub4e4\uc774 \uc9c1\uad00\uc801\uc774\uc9c0 \uc54a\uc544 \uc810\uc218\uac00 \ub0ae\uc744 \ub54c \uad6c\uccb4\uc801\uc778 \ubb38\uc81c \ud30c\uc545\uc744 \uc704\ud574 \ucd94\uac00\uc801\uc778 \ubd84\uc11d\uc774 \ud544\uc694\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"evaluation/serving_optimization/","title":"\ubaa8\ub378 \uc11c\ube59 \ubc0f \ucd5c\uc801\ud654","text":""},{"location":"evaluation/serving_optimization/#_2","title":"\uc11c\ube59 \uc2dc\uc2a4\ud15c \uad6c\ucd95","text":"<ul> <li>\ubaa8\ub378 \ubc30\ud3ec \uc544\ud0a4\ud14d\ucc98</li> <li>\ud655\uc7a5\uc131 \uace0\ub824\uc0ac\ud56d</li> <li>\ubaa8\ub2c8\ud130\ub9c1 \uc124\uc815</li> </ul>"},{"location":"evaluation/serving_optimization/#vllm","title":"vLLM \ud65c\uc6a9","text":"<ul> <li>vLLM \uc18c\uac1c \ubc0f \uc7a5\uc810</li> <li>PagedAttention \uba54\ucee4\ub2c8\uc998</li> <li>\uc124\uce58 \ubc0f \uad6c\uc131 \ubc29\ubc95</li> </ul>"},{"location":"evaluation/serving_optimization/#_3","title":"\ucd94\ub860 \ucd5c\uc801\ud654 \uae30\ubc95","text":"<ul> <li>\ubc30\uce58 \ucc98\ub9ac</li> <li>KV \uce90\uc2f1</li> <li>\uc591\uc790\ud654\ub97c \ud1b5\ud55c \ucd94\ub860 \uac00\uc18d</li> </ul>"},{"location":"evaluation/serving_optimization/#_4","title":"\uc11c\ube59 \uc2dc \uace0\ub824\uc0ac\ud56d","text":"<ul> <li>\uc9c0\uc5f0 \uc2dc\uac04 \ucd5c\uc801\ud654</li> <li>\ucc98\ub9ac\ub7c9 \ud5a5\uc0c1</li> <li>\uc790\uc6d0 \ud65c\uc6a9 \ud6a8\uc728\ud654 </li> </ul>"},{"location":"practice_guides/dpo_tutorial/","title":"DPO\ub97c \ud65c\uc6a9\ud55c \ud29c\ub2dd \uc2e4\uc2b5","text":""},{"location":"practice_guides/dpo_tutorial/#dpo_1","title":"DPO \ud658\uacbd \uc124\uc815","text":"<ul> <li>\ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac</li> <li>\ub370\uc774\ud130 \ud615\uc2dd \uc694\uad6c\uc0ac\ud56d</li> <li>\ubca0\uc774\uc2a4 \ubaa8\ub378 \uc900\ube44</li> </ul>"},{"location":"practice_guides/dpo_tutorial/#_1","title":"\uc120\ud638\ub3c4 \ub370\uc774\ud130 \uc900\ube44","text":"<ul> <li>\uc120\ud638/\ube44\uc120\ud638 \uc751\ub2f5 \uc30d \uad6c\uc131</li> <li>\ub370\uc774\ud130 \ud615\uc2dd \ub9de\ucd94\uae30</li> <li>\ub370\uc774\ud130 \ud488\uc9c8 \uac80\uc99d</li> </ul>"},{"location":"practice_guides/dpo_tutorial/#dpo_2","title":"DPO \ud29c\ub2dd \uc2e4\ud589","text":"<ul> <li>DPOTrainer \uc124\uc815</li> <li>\ubca0\ud0c0 \ud30c\ub77c\ubbf8\ud130 \uc870\uc815</li> <li>\ud559\uc2b5 \uc9c4\ud589</li> </ul>"},{"location":"practice_guides/dpo_tutorial/#_2","title":"\ubaa8\ub378 \ud3c9\uac00 \ubc0f \ube44\uad50","text":"<ul> <li>\uc120\ud638\ub3c4 \ud559\uc2b5 \ud6a8\uacfc \uce21\uc815</li> <li>SFT \ubaa8\ub378\uacfc \ube44\uad50</li> <li>\uc0ac\uc6a9\uc790 \ud53c\ub4dc\ubc31 \uc218\uc9d1 </li> </ul>"},{"location":"practice_guides/gpt_finetuning/","title":"OpenAI GPT \ubaa8\ub378 \ud30c\uc778\ud29c\ub2dd","text":"<p>OpenAI \ub294 GPT \ubaa8\ub378\ub4e4\uc758 Fine Tuning \uae30\ub2a5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. </p>"},{"location":"practice_guides/gpt_finetuning/#openai-fine-tuning-api","title":"OpenAI Fine-Tuning API \uc18c\uac1c","text":""},{"location":"practice_guides/gpt_finetuning/#_1","title":"\uc9c0\uc6d0\ub418\ub294 \ubaa8\ub378","text":"<p>\ud30c\uc778\ud29c\ub2dd\uc740 \ud604\uc7ac \ub2e4\uc74c \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4:</p> <ul> <li>gpt-4o-2024-08-06</li> <li>gpt-4o-mini-2024-07-18</li> <li>gpt-4-0613</li> <li>gpt-3.5-turbo-0125</li> <li>gpt-3.5-turbo-1106</li> <li>gpt-3.5-turbo-0613</li> </ul> <p>\uc774\ubbf8 \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc744 \ub2e4\uc2dc \ud30c\uc778\ud29c\ub2dd\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ucd94\uac00 \ub370\uc774\ud130\ub97c \ud655\ubcf4\ud588\uc744 \ub54c \uc774\uc804 \ud559\uc2b5 \ub2e8\uacc4\ub97c \ubc18\ubcf5\ud558\uc9c0 \uc54a\uace0\ub3c4 \ubaa8\ub378\uc744 \uac1c\uc120\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p> <p>\ub300\ubd80\ubd84\uc758 \uc0ac\uc6a9\uc790\uc5d0\uac8c\ub294 \uc131\ub2a5, \ube44\uc6a9, \uc0ac\uc6a9 \ud3b8\uc758\uc131 \uce21\uba74\uc5d0\uc11c gpt-4o-mini\uac00 \uac00\uc7a5 \uc801\ud569\ud55c \ubaa8\ub378\uc785\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_2","title":"\uc694\uae08","text":"<p>25.03 \uae30\uc900 \uac00\uaca9\ud45c\uc785\ub2c8\ub2e4, \uac00\uaca9\uc740 \ubcc0\ub3d9\ub420 \uc218 \uc788\uc73c\ub2c8 \uaf2d \ud655\uc778\ud558\uc138\uc694. \uc9c1\uad00\uc801\uc73c\ub85c \ubcf4\uba74, inference output\uc758 2\ubc30 \uc815\ub3c4 \ube44\uc6a9\uc774 \ub4e0\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p> \ubaa8\ub378 \uc720\ud615 \uac00\uaca9 (USD/1M tokens) GPT-4o mini Input $0.30 Cached input $0.15 Output $1.20 Training $3.00 GPT-4o Input $3.750 Cached input $1.875 Output $15.000 Training $25.000 <p>(\ud1a0\ud070\ub2f9 \uae30\ubcf8 \ud559\uc2b5 \ube44\uc6a9) \u00d7 \uc785\ub825 \ud30c\uc77c\uc758 \ud1a0\ud070 \uc218 \u00d7 \ud559\uc2b5\ub41c Epoch</p> <p>(Ex) 10\ub9cc \ud1a0\ud070\uc774 \ud3ec\ud568\ub41c \ud559\uc2b5 \ud30c\uc77c\uc744 1 epoch \ud559\uc2b5\ud560 \uacbd\uc6b0 \uc608\uc0c1 \ube44\uc6a9 </p> <ul> <li>gpt-4o-mini-2024-07-18: \uc57d $0.30 USD</li> </ul>"},{"location":"practice_guides/gpt_finetuning/#_3","title":"\uc0ac\uc6a9 \uc81c\ud55c \uc0ac\ud56d","text":"<p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uc81c\ud55c\uc774 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li>\ucd5c\uc18c 10\uac1c\uc758 \uc608\uc81c\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.</li> <li>\ubaa8\ub378\ubcc4 \ucd5c\ub300 \ucee8\ud14d\uc2a4\ud2b8 \uae38\uc774\uac00 \ub2e4\ub985\ub2c8\ub2e4:</li> <li>gpt-4o \ubaa8\ub378: \ud559\uc2b5 65,536 \ud1a0\ud070 (128k\ub85c \ud655\uc7a5 \uc608\uc815, \ucc38\uace0\ub85c \ucd94\ub860\uc740 128k \ud1a0\ud070 \uc785\ub2c8\ub2e4.)</li> </ul> <p>\uc790\uc138\ud55c \uc81c\ud55c \uc0ac\ud56d\uc740 OpenAI \uc18d\ub3c4 \uc81c\ud55c \ud398\uc774\uc9c0\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_4","title":"\ub370\uc774\ud130 \uc900\ube44","text":""},{"location":"practice_guides/gpt_finetuning/#jsonl","title":"JSONL \ud3ec\ub9f7 \uc791\uc131","text":"<p>\ud30c\uc778\ud29c\ub2dd\uc744 \uc704\ud55c \ub370\uc774\ud130\ub294 JSONL \ud615\uc2dd\uc73c\ub85c \uc900\ube44\ud574\uc57c \ud569\ub2c8\ub2e4. \uac01 \uc904\uc740 \ud558\ub098\uc758 \ub300\ud654 \uc608\uc81c\ub97c \ub098\ud0c0\ub0b4\uba70, \ub2e4\uc74c\uacfc \uac19\uc740 \ud615\uc2dd\uc744 \ub530\ub985\ub2c8\ub2e4:</p> <pre><code>{\"messages\":\n [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n  {\"role\": \"user\", \"content\": \"Hello!\"}, {\"role\": \"assistant\",\n   \"content\": \"Hi there! How can I help you today?\"}]\n}\n</code></pre> <p>\uac01 \uc608\uc81c\ub294 Chat Completions API\uc640 \ub3d9\uc77c\ud55c \ud615\uc2dd\uc758 \ub300\ud654\uc5ec\uc57c \ud558\uba70, \uac01 \uba54\uc2dc\uc9c0\uc5d0\ub294 \uc5ed\ud560(role), \ub0b4\uc6a9(content), \uadf8\ub9ac\uace0 \uc120\ud0dd\uc801\uc73c\ub85c \uc774\ub984(name)\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_5","title":"\ub2e4\uc911 \ud134 \ub300\ud654 \uc608\uc81c","text":"<p>\ub300\ud654 \ud615\uc2dd\uc758 \uc608\uc81c\uc5d0\ub294 \uc5ec\ub7ec \uac1c\uc758 assistant \uc5ed\ud560 \uba54\uc2dc\uc9c0\uac00 \ud3ec\ud568\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c \ud30c\uc778\ud29c\ub2dd\uc740 \ud558\ub098\uc758 \uc608\uc81c \ub0b4\uc758 \ubaa8\ub4e0 assistant \uba54\uc2dc\uc9c0\uc5d0 \ub300\ud574 \ud559\uc2b5\ud569\ub2c8\ub2e4. \ud2b9\uc815 assistant \uba54\uc2dc\uc9c0\uc5d0 \ub300\ud55c \ud30c\uc778\ud29c\ub2dd\uc744 \uac74\ub108\ub6f0\ub824\uba74 weight \ud0a4\ub97c \ucd94\uac00\ud558\uc5ec \ud574\ub2f9 \uba54\uc2dc\uc9c0\uc5d0 \ub300\ud55c \ud559\uc2b5\uc744 \ube44\ud65c\uc131\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. weight\uc758 \ud5c8\uc6a9 \uac12\uc740 \ud604\uc7ac 0 \ub610\ub294 1\uc785\ub2c8\ub2e4.</p> <pre><code>{\"messages\":\n [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n  {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n  {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0},\n  {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"},\n  {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]\n}\n\n{\"messages\": [\n  {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n  {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"},\n  {\"role\": \"assistant\", \"content\": \"William Shakespeare\", \"weight\": 0},\n  {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"},\n  {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\", \"weight\": 1}\n]}\n\n{\"messages\": [\n  {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n  {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"},\n  {\"role\": \"assistant\", \"content\": \"384,400 kilometers\", \"weight\": 0},\n  {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"},\n  {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\", \"weight\": 1}\n]}\n</code></pre>"},{"location":"practice_guides/gpt_finetuning/#_6","title":"\ub370\uc774\ud130 \uac80\uc99d","text":"<p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc744 \uc0dd\uc131\ud558\uae30 \uc804\uc5d0 \ub370\uc774\ud130 \ud615\uc2dd\uc744 \ud655\uc778\ud558\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4. OpenAI\ub294 \ub370\uc774\ud130 \ud615\uc2dd \uc624\ub958\ub97c \ucc3e\uace0, \ud1a0\ud070 \uc218\ub97c \uac80\ud1a0\ud558\uace0, \ud30c\uc778\ud29c\ub2dd \uc791\uc5c5 \ube44\uc6a9\uc744 \ucd94\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uac04\ub2e8\ud55c Python \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.</p> <p>\ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130 \ud615\uc2dd \uac80\uc99d\uc5d0\uc11c \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_7","title":"\ub370\uc774\ud130 \uc5c5\ub85c\ub4dc","text":"<p>\ub370\uc774\ud130\uac00 \uac80\uc99d\ub418\uba74 Files API\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud30c\uc77c\uc744 \uc5c5\ub85c\ub4dc\ud574\uc57c \ud569\ub2c8\ub2e4:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\nclient.files.create(\n  file=open(\"mydata.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\n</code></pre> <p>\ud30c\uc77c\uc744 \uc5c5\ub85c\ub4dc\ud55c \ud6c4 \ucc98\ub9ac\ud558\ub294 \ub370 \uc2dc\uac04\uc774 \uac78\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud30c\uc77c\uc774 \ucc98\ub9ac\ub418\ub294 \ub3d9\uc548\uc5d0\ub3c4 \ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc, \ud30c\uc77c \ucc98\ub9ac\uac00 \uc644\ub8cc\ub420 \ub54c\uae4c\uc9c0 \uc791\uc5c5\uc774 \uc2dc\uc791\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p> <p>\ucd5c\ub300 \uc5c5\ub85c\ub4dc \ud06c\uae30\ub294 Files API\ub97c \uc0ac\uc6a9\ud560 \uacbd\uc6b0 512MB\uc785\ub2c8\ub2e4. \ucd5c\ub300 8GB \ud06c\uae30\uc758 \ud30c\uc77c\uc740 Uploads API\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec \ubd80\ubd84\uc73c\ub85c \uc5c5\ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_8","title":"\ud30c\uc778\ud29c\ub2dd \uc2e4\ud589","text":""},{"location":"practice_guides/gpt_finetuning/#api","title":"API \ud638\ucd9c \ubc29\ubc95","text":"<p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc744 \uc0dd\uc131\ud558\ub824\uba74 OpenAI SDK\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc74c\uacfc \uac19\uc774 \ud638\ucd9c\ud569\ub2c8\ub2e4:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\nclient.fine_tuning.jobs.create(\n  training_file=\"file-abc123\",\n  model=\"gpt-3.5-turbo-0125\",\n  suffix=\"my-suffix\",\n  hyperparameters={\n    \"n_epochs\": 3\n  }\n)\n</code></pre> <p>\uc774 \uc608\uc81c\uc5d0\uc11c model\uc740 \ud30c\uc778\ud29c\ub2dd\ud558\ub824\ub294 \ubaa8\ub378\uc758 \uc774\ub984\uc774\uace0, training_file\uc740 OpenAI API\uc5d0 \uc5c5\ub85c\ub4dc\ub41c \ud559\uc2b5 \ud30c\uc77c\uc758 ID\uc785\ub2c8\ub2e4. suffix \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc758 \uc774\ub984\uc744 \uc0ac\uc6a9\uc790 \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ubc29\ubc95\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \uae30\ubcf8\uac12\uc740 \uc9c0\ub3c4 \ud30c\uc778\ud29c\ub2dd(Supervised Fine-Tuning, SFT)\uc785\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_9","title":"\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815","text":"<p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc5d0 \ub2e4\uc74c\uacfc \uac19\uc740 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li>epochs: \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc744 \uba87 \ubc88 \ubc18\ubcf5\ud560\uc9c0 \uacb0\uc815</li> <li>learning_rate_multiplier: \ud559\uc2b5\ub960 \uc870\uc815</li> <li>batch_size: \ubc30\uce58 \ud06c\uae30 \uc124\uc815</li> </ul> <p>\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>client.fine_tuning.jobs.create(\n  training_file=\"file-abc123\",\n  model=\"gpt-3.5-turbo-0125\",\n  hyperparameters={\n    \"n_epochs\": 3,\n    \"learning_rate_multiplier\": 1.0,\n    \"batch_size\": 1\n  }\n)\n</code></pre> <p>\ucc98\uc74c\uc5d0\ub294 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc9c0\uc815\ud558\uc9c0 \uc54a\uace0 \ud559\uc2b5\ud558\uc5ec \ub370\uc774\ud130\uc14b \ud06c\uae30\uc5d0 \ub530\ub77c \uae30\ubcf8\uac12\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \uadf8 \ud6c4 \ub2e4\uc74c\uacfc \uac19\uc740 \uc0c1\ud669\uc5d0 \ub530\ub77c \uc870\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li>\ubaa8\ub378\uc774 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc608\uc0c1\ub9cc\ud07c \ub530\ub974\uc9c0 \uc54a\ub294 \uacbd\uc6b0: \uc5d0\ud3ec\ud06c \uc218\ub97c 1~2 \uc99d\uac00</li> <li>\ubaa8\ub378\uc774 \uc608\uc0c1\ubcf4\ub2e4 \ub2e4\uc591\uc131\uc774 \ub5a8\uc5b4\uc9c0\ub294 \uacbd\uc6b0: \uc5d0\ud3ec\ud06c \uc218\ub97c 1~2 \uac10\uc18c</li> <li>\ubaa8\ub378\uc774 \uc218\ub834\ud558\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\ub294 \uacbd\uc6b0: \ud559\uc2b5\ub960 \uc2b9\uc218 \uc99d\uac00</li> </ul>"},{"location":"practice_guides/gpt_finetuning/#_10","title":"\uc9c4\ud589 \uc0c1\ud669 \ubaa8\ub2c8\ud130\ub9c1","text":"<p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc744 \uc2dc\uc791\ud55c \ud6c4\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \uc9c4\ud589 \uc0c1\ud669\uc744 \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code># \ud2b9\uc815 \uc791\uc5c5\uc758 \uc138\ubd80 \uc815\ubcf4 \ud655\uc778\njob = client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")\nprint(job)\n\n# \uc791\uc5c5\uc758 \uc774\ubca4\ud2b8 \ud655\uc778\nevents = client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\")\nprint(events)\n\n# \ubaa8\ub4e0 \uc791\uc5c5 \ubaa9\ub85d \uac00\uc838\uc624\uae30\njobs = client.fine_tuning.jobs.list()\nprint(jobs)\n</code></pre> <p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc774 \uc644\ub8cc\ub418\uba74 \uc791\uc5c5\uc744 \uc0dd\uc131\ud55c \uc0ac\uc6a9\uc790\uc5d0\uac8c \uc774\uba54\uc77c \ud655\uc778\uc774 \uc804\uc1a1\ub429\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_11","title":"\ubaa8\ub378 \ud3c9\uac00 \ubc0f \uc0ac\uc6a9","text":""},{"location":"practice_guides/gpt_finetuning/#_12","title":"\ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378 \ud14c\uc2a4\ud2b8","text":"<p>\uc791\uc5c5\uc774 \uc131\uacf5\ud558\uba74 \uc791\uc5c5 \uc138\ubd80 \uc815\ubcf4\ub97c \uac80\uc0c9\ud560 \ub54c fine_tuned_model \ud544\ub4dc\uc5d0 \ubaa8\ub378 \uc774\ub984\uc774 \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc774\uc81c \uc774 \ubaa8\ub378\uc744 Chat Completions API\uc758 \ub9e4\uac1c\ubcc0\uc218\ub85c \uc9c0\uc815\ud558\uace0 Playground\uc5d0\uc11c \uc694\uccad\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>completion = client.chat.completions.create(\n  model=\"ft:gpt-3.5-turbo-0125:my-org:my-suffix:abc123\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n</code></pre> <p>\uc791\uc5c5\uc774 \uc644\ub8cc\ub41c \ud6c4 \ubaa8\ub378\uc740 \ucd94\ub860 \uc0ac\uc6a9\uc744 \uc704\ud574 \uc989\uc2dc \uc0ac\uc6a9 \uac00\ub2a5\ud574\uc57c \ud569\ub2c8\ub2e4. \uc77c\ubd80 \uacbd\uc6b0\uc5d0\ub294 \ubaa8\ub378\uc774 \uc694\uccad\uc744 \ucc98\ub9ac\ud560 \uc900\ube44\uac00 \ub418\uae30\uae4c\uc9c0 \uba87 \ubd84\uc774 \uac78\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_13","title":"\uc131\ub2a5 \ube44\uad50","text":"<p>\ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\ub294 \uac00\uc7a5 \uad00\ub828\uc131 \ub192\uc740 \ubc29\ubc95\uc740 Evals \uc81c\ud488\uc744 \uc0ac\uc6a9\ud558\uc5ec \uae30\ubcf8 \ubaa8\ub378\uacfc \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc744 \ube44\uad50\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub610\ub294 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ub450 \ubaa8\ub378\uc758 \uc0d8\ud50c\uc744 \uc218\ub3d9\uc73c\ub85c \uc0dd\uc131\ud558\uace0 \uc0d8\ud50c\uc744 \ub098\ub780\ud788 \ube44\uad50\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc758 \uacb0\uacfc\uac00 \uc608\uc0c1\ub9cc\ud07c \uc88b\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc744 \uc870\uc815\ud558\ub294 \uac83\uc744 \uace0\ub824\ud574 \ubcf4\uc138\uc694:</p> <ul> <li>\ub0a8\uc544 \uc788\ub294 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \uc608\uc81c \uc218\uc9d1</li> <li>\uae30\uc874 \uc608\uc81c\uc758 \ubb38\uc81c\uc810 \uac80\ud1a0</li> <li>\ub370\uc774\ud130\uc758 \uade0\ud615\uacfc \ub2e4\uc591\uc131 \uace0\ub824</li> <li>\ud559\uc2b5 \uc608\uc81c\uc5d0 \uc751\ub2f5\uc5d0 \ud544\uc694\ud55c \ubaa8\ub4e0 \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778</li> <li>\ud559\uc2b5 \uc608\uc81c\uc758 \uc77c\uad00\uc131 \ud655\uc778</li> <li>\ubaa8\ub4e0 \ud559\uc2b5 \uc608\uc81c\uac00 \ucd94\ub860\uc5d0 \uc608\uc0c1\ub418\ub294 \uac83\uacfc \ub3d9\uc77c\ud55c \ud615\uc2dd\uc778\uc9c0 \ud655\uc778</li> </ul>"},{"location":"practice_guides/gpt_finetuning/#_14","title":"\uc2e4\uc81c \uc11c\ube44\uc2a4 \uc801\uc6a9","text":"<p>\ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc740 \uc77c\ubc18 OpenAI \ubaa8\ub378\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc740 \uae30\ubcf8 \ubaa8\ub378\uacfc \ub3d9\uc77c\ud55c \uacf5\uc720 \uc18d\ub3c4 \uc81c\ud55c\uc5d0\uc11c \uac00\uc838\uc635\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc8fc\uc5b4\uc9c4 \uc2dc\uac04 \ub3d9\uc548 \ud45c\uc900 gpt-4o-mini \ubaa8\ub378\ub85c TPM \uc18d\ub3c4 \uc81c\ud55c\uc758 \uc808\ubc18\uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, gpt-4o-mini\uc5d0\uc11c \ud30c\uc778\ud29c\ub2dd\ud55c \ubaa8\ub378\uc740 \uc6a9\ub7c9\uc774 \ub3d9\uc77c\ud55c \uc720\ud615\uc758 \ubaa8\ub4e0 \ubaa8\ub378\uc5d0\uc11c \uacf5\uc720\ub418\ubbc0\ub85c TPM \uc18d\ub3c4 \uc81c\ud55c\uc758 \ub098\uba38\uc9c0 \uc808\ubc18\ub9cc \uc561\uc138\uc2a4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ub2e4\uc2dc \ub9d0\ud574, \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc774 \uc788\ub2e4\uace0 \ud574\uc11c \ucd1d \ucc98\ub9ac\ub7c9 \uad00\uc810\uc5d0\uc11c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc6a9\ub7c9\uc774 \ub354 \ub9ce\uc544\uc9c0\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4.</p>"},{"location":"practice_guides/gpt_finetuning/#_15","title":"\uace0\uae09 \ud30c\uc778\ud29c\ub2dd \uae30\ubc95","text":""},{"location":"practice_guides/gpt_finetuning/#_16","title":"\ube44\uc804 \ud30c\uc778\ud29c\ub2dd","text":"<p>JSONL \ud30c\uc77c\uc5d0 \uc774\ubbf8\uc9c0\ub97c \ud3ec\ud568\ud558\uc5ec \ud30c\uc778\ud29c\ub2dd\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. Chat Completions\uc5d0 \ud558\ub098 \uc774\uc0c1\uc758 \uc774\ubbf8\uc9c0 \uc785\ub825\uc744 \ubcf4\ub0bc \uc218 \uc788\ub294 \uac83\ucc98\ub7fc, \ud559\uc2b5 \ub370\uc774\ud130 \ub0b4\uc5d0 \ub3d9\uc77c\ud55c \uba54\uc2dc\uc9c0 \uc720\ud615\uc744 \ud3ec\ud568\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\ub294 HTTP URL \ub610\ub294 base64\ub85c \uc778\ucf54\ub529\ub41c \uc774\ubbf8\uc9c0\ub97c \ud3ec\ud568\ud558\ub294 \ub370\uc774\ud130 URL\ub85c \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc14b \uc694\uad6c\uc0ac\ud56d: - \ud06c\uae30: \ucd5c\ub300 50,000\uac1c\uc758 \uc774\ubbf8\uc9c0 \ud3ec\ud568 \uc608\uc81c, \uc608\uc81c\ub2f9 \ucd5c\ub300 10\uac1c \uc774\ubbf8\uc9c0, \uc774\ubbf8\uc9c0\ub2f9 \ucd5c\ub300 10MB - \ud615\uc2dd: JPEG, PNG \ub610\ub294 WEBP \ud615\uc2dd, RGB \ub610\ub294 RGBA \uc774\ubbf8\uc9c0 \ubaa8\ub4dc - \ucf58\ud150\uce20 \uc81c\ud55c: \uc0ac\ub78c, \uc5bc\uad74, \uc5b4\ub9b0\uc774, CAPTCHA\uac00 \ud3ec\ud568\ub41c \uc774\ubbf8\uc9c0\ub294 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\uc678\ub429\ub2c8\ub2e4</p>"},{"location":"practice_guides/gpt_finetuning/#_17","title":"\uc120\ud638\ub3c4 \ud30c\uc778\ud29c\ub2dd","text":"<p>Direct Preference Optimization(DPO) \ud30c\uc778\ud29c\ub2dd\uc744 \uc0ac\uc6a9\ud558\uba74 \ud504\ub86c\ud504\ud2b8\uc640 \uc751\ub2f5 \uc30d\uc744 \uae30\ubc18\uc73c\ub85c \ubaa8\ub378\uc744 \ud30c\uc778\ud29c\ub2dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc811\uadfc \ubc29\uc2dd\uc744 \ud1b5\ud574 \ubaa8\ub378\uc740 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0\uc11c \ud559\uc2b5\ud558\uc5ec \uc120\ud638\ub420 \uac00\ub2a5\uc131\uc774 \ub192\uc740 \ucd9c\ub825\uc744 \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>DPO\ub97c \uc704\ud55c \ub370\uc774\ud130\uc14b \uc900\ube44: - \uac01 \uc608\uc81c\uc5d0\ub294 \ud504\ub86c\ud504\ud2b8(\uc0ac\uc6a9\uc790 \uba54\uc2dc\uc9c0\uc640 \uac19\uc740), \uc120\ud638\ud558\ub294 \ucd9c\ub825(\uc774\uc0c1\uc801\uc778 \uc5b4\uc2dc\uc2a4\ud134\ud2b8 \uc751\ub2f5), \uc120\ud638\ud558\uc9c0 \uc54a\ub294 \ucd9c\ub825(\ucd5c\uc801\uc774 \uc544\ub2cc \uc5b4\uc2dc\uc2a4\ud134\ud2b8 \uc751\ub2f5)\uc774 \ud3ec\ud568\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. - \ub370\uc774\ud130\ub294 JSONL \ud615\uc2dd\uc73c\ub85c \ud3ec\ub9f7\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.</p> <p>DPO \ud30c\uc778\ud29c\ub2dd \uc791\uc5c5 \uad6c\uc131: <pre><code>client.fine_tuning.jobs.create(\n  training_file=\"file-abc123\",\n  model=\"gpt-3.5-turbo-0125\",\n  hyperparameters={\n    \"n_epochs\": 1\n  },\n  method=\"dpo\",\n  beta=0.1\n)\n</code></pre></p> <p>beta \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \uc0c8 \ubaa8\ub378\uc774 \uc774\uc804 \ub3d9\uc791\uc744 \uc5bc\ub9c8\ub098 \uc5c4\uaca9\ud558\uac8c \uace0\uc218\ud560\uc9c0 \ub300 \uc81c\uacf5\ub41c \uc120\ud638\ub3c4\uc5d0 \ub9de\ucd9c\uc9c0\ub97c \uc81c\uc5b4\ud569\ub2c8\ub2e4. \ub192\uc740 \uc22b\uc790\ub294 \ub354 \ubcf4\uc218\uc801\uc774\uace0(\uc774\uc804 \ub3d9\uc791 \uc120\ud638), \ub0ae\uc740 \uc22b\uc790\ub294 \ub354 \uacf5\uaca9\uc801\uc785\ub2c8\ub2e4(\uc0c8\ub85c \uc81c\uacf5\ub41c \uc120\ud638\ub3c4\ub97c \ub354 \uc790\uc8fc \uc120\ud638).</p>"},{"location":"practice_guides/gpt_finetuning/#_18","title":"\ucc38\uace0 \uc790\ub8cc","text":"<ul> <li>OpenAI Fine-tuning \ubb38\uc11c</li> <li>\ube44\uc804 \ud30c\uc778\ud29c\ub2dd \uc608\uc81c</li> </ul>"},{"location":"practice_guides/open_weight_finetuning/","title":"\uc624\ud508 \uc6e8\uc774\ud2b8 \ubaa8\ub378 \ud30c\uc778\ud29c\ub2dd \uc2e4\uc2b5","text":""},{"location":"practice_guides/open_weight_finetuning/#_2","title":"\ud658\uacbd \uc124\uc815","text":"<ul> <li>\ud544\uc694 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58</li> <li>GPU \ud658\uacbd \uad6c\uc131</li> <li>\ubaa8\ub378 \ub2e4\uc6b4\ub85c\ub4dc</li> </ul>"},{"location":"practice_guides/open_weight_finetuning/#huggingface-trl","title":"HuggingFace TRL \ud65c\uc6a9","text":"<ul> <li>TRL \ub77c\uc774\ube0c\ub7ec\ub9ac \uc18c\uac1c</li> <li>SFTTrainer \uc124\uc815</li> <li>\ud559\uc2b5 \ud30c\uc774\ud504\ub77c\uc778 \uad6c\uc131</li> </ul>"},{"location":"practice_guides/open_weight_finetuning/#loraqlora","title":"LoRA/QLoRA \uc801\uc6a9","text":"<ul> <li>PEFT \uc124\uc815</li> <li>\uc591\uc790\ud654 \uad6c\uc131</li> <li>\ud559\uc2b5 \uc2e4\ud589</li> </ul>"},{"location":"practice_guides/open_weight_finetuning/#_3","title":"\ubaa8\ub378 \uc800\uc7a5 \ubc0f \ubc30\ud3ec","text":"<ul> <li>\uccb4\ud06c\ud3ec\uc778\ud2b8 \uad00\ub9ac</li> <li>\ubaa8\ub378 \ubcd1\ud569</li> <li>\ud5c8\uae45\ud398\uc774\uc2a4 \ud5c8\ube0c \uc5c5\ub85c\ub4dc </li> </ul> <p>https://www.deeplearning.ai/courses/?courses_date_desc%5Bquery%5D=upstage</p>"},{"location":"practice_guides/reasoning_model/","title":"SFT\ub85c Reasoning \ubaa8\ub378 \ub9cc\ub4e4\uae30 \uc2e4\uc2b5","text":""},{"location":"practice_guides/reasoning_model/#_1","title":"\ucd94\ub860 \ub370\uc774\ud130 \uc900\ube44","text":"<ul> <li>CoT(Chain-of-Thought) \ub370\uc774\ud130 \uad6c\uc870</li> <li>\ub2e4\uc591\ud55c \ucd94\ub860 \ubb38\uc81c \uc218\uc9d1</li> <li>\ub2e8\uacc4\ubcc4 \ud574\ubc95 \uc791\uc131</li> </ul>"},{"location":"practice_guides/reasoning_model/#_2","title":"\ubaa8\ub378 \uc120\ud0dd \ubc0f \uc124\uc815","text":"<ul> <li>\ucd94\ub860\uc5d0 \uc801\ud569\ud55c \ubca0\uc774\uc2a4 \ubaa8\ub378</li> <li>\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815</li> <li>\ud559\uc2b5 \uad6c\uc131</li> </ul>"},{"location":"practice_guides/reasoning_model/#_3","title":"\ud6c8\ub828 \ubc0f \ucd5c\uc801\ud654","text":"<ul> <li>\ud559\uc2b5 \uc9c4\ud589</li> <li>\uacfc\uc801\ud569 \ubaa8\ub2c8\ud130\ub9c1</li> <li>\ubaa8\ub378 \uac1c\uc120 \uc0ac\uc774\ud074</li> </ul>"},{"location":"practice_guides/reasoning_model/#_4","title":"\ud3c9\uac00 \ubc0f \ud14c\uc2a4\ud2b8","text":"<ul> <li>\ucd94\ub860 \ub2a5\ub825 \ud3c9\uac00 \ubc29\ubc95</li> <li>\ubcf5\uc7a1\ud55c \ubb38\uc81c \ud574\uacb0 \ud14c\uc2a4\ud2b8</li> <li>\uc624\ub958 \ubd84\uc11d \ubc0f \uac1c\uc120 </li> </ul>"},{"location":"practice_ipynb/1_openai_finetuning/","title":"GPT (Chat Model) \ud30c\uc778\ud29c\ub2dd \ud558\uae30!","text":"In\u00a0[6]: Copied! <pre># make sure to use the latest version of the openai python package\n!pip install --upgrade --quiet openai\n</pre> # make sure to use the latest version of the openai python package !pip install --upgrade --quiet openai In\u00a0[69]: Copied! <pre>import json\nimport openai\nimport os\nimport pandas as pd\nfrom pprint import pprint\n\nclient = openai.OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    organization=\"&lt;org id&gt;\",\n    project=\"&lt;project id&gt;\",\n)\n</pre> import json import openai import os import pandas as pd from pprint import pprint  client = openai.OpenAI(     api_key=os.environ.get(\"OPENAI_API_KEY\"),     organization=\"\",     project=\"\", ) <p>Fine-tuning works best when focused on a particular domain. It's important to make sure your dataset is both focused enough for the model to learn, but general enough that unseen examples won't be missed. Having this in mind, we have extracted a subset from the RecipesNLG dataset to only contain documents from cookbooks.com.</p> In\u00a0[70]: Copied! <pre># Read in the dataset we'll use for this task.\n# This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com\nrecipe_df = pd.read_csv(\"data/cookbook_recipes_nlg_10k.csv\")\n\nrecipe_df.head()\n</pre> # Read in the dataset we'll use for this task. # This will be the RecipesNLG dataset, which we've cleaned to only contain documents from www.cookbooks.com recipe_df = pd.read_csv(\"data/cookbook_recipes_nlg_10k.csv\")  recipe_df.head() Out[70]: title ingredients directions link source NER 0 No-Bake Nut Cookies [\"1 c. firmly packed brown sugar\", \"1/2 c. eva... [\"In a heavy 2-quart saucepan, mix brown sugar... www.cookbooks.com/Recipe-Details.aspx?id=44874 www.cookbooks.com [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu... 1 Jewell Ball'S Chicken [\"1 small jar chipped beef, cut up\", \"4 boned ... [\"Place chipped beef on bottom of baking dish.... www.cookbooks.com/Recipe-Details.aspx?id=699419 www.cookbooks.com [\"beef\", \"chicken breasts\", \"cream of mushroom... 2 Creamy Corn [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg... [\"In a slow cooker, combine all ingredients. C... www.cookbooks.com/Recipe-Details.aspx?id=10570 www.cookbooks.com [\"frozen corn\", \"cream cheese\", \"butter\", \"gar... 3 Chicken Funny [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans... [\"Boil and debone chicken.\", \"Put bite size pi... www.cookbooks.com/Recipe-Details.aspx?id=897570 www.cookbooks.com [\"chicken\", \"chicken gravy\", \"cream of mushroo... 4 Reeses Cups(Candy) [\"1 c. peanut butter\", \"3/4 c. graham cracker ... [\"Combine first four ingredients and press in ... www.cookbooks.com/Recipe-Details.aspx?id=659239 www.cookbooks.com [\"peanut butter\", \"graham cracker crumbs\", \"bu... In\u00a0[71]: Copied! <pre>system_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"\n\n\ndef create_user_message(row):\n    return f\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"\n\n\ndef prepare_example_conversation(row):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": create_user_message(row)},\n            {\"role\": \"assistant\", \"content\": row[\"NER\"]},\n        ]\n    }\n\n\npprint(prepare_example_conversation(recipe_df.iloc[0]))\n</pre> system_message = \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"   def create_user_message(row):     return f\"Title: {row['title']}\\n\\nIngredients: {row['ingredients']}\\n\\nGeneric ingredients: \"   def prepare_example_conversation(row):     return {         \"messages\": [             {\"role\": \"system\", \"content\": system_message},             {\"role\": \"user\", \"content\": create_user_message(row)},             {\"role\": \"assistant\", \"content\": row[\"NER\"]},         ]     }   pprint(prepare_example_conversation(recipe_df.iloc[0])) <pre>{'messages': [{'content': 'You are a helpful recipe assistant. You are to '\n                          'extract the generic ingredients from each of the '\n                          'recipes provided.',\n               'role': 'system'},\n              {'content': 'Title: No-Bake Nut Cookies\\n'\n                          '\\n'\n                          'Ingredients: [\"1 c. firmly packed brown sugar\", '\n                          '\"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 '\n                          'c. broken nuts (pecans)\", \"2 Tbsp. butter or '\n                          'margarine\", \"3 1/2 c. bite size shredded rice '\n                          'biscuits\"]\\n'\n                          '\\n'\n                          'Generic ingredients: ',\n               'role': 'user'},\n              {'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", '\n                          '\"butter\", \"bite size shredded rice biscuits\"]',\n               'role': 'assistant'}]}\n</pre> <p>Let's now do this for a subset of the dataset to use as our training data. You can begin with even 30-50 well-pruned examples. You should see performance continue to scale linearly as you increase the size of the training set, but your jobs will also take longer.</p> In\u00a0[72]: Copied! <pre># use the first 100 rows of the dataset for training\ntraining_df = recipe_df.loc[0:100]\n\n# apply the prepare_example_conversation function to each row of the training_df\ntraining_data = training_df.apply(prepare_example_conversation, axis=1).tolist()\n\nfor example in training_data[:5]:\n    print(example)\n</pre> # use the first 100 rows of the dataset for training training_df = recipe_df.loc[0:100]  # apply the prepare_example_conversation function to each row of the training_df training_data = training_df.apply(prepare_example_conversation, axis=1).tolist()  for example in training_data[:5]:     print(example) <pre>{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}]}\n{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Jewell Ball\\'S Chicken\\n\\nIngredients: [\"1 small jar chipped beef, cut up\", \"4 boned chicken breasts\", \"1 can cream of mushroom soup\", \"1 carton sour cream\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"beef\", \"chicken breasts\", \"cream of mushroom soup\", \"sour cream\"]'}]}\n{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Creamy Corn\\n\\nIngredients: [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"frozen corn\", \"cream cheese\", \"butter\", \"garlic powder\", \"salt\", \"pepper\"]'}]}\n{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Chicken Funny\\n\\nIngredients: [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans chicken gravy\", \"1 (10 1/2 oz.) can cream of mushroom soup\", \"1 (6 oz.) box Stove Top stuffing\", \"4 oz. shredded cheese\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"chicken\", \"chicken gravy\", \"cream of mushroom soup\", \"shredded cheese\"]'}]}\n{'messages': [{'role': 'system', 'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'}, {'role': 'user', 'content': 'Title: Reeses Cups(Candy)  \\n\\nIngredients: [\"1 c. peanut butter\", \"3/4 c. graham cracker crumbs\", \"1 c. melted butter\", \"1 lb. (3 1/2 c.) powdered sugar\", \"1 large pkg. chocolate chips\"]\\n\\nGeneric ingredients: '}, {'role': 'assistant', 'content': '[\"peanut butter\", \"graham cracker crumbs\", \"butter\", \"powdered sugar\", \"chocolate chips\"]'}]}\n</pre> <p>In addition to training data, we can also optionally provide validation data, which will be used to make sure that the model does not overfit your training set.</p> In\u00a0[73]: Copied! <pre>validation_df = recipe_df.loc[101:200]\nvalidation_data = validation_df.apply(\n    prepare_example_conversation, axis=1).tolist()\n</pre> validation_df = recipe_df.loc[101:200] validation_data = validation_df.apply(     prepare_example_conversation, axis=1).tolist() <p>We then need to save our data as <code>.jsonl</code> files, with each line being one training example conversation.</p> In\u00a0[74]: Copied! <pre>def write_jsonl(data_list: list, filename: str) -&gt; None:\n    with open(filename, \"w\") as out:\n        for ddict in data_list:\n            jout = json.dumps(ddict) + \"\\n\"\n            out.write(jout)\n</pre> def write_jsonl(data_list: list, filename: str) -&gt; None:     with open(filename, \"w\") as out:         for ddict in data_list:             jout = json.dumps(ddict) + \"\\n\"             out.write(jout) In\u00a0[75]: Copied! <pre>training_file_name = \"tmp_recipe_finetune_training.jsonl\"\nwrite_jsonl(training_data, training_file_name)\n\nvalidation_file_name = \"tmp_recipe_finetune_validation.jsonl\"\nwrite_jsonl(validation_data, validation_file_name)\n</pre> training_file_name = \"tmp_recipe_finetune_training.jsonl\" write_jsonl(training_data, training_file_name)  validation_file_name = \"tmp_recipe_finetune_validation.jsonl\" write_jsonl(validation_data, validation_file_name) <p>This is what the first 5 lines of our training <code>.jsonl</code> file look like:</p> In\u00a0[76]: Copied! <pre># print the first 5 lines of the training file\n!head -n 5 tmp_recipe_finetune_training.jsonl\n</pre> # print the first 5 lines of the training file !head -n 5 tmp_recipe_finetune_training.jsonl <pre>{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: No-Bake Nut Cookies\\n\\nIngredients: [\\\"1 c. firmly packed brown sugar\\\", \\\"1/2 c. evaporated milk\\\", \\\"1/2 tsp. vanilla\\\", \\\"1/2 c. broken nuts (pecans)\\\", \\\"2 Tbsp. butter or margarine\\\", \\\"3 1/2 c. bite size shredded rice biscuits\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"brown sugar\\\", \\\"milk\\\", \\\"vanilla\\\", \\\"nuts\\\", \\\"butter\\\", \\\"bite size shredded rice biscuits\\\"]\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Jewell Ball'S Chicken\\n\\nIngredients: [\\\"1 small jar chipped beef, cut up\\\", \\\"4 boned chicken breasts\\\", \\\"1 can cream of mushroom soup\\\", \\\"1 carton sour cream\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"beef\\\", \\\"chicken breasts\\\", \\\"cream of mushroom soup\\\", \\\"sour cream\\\"]\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Creamy Corn\\n\\nIngredients: [\\\"2 (16 oz.) pkg. frozen corn\\\", \\\"1 (8 oz.) pkg. cream cheese, cubed\\\", \\\"1/3 c. butter, cubed\\\", \\\"1/2 tsp. garlic powder\\\", \\\"1/2 tsp. salt\\\", \\\"1/4 tsp. pepper\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"frozen corn\\\", \\\"cream cheese\\\", \\\"butter\\\", \\\"garlic powder\\\", \\\"salt\\\", \\\"pepper\\\"]\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Chicken Funny\\n\\nIngredients: [\\\"1 large whole chicken\\\", \\\"2 (10 1/2 oz.) cans chicken gravy\\\", \\\"1 (10 1/2 oz.) can cream of mushroom soup\\\", \\\"1 (6 oz.) box Stove Top stuffing\\\", \\\"4 oz. shredded cheese\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"chicken\\\", \\\"chicken gravy\\\", \\\"cream of mushroom soup\\\", \\\"shredded cheese\\\"]\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.\"}, {\"role\": \"user\", \"content\": \"Title: Reeses Cups(Candy)  \\n\\nIngredients: [\\\"1 c. peanut butter\\\", \\\"3/4 c. graham cracker crumbs\\\", \\\"1 c. melted butter\\\", \\\"1 lb. (3 1/2 c.) powdered sugar\\\", \\\"1 large pkg. chocolate chips\\\"]\\n\\nGeneric ingredients: \"}, {\"role\": \"assistant\", \"content\": \"[\\\"peanut butter\\\", \\\"graham cracker crumbs\\\", \\\"butter\\\", \\\"powdered sugar\\\", \\\"chocolate chips\\\"]\"}]}\n</pre> In\u00a0[77]: Copied! <pre>def upload_file(file_name: str, purpose: str) -&gt; str:\n    with open(file_name, \"rb\") as file_fd:\n        response = client.files.create(file=file_fd, purpose=purpose)\n    return response.id\n\n\ntraining_file_id = upload_file(training_file_name, \"fine-tune\")\nvalidation_file_id = upload_file(validation_file_name, \"fine-tune\")\n\nprint(\"Training file ID:\", training_file_id)\nprint(\"Validation file ID:\", validation_file_id)\n</pre> def upload_file(file_name: str, purpose: str) -&gt; str:     with open(file_name, \"rb\") as file_fd:         response = client.files.create(file=file_fd, purpose=purpose)     return response.id   training_file_id = upload_file(training_file_name, \"fine-tune\") validation_file_id = upload_file(validation_file_name, \"fine-tune\")  print(\"Training file ID:\", training_file_id) print(\"Validation file ID:\", validation_file_id) <pre>Training file ID: file-3wfAfDoYcGrSpaE17qK0vXT0\nValidation file ID: file-HhFhnyGJhazYdPcd3wrtvIoX\n</pre> In\u00a0[81]: Copied! <pre>MODEL = \"gpt-4o-mini-2024-07-18\"\n\nresponse = client.fine_tuning.jobs.create(\n    training_file=training_file_id,\n    validation_file=validation_file_id,\n    model=MODEL,\n    suffix=\"recipe-ner\",\n)\n\njob_id = response.id\n\nprint(\"Job ID:\", response.id)\nprint(\"Status:\", response.status)\n</pre> MODEL = \"gpt-4o-mini-2024-07-18\"  response = client.fine_tuning.jobs.create(     training_file=training_file_id,     validation_file=validation_file_id,     model=MODEL,     suffix=\"recipe-ner\", )  job_id = response.id  print(\"Job ID:\", response.id) print(\"Status:\", response.status) <pre>Job ID: ftjob-UiaiLwGdGBfdLQDBAoQheufN\nStatus: validating_files\n</pre> In\u00a0[91]: Copied! <pre>response = client.fine_tuning.jobs.retrieve(job_id)\n\nprint(\"Job ID:\", response.id)\nprint(\"Status:\", response.status)\nprint(\"Trained Tokens:\", response.trained_tokens)\n</pre> response = client.fine_tuning.jobs.retrieve(job_id)  print(\"Job ID:\", response.id) print(\"Status:\", response.status) print(\"Trained Tokens:\", response.trained_tokens) <pre>Job ID: ftjob-UiaiLwGdGBfdLQDBAoQheufN\nStatus: running\nTrained Tokens: None\n</pre> <p>We can track the progress of the fine-tune with the events endpoint. You can rerun the cell below a few times until the fine-tune is ready.</p> In\u00a0[94]: Copied! <pre>response = client.fine_tuning.jobs.list_events(job_id)\n\nevents = response.data\nevents.reverse()\n\nfor event in events:\n    print(event.message)\n</pre> response = client.fine_tuning.jobs.list_events(job_id)  events = response.data events.reverse()  for event in events:     print(event.message) <pre>Step 288/303: training loss=0.00\nStep 289/303: training loss=0.01\nStep 290/303: training loss=0.00, validation loss=0.31\nStep 291/303: training loss=0.00\nStep 292/303: training loss=0.00\nStep 293/303: training loss=0.00\nStep 294/303: training loss=0.00\nStep 295/303: training loss=0.00\nStep 296/303: training loss=0.00\nStep 297/303: training loss=0.00\nStep 298/303: training loss=0.01\nStep 299/303: training loss=0.00\nStep 300/303: training loss=0.00, validation loss=0.04\nStep 301/303: training loss=0.16\nStep 302/303: training loss=0.00\nStep 303/303: training loss=0.00, full validation loss=0.33\nCheckpoint created at step 101 with Snapshot ID: ft:gpt-4o-mini-2024-07-18:openai-gtm:recipe-ner:9o1eNlSa:ckpt-step-101\nCheckpoint created at step 202 with Snapshot ID: ft:gpt-4o-mini-2024-07-18:openai-gtm:recipe-ner:9o1eNFnj:ckpt-step-202\nNew fine-tuned model created: ft:gpt-4o-mini-2024-07-18:openai-gtm:recipe-ner:9o1eNNKO\nThe job has successfully completed\n</pre> <p>Now that it's done, we can get a fine-tuned model ID from the job:</p> In\u00a0[95]: Copied! <pre>response = client.fine_tuning.jobs.retrieve(job_id)\nfine_tuned_model_id = response.fine_tuned_model\n\nif fine_tuned_model_id is None:\n    raise RuntimeError(\n        \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n    )\n\nprint(\"Fine-tuned model ID:\", fine_tuned_model_id)\n</pre> response = client.fine_tuning.jobs.retrieve(job_id) fine_tuned_model_id = response.fine_tuned_model  if fine_tuned_model_id is None:     raise RuntimeError(         \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"     )  print(\"Fine-tuned model ID:\", fine_tuned_model_id) <pre>Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:openai-gtm:recipe-ner:9o1eNNKO\n</pre> <p>The last step is to use your fine-tuned model for inference. Similar to the classic <code>FineTuning</code>, you simply call <code>ChatCompletions</code> with your new fine-tuned model name filling the <code>model</code> parameter.</p> In\u00a0[96]: Copied! <pre>test_df = recipe_df.loc[201:300]\ntest_row = test_df.iloc[0]\ntest_messages = []\ntest_messages.append({\"role\": \"system\", \"content\": system_message})\nuser_message = create_user_message(test_row)\ntest_messages.append({\"role\": \"user\", \"content\": user_message})\n\npprint(test_messages)\n</pre> test_df = recipe_df.loc[201:300] test_row = test_df.iloc[0] test_messages = [] test_messages.append({\"role\": \"system\", \"content\": system_message}) user_message = create_user_message(test_row) test_messages.append({\"role\": \"user\", \"content\": user_message})  pprint(test_messages) <pre>[{'content': 'You are a helpful recipe assistant. You are to extract the '\n             'generic ingredients from each of the recipes provided.',\n  'role': 'system'},\n {'content': 'Title: Beef Brisket\\n'\n             '\\n'\n             'Ingredients: [\"4 lb. beef brisket\", \"1 c. catsup\", \"1 c. water\", '\n             '\"1/2 onion, minced\", \"2 Tbsp. cider vinegar\", \"1 Tbsp. prepared '\n             'horseradish\", \"1 Tbsp. prepared mustard\", \"1 tsp. salt\", \"1/2 '\n             'tsp. pepper\"]\\n'\n             '\\n'\n             'Generic ingredients: ',\n  'role': 'user'}]\n</pre> In\u00a0[97]: Copied! <pre>response = client.chat.completions.create(\n    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n)\nprint(response.choices[0].message.content)\n</pre> response = client.chat.completions.create(     model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500 ) print(response.choices[0].message.content) <pre>[\"beef brisket\", \"catsup\", \"water\", \"onion\", \"cider vinegar\", \"horseradish\", \"mustard\", \"salt\", \"pepper\"]\n</pre>"},{"location":"practice_ipynb/1_openai_finetuning/#gpt-chat-model","title":"GPT (Chat Model) \ud30c\uc778\ud29c\ub2dd \ud558\uae30!\u00b6","text":"<p>\ud30c\uc778\ud29c\ub2dd\uc740 \ud504\ub86c\ud504\ud2b8\uc5d0 \ub2f4\uc744 \uc218 \uc788\ub294 \uac83\ubcf4\ub2e4 \ud6e8\uc52c \ub354 \ub9ce\uc740 \uc608\uc81c\ub97c \ud559\uc2b5\uc2dc\ucf1c \ubaa8\ub378\uc744 \uac1c\uc120\ud558\uace0, \ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0\uc11c \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4. \uc774 \ub178\ud2b8\ubd81\uc740 \uc0c8\ub85c\uc6b4 GPT-4o \ubbf8\ub2c8 \ud30c\uc778\ud29c\ub2dd\uc744 \uc704\ud55c \ub2e8\uacc4\ubcc4 \uac00\uc774\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 RecipeNLG \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5d4\ud2f0\ud2f0 \ucd94\ucd9c\uc744 \uc218\ud589\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b\uc740 \ub2e4\uc591\ud55c \ub808\uc2dc\ud53c\uc640 \uac01 \ub808\uc2dc\ud53c\uc5d0 \ub300\ud55c \uc77c\ubc18\uc801\uc778 \uc7ac\ub8cc \ubaa9\ub85d\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 \uba85\uba85\ub41c \uc5d4\ud2f0\ud2f0 \uc778\uc2dd(NER) \uc791\uc5c5\uc5d0 \ud754\ud788 \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc14b\uc785\ub2c8\ub2e4.</p> <p>\ucc38\uace0: GPT-4o \ubbf8\ub2c8 \ud30c\uc778\ud29c\ub2dd\uc740 Tier 4 \ubc0f 5 \uc0ac\uc6a9 \ub4f1\uae09\uc5d0\uc11c \uac1c\ubc1c\uc790\uc5d0\uac8c \uc81c\uacf5\ub429\ub2c8\ub2e4. \ud30c\uc778\ud29c\ub2dd \ub300\uc2dc\ubcf4\ub4dc\ub97c \ubc29\ubb38\ud558\uc5ec \"\uc0dd\uc131\"\uc744 \ud074\ub9ad\ud558\uace0 \uae30\ubcf8 \ubaa8\ub378 \ub4dc\ub86d\ub2e4\uc6b4\uc5d0\uc11c \"gpt-4o-mini-2024-07-18\"\uc744 \uc120\ud0dd\ud558\uc5ec GPT-4o \ubbf8\ub2c8 \ud30c\uc778\ud29c\ub2dd\uc744 \uc2dc\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc6b0\ub9ac\ub294 \ub2e4\uc74c \ub2e8\uacc4\ub97c \uac70\uce60 \uac83\uc785\ub2c8\ub2e4:</p> <ol> <li>\uc124\uc815: \ub370\uc774\ud130\uc14b\uc744 \ub85c\ub4dc\ud558\uace0 \ud30c\uc778\ud29c\ub2dd\ud560 \ub3c4\uba54\uc778\uc73c\ub85c \ud544\ud130\ub9c1\ud569\ub2c8\ub2e4.</li> <li>\ub370\uc774\ud130 \uc900\ube44: \ud559\uc2b5 \ubc0f \uac80\uc99d \uc608\uc81c\ub97c \uc0dd\uc131\ud558\uace0 \uc774\ub97c <code>Files</code> \uc5d4\ub4dc\ud3ec\uc778\ud2b8\uc5d0 \uc5c5\ub85c\ub4dc\ud558\uc5ec \ud30c\uc778\ud29c\ub2dd\uc744 \uc704\ud55c \ub370\uc774\ud130\ub97c \uc900\ube44\ud569\ub2c8\ub2e4.</li> <li>\ud30c\uc778\ud29c\ub2dd: \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.</li> <li>\ucd94\ub860: \uc0c8\ub85c\uc6b4 \uc785\ub825\uc5d0 \ub300\ud574 \ud30c\uc778\ud29c\ub2dd\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd94\ub860\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.</li> </ol> <p>\uc774 \uacfc\uc815\uc744 \ub9c8\uce58\uba74 <code>gpt-4o-mini-2024-07-18</code> \ubaa8\ub378\uc744 \ud559\uc2b5, \ud3c9\uac00 \ubc0f \ubc30\ud3ec\ud560 \uc218 \uc788\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4.</p> <p>\ud30c\uc778\ud29c\ub2dd\uc5d0 \ub300\ud55c \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294 \ubb38\uc11c \uac00\uc774\ub4dc \ub610\ub294 API \ucc38\uc870\ub97c \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.</p> <p>\ubcf8 \uc2e4\uc2b5\uc740 OpenAI \uc5d0\uc11c \ubc1c\ud589\ud55c Cookbook \uc785\ub2c8\ub2e4.</p>"},{"location":"practice_ipynb/1_openai_finetuning/#setup","title":"Setup\u00b6","text":""},{"location":"practice_ipynb/1_openai_finetuning/#data-preparation","title":"Data preparation\u00b6","text":"<p>We'll begin by preparing our data. When fine-tuning with the <code>ChatCompletion</code> format, each training example is a simple list of <code>messages</code>. For example, an entry could look like:</p> <pre><code>[{'role': 'system',\n  'content': 'You are a helpful recipe assistant. You are to extract the generic ingredients from each of the recipes provided.'},\n\n {'role': 'user',\n  'content': 'Title: No-Bake Nut Cookies\\n\\nIngredients: [\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]\\n\\nGeneric ingredients: '},\n\n {'role': 'assistant',\n  'content': '[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]'}]\n</code></pre> <p>During the training process this conversation will be split, with the final entry being the <code>completion</code> that the model will produce, and the remainder of the <code>messages</code> acting as the prompt. Consider this when building your training examples - if your model will act on multi-turn conversations, then please provide representative examples so it doesn't perform poorly when the conversation starts to expand.</p> <p>Please note that currently there is a 4096 token limit for each training example. Anything longer than this will be truncated at 4096 tokens.</p>"},{"location":"practice_ipynb/1_openai_finetuning/#upload-files","title":"Upload files\u00b6","text":"<p>You can now upload the files to our <code>Files</code> endpoint to be used by the fine-tuned model.</p>"},{"location":"practice_ipynb/1_openai_finetuning/#fine-tuning","title":"Fine-tuning\u00b6","text":"<p>Now we can create our fine-tuning job with the generated files and an optional suffix to identify the model. The response will contain an <code>id</code> which you can use to retrieve updates on the job.</p> <p>Note: The files have to first be processed by our system, so you might get a <code>File not ready</code> error. In that case, simply retry a few minutes later.</p>"},{"location":"practice_ipynb/1_openai_finetuning/#check-job-status","title":"Check job status\u00b6","text":"<p>You can make a <code>GET</code> request to the <code>https://api.openai.com/v1/alpha/fine-tunes</code> endpoint to list your alpha fine-tune jobs. In this instance you'll want to check that the ID you got from the previous step ends up as <code>status: succeeded</code>.</p> <p>Once it is completed, you can use the <code>result_files</code> to sample the results from the validation set (if you uploaded one), and use the ID from the <code>fine_tuned_model</code> parameter to invoke your trained model.</p>"},{"location":"practice_ipynb/1_openai_finetuning/#inference","title":"Inference\u00b6","text":""},{"location":"practice_ipynb/1_openai_finetuning/#conclusion","title":"Conclusion\u00b6","text":"<p>Congratulations, you are now ready to fine-tune your own models using the <code>ChatCompletion</code> format! We look forward to seeing what you build</p>"},{"location":"trends_projects/competition_guide/","title":"\uc131\ub2a5 \uacbd\uc7c1 \ubaa8\ub378 \uac1c\ubc1c \uac00\uc774\ub4dc","text":""},{"location":"trends_projects/competition_guide/#_2","title":"\ub300\ud68c \uc18c\uac1c","text":"<ul> <li>\ubaa9\ud45c \ubc0f \ud3c9\uac00 \uae30\uc900</li> <li>\uc81c\ud55c \uc0ac\ud56d \ubc0f \uaddc\uce59</li> <li>\uc81c\ucd9c \ubc29\ubc95</li> </ul>"},{"location":"trends_projects/competition_guide/#_3","title":"\uc131\ub2a5 \ucd5c\uc801\ud654 \uc804\ub7b5","text":"<ul> <li>\ub370\uc774\ud130 \uc804\ub7b5 \uc218\ub9bd</li> <li>\ubaa8\ub378 \uc120\ud0dd \uac00\uc774\ub4dc</li> <li>\uc559\uc0c1\ube14 \ubc0f \uc99d\ub958 \ud65c\uc6a9</li> </ul>"},{"location":"trends_projects/competition_guide/#_4","title":"\uac1c\ubc1c \ud504\ub85c\uc138\uc2a4","text":"<ul> <li>\uc2e4\ud5d8 \uc124\uacc4</li> <li>\ud3c9\uac00 \ubc0f \ud53c\ub4dc\ubc31 \ub8e8\ud504</li> <li>\ud300 \ud611\uc5c5 \ubc29\ubc95</li> </ul>"},{"location":"trends_projects/competition_guide/#_5","title":"\uc0ac\ub840 \ubc0f \ucc38\uace0\uc790\ub8cc","text":"<ul> <li>\uc774\uc804 \ub300\ud68c \uc131\uacf5 \uc0ac\ub840</li> <li>\ucc38\uace0\ud560 \ub9cc\ud55c \uc5f0\uad6c \ub17c\ubb38</li> <li>\uc720\uc6a9\ud55c \ucf54\ub4dc \uc800\uc7a5\uc18c </li> </ul>"},{"location":"trends_projects/domain_specific/","title":"\ub3c4\uba54\uc778 \ud2b9\ud654 \ubaa8\ub378 \uac1c\ubc1c \uac00\uc774\ub4dc","text":""},{"location":"trends_projects/domain_specific/#_2","title":"\ub3c4\uba54\uc778 \ubd84\uc11d","text":"<ul> <li>\ub300\uc0c1 \ub3c4\uba54\uc778 \uc694\uad6c\uc0ac\ud56d \ud30c\uc545</li> <li>\ubca4\uce58\ub9c8\ud0b9 \ubc0f \uae30\uc900 \uc124\uc815</li> <li>\ud575\uc2ec \ud0dc\uc2a4\ud06c \uc815\uc758</li> </ul>"},{"location":"trends_projects/domain_specific/#_3","title":"\uac1c\ubc1c \ub85c\ub4dc\ub9f5","text":"<ul> <li>\ub370\uc774\ud130 \uc218\uc9d1 \uacc4\ud68d</li> <li>\ubaa8\ub378 \uc120\ud0dd \ubc0f \uc804\ub7b5</li> <li>\ud29c\ub2dd \ud30c\uc774\ud504\ub77c\uc778 \uc124\uacc4</li> </ul>"},{"location":"trends_projects/domain_specific/#_4","title":"\ub3c4\uba54\uc778\ubcc4 \ud2b9\ud654 \uc804\ub7b5","text":"<ul> <li>\uc758\ub8cc: \uc548\uc804\uc131 \ubc0f \uc815\ud655\uc131 \ucd5c\uc6b0\uc120</li> <li>\ubc95\ub960: \uc778\uc6a9 \ubc0f \ub17c\ub9ac\uc801 \uc77c\uad00\uc131</li> <li>\ucf54\ub529: \uc2e4\ud589 \uac00\ub2a5\ud55c \ucf54\ub4dc \uc0dd\uc131</li> <li>\ucc3d\uc791: \uc2a4\ud0c0\uc77c \ubc0f \ub3c5\ucc3d\uc131</li> </ul>"},{"location":"trends_projects/domain_specific/#_5","title":"\ud3c9\uac00 \ubc0f \ubc30\ud3ec","text":"<ul> <li>\ub3c4\uba54\uc778 \uc804\ubb38\uac00 \ud3c9\uac00</li> <li>\uc9c0\uc18d\uc801 \uac1c\uc120 \uccb4\uacc4</li> <li>\uc0ac\uc6a9\uc790 \ud53c\ub4dc\ubc31 \uc218\uc9d1 </li> </ul>"},{"location":"trends_projects/latest_research/","title":"\ucd5c\uc2e0 \uc5f0\uad6c \ub3d9\ud5a5","text":""},{"location":"trends_projects/latest_research/#self-alignment-ai-feedback","title":"Self-Alignment / AI Feedback","text":"<ul> <li>\uc778\uac04 \ud53c\ub4dc\ubc31 \uc758\uc874 \uac10\uc18c</li> <li>AI \uc790\uccb4 \ud3c9\uac00 \uba54\ucee4\ub2c8\uc998</li> <li>Constitutional AI \uc811\uadfc\ubc95</li> </ul>"},{"location":"trends_projects/latest_research/#tools-plugin","title":"Tools \ubc0f Plugin \ud1b5\ud569","text":"<ul> <li>\uc678\ubd80 \ub3c4\uad6c \ud65c\uc6a9 \ud559\uc2b5</li> <li>API \ud638\ucd9c \ub2a5\ub825 \ud6c8\ub828</li> <li>\ubaa8\ub378 \ud55c\uacc4 \uadf9\ubcf5 \ubc29\ubc95</li> </ul>"},{"location":"trends_projects/latest_research/#_2","title":"\uc7a5\ubb38 \ucee8\ud14d\uc2a4\ud2b8 \ubc0f \uba54\ubaa8\ub9ac","text":"<ul> <li>\uae34 \ubb38\ub9e5 \ucc98\ub9ac \uae30\uc220</li> <li>\uc704\uce58 \uc778\ucf54\ub529 \uac1c\uc120</li> <li>\uc9c0\uc18d \uba54\ubaa8\ub9ac \uba54\ucee4\ub2c8\uc998</li> </ul>"},{"location":"trends_projects/latest_research/#_3","title":"\uc9c0\uc2dd \uc99d\ub958 \ubc0f \ubaa8\ub378 \ud569\uc131","text":"<ul> <li>\ub300\ud615 \ubaa8\ub378\uc5d0\uc11c \uc18c\ud615 \ubaa8\ub378\ub85c \uc9c0\uc2dd \uc804\uc774</li> <li>\ub2e4\uc911 \ubaa8\ub378 \uc559\uc0c1\ube14</li> <li>\ud6a8\uc728\uc801 \uc99d\ub958 \uae30\ubc95 </li> </ul>"},{"location":"trends_projects/license_data_issues/","title":"\ub370\uc774\ud130 \ubc0f \ub77c\uc774\uc120\uc2a4 \ubb38\uc81c","text":""},{"location":"trends_projects/license_data_issues/#_2","title":"\ub370\uc774\ud130 \uc0ac\uc6a9 \uc724\ub9ac","text":"<ul> <li>\ud06c\ub864\ub9c1 \ub370\uc774\ud130 \uc0ac\uc6a9 \ubb38\uc81c</li> <li>\uc800\uc791\uad8c \uace0\ub824\uc0ac\ud56d</li> <li>\ub370\uc774\ud130 \ucd9c\ucc98 \ud22c\uba85\uc131</li> </ul>"},{"location":"trends_projects/license_data_issues/#_3","title":"\ubaa8\ub378 \ub77c\uc774\uc120\uc2a4 \uc774\uc288","text":"<ul> <li>\uc624\ud508\uc18c\uc2a4 vs \uc0c1\uc5c5\uc6a9 \ub77c\uc774\uc120\uc2a4</li> <li>\ud30c\uc0dd \ubaa8\ub378\uc758 \ub77c\uc774\uc120\uc2a4 \uc81c\uc57d</li> <li>\uc8fc\uc694 \uc624\ud508\uc18c\uc2a4 \ubaa8\ub378\uc758 \ub77c\uc774\uc120\uc2a4 \ube44\uad50</li> </ul>"},{"location":"trends_projects/license_data_issues/#_4","title":"\uac1c\uc778\uc815\ubcf4 \ubc0f \ud3b8\ud5a5","text":"<ul> <li>\ubbfc\uac10 \uc815\ubcf4 \ucc98\ub9ac \ubc29\ubc95</li> <li>\ud3b8\ud5a5 \ubc0f \uc720\ud574 \ucf58\ud150\uce20 \ud544\ud130\ub9c1</li> <li>\uc548\uc804\ud55c \ubaa8\ub378 \uac1c\ubc1c \uac00\uc774\ub4dc\ub77c\uc778</li> </ul>"},{"location":"trends_projects/license_data_issues/#_5","title":"\uaddc\uc81c \ubc0f \ucef4\ud50c\ub77c\uc774\uc5b8\uc2a4","text":"<ul> <li>\uad6d\uac00\ubcc4 AI \uaddc\uc81c \ud604\ud669</li> <li>API \ucd9c\ub825\ubb3c \uc0ac\uc6a9 \uc81c\uc57d</li> <li>\ubc95\uc801 \ub9ac\uc2a4\ud06c \ucd5c\uc18c\ud654 \uc804\ub7b5 </li> </ul>"},{"location":"tuning_techniques/%08finetuning_frameworks/","title":"Supervised Fine-Tuning (SFT)","text":""},{"location":"tuning_techniques/%08finetuning_frameworks/#sft","title":"SFT \uac1c\uc694","text":"<ul> <li>SFT\uc758 \ubaa9\uc801\uacfc \uc911\uc694\uc131</li> <li>\uc785\ub825-\ucd9c\ub825 \ub9e4\ud551\uc744 \ud1b5\ud55c \ud559\uc2b5</li> </ul>"},{"location":"tuning_techniques/%08finetuning_frameworks/#sft_1","title":"SFT \ub370\uc774\ud130 \uc900\ube44","text":"<ul> <li>\uc9c0\uc2dc-\uc751\ub2f5 \uc30d \uad6c\uc131</li> <li>\ub2e4\uc591\ud55c \uc9c0\uc2dc \ud615\ud0dc \ud3ec\ud568\ud558\uae30</li> <li>\ud488\uc9c8 \uc911\uc2ec \ub370\uc774\ud130 \ud050\ub808\uc774\uc158</li> </ul>"},{"location":"tuning_techniques/%08finetuning_frameworks/#sft_2","title":"SFT \ucd5c\uc801\ud654 \uc804\ub7b5","text":"<ul> <li>\uacfc\uc801\ud569 \ubc29\uc9c0 \uae30\ubc95</li> <li>\ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub9c1</li> <li>\uc870\uae30 \uc885\ub8cc \uc804\ub7b5 </li> </ul>"},{"location":"tuning_techniques/%08finetuning_frameworks/#llm-hugging-face-vs-deepspeed-vs-unsloth","title":"\ub300\uaddc\ubaa8 \uc5b8\uc5b4\ubaa8\ub378(LLM) \uc9c0\ub3c4 \ud30c\uc778\ud29c\ub2dd: Hugging Face vs. DeepSpeed vs. Unsloth","text":""},{"location":"tuning_techniques/%08finetuning_frameworks/#introduction","title":"\uac1c\uc694 (Introduction)","text":"<p>\ub300\uaddc\ubaa8 \uc5b8\uc5b4\ubaa8\ub378(LLM)\uc758 \uc9c0\ub3c4 \ud559\uc2b5 \uae30\ubc18 \ud30c\uc778\ud29c\ub2dd(Supervised Fine-Tuning, SFT)\uc740 \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130\uc14b\uc5d0 \ub9de\ucdb0 \ubbf8\uc138\uc870\uc815\ud558\uc5ec \ud2b9\uc815 \uc791\uc5c5 \uc131\ub2a5\uc774\ub098 \uc751\ub2f5 \ud488\uc9c8\uc744 \ub192\uc774\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4. \ud2b9\ud788 Decoder-Only Transformer \uad6c\uc870(\uc608: GPT \uacc4\uc5f4 \ubaa8\ub378)\uc758 \ud30c\uc778\ud29c\ub2dd\uc740 \uc8fc\uc5b4\uc9c4 \ud504\ub86c\ud504\ud2b8\uc5d0 \uc774\uc5b4\uc9c0\ub294 \ub2e4\uc74c \ud1a0\ud070\uc744 \uc608\uce21\ud558\ub3c4\ub85d \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ud615\ud0dc\ub85c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4. \ucd5c\uadfc 2\ub144\uac04 LLM \ud30c\uc778\ud29c\ub2dd \ubd84\uc57c\uc5d0\uc11c\ub294 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ube44\ud574 \ud55c\uc815\ub41c \uc790\uc6d0\uc73c\ub85c\ub3c4 \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\ub294 \ub2e4\uc591\ud55c \uae30\ubc95\uacfc \ub3c4\uad6c\ub4e4\uc774 \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4. \ubcf8 \ubcf4\uace0\uc11c\uc5d0\uc11c\ub294 Hugging Face \uc0dd\ud0dc\uacc4, Microsoft DeepSpeed, \uadf8\ub9ac\uace0 \ucd5c\uc2e0 \ucee4\ubba4\ub2c8\ud2f0 \ud234\uc778 Unsloth\ub97c \ud65c\uc6a9\ud55c \uc9c0\ub3c4 \ud30c\uc778\ud29c\ub2dd \ubc29\ubc95\uc744 \ube44\uad50\ud569\ub2c8\ub2e4. \ub610\ud55c \uac01 \uc811\uadfc\ubc95\uc758 \ud2b9\uc9d5\uacfc \uc7a5\uc810, \ucd5c\uc2e0 \uc5f0\uad6c \ub3d9\ud5a5, \uc2e4\ubb34 \uc801\uc6a9 \uc608\uc81c \ucf54\ub4dc, \uc131\ub2a5 \ubc0f \ud6a8\uc728 \ud3c9\uac00 \uae30\uc900, Decoder-Only \ucd5c\uc801\ud654 \uae30\ubc95 \ub4f1\uc744 \uc815\ub9ac\ud569\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/%08finetuning_frameworks/#hugging-face-llm","title":"Hugging Face \uae30\ubc18 LLM \ud30c\uc778\ud29c\ub2dd","text":"<p>Hugging Face\uc758 Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\ub294 \ubc29\ub300\ud55c \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378 \uc800\uc7a5\uc18c\uc640 \ud3b8\ub9ac\ud55c API\ub97c \uc81c\uacf5\ud558\uc5ec LLM \ud30c\uc778\ud29c\ub2dd\uc744 \uc190\uc27d\uac8c \uc2dc\uc791\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4. PyTorch \uae30\ubc18\uc73c\ub85c \uad6c\ud604\ub41c <code>Trainer</code> \ud074\ub798\uc2a4 \ub610\ub294 \ud83e\udd17 Accelerate\ub97c \ud1b5\ud574 \ub2e8\uc77c GPU\ubd80\ud130 \ubd84\uc0b0 GPU\uae4c\uc9c0 \uc190\uc26c\uc6b4 \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8 \uad6c\uc131\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. Hugging Face\uc758 \uc8fc\uc694 \uac15\uc810\uc740 \uad11\ubc94\uc704\ud55c \ubaa8\ub378 \uc9c0\uc6d0\uacfc \ucee4\ubba4\ub2c8\ud2f0 \uc911\uc2ec\uc758 \uc2e0\uc18d\ud55c \uac1c\uc120\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, 2022\ub144 \ubc1c\ud45c\ub41c LoRA(Low-Rank Adaptation) \uae30 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011\uc744 \ube60\ub974\uac8c PEFT \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \ud1b5\ud569\ud558\uace0, 2023\ub144 \ub4f1\uc7a5\ud55c QLoRA \ubc29\ubc95\ub860\ub3c4 \uace7\ubc14\ub85c \uc9c0\uc6d0\ud558\uc600\uc2b5\ub2c8 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. \uc774\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790\ub294 \ucd5c\uc18c\ud55c\uc758 \ucf54\ub4dc \ubcc0\uacbd\ub9cc\uc73c\ub85c \ucd5c\uc2e0 \uc5f0\uad6c \uc131\uacfc\ub97c \uc2e4\uc2b5\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Hugging Face\ub294 \uba54\ubaa8\ub9ac \ucd5c\uc801\ud654\ub97c \uc704\ud574 8-bit \ubc0f 4-bit \uc591\uc790\ud654(qunatization)\ub97c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 <code>transformers</code>\uc5d0\uc11c <code>from_pretrained</code> \ud638\ucd9c \uc2dc <code>load_in_4bit=True</code>\ub85c \uc124\uc815\ud558\uba74, \uc0ac\uc804\ud559\uc2b5\ub41c \ubaa8\ub378 \uac00\uc911\uce58\ub97c 4\ube44\ud2b8 \uc815\ubc00\ub3c4\ub85c \ubd88\ub7ec\uc62c \uc218 \uc788\uc2b5\ub2c8 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA) (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. \uc774\ub807\uac8c \ud558\uba74 \ubaa8\ub378 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ud06c\uac8c \uc904\uc77c \uc218 \uc788\uc5b4, \ube44\uad50\uc801 *\uc801\uc740 GPU \uba54\ubaa8\ub9ac\ub85c\ub3c4 \ub300\ud615 \ubaa8\ub378\uc744 \ub2e4\ub8f0 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. \uc544\ub798 \uc608\uc2dc\ub294 OPT-350M \ubaa8\ub378\uc744 4\ube44\ud2b8\ub85c \ubd88\ub7ec\uc624\ub294 \ucf54\ub4dc\uc785\ub2c8\ub2e4:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"facebook/opt-350m\",\n    load_in_4bit=True,  # 4\ube44\ud2b8 \uc591\uc790\ud654 \ub85c\ub4dc\n    device_map=\"auto\"   # \uac00\uc6a9 GPU \uc790\ub3d9\ud560\ub2f9\n)\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n</code></pre> <p>\ub610\ud55c Hugging Face PEFT \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud558\uba74 LoRA \uc5b4\ub311\ud130\ub97c \uc190\uc27d\uac8c \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. LoRA\ub294 \ubaa8\ub378\uc758 \ubaa8\ub4e0 \uac00\uc911\uce58\ub97c \ubbf8\uc138\uc870\uc815\ud558\ub294 \ub300\uc2e0, \uc77c\ubd80 \ub9e4\ud2b8\ub9ad\uc2a4\uc5d0 \uc18c\uaddc\ubaa8\uc758 \ud559\uc2b5\uac00\ub2a5\ud55c \uc800\ub7ad\ud06c \ud589\ub82c(Adapters)\uc744 \ucd94\uac00\ud558\uc5ec \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc785\ub2c8 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. Hugging Face\ub294 <code>peft.LoraConfig</code>\uc640 <code>get_peft_model</code> \ub4f1\uc744 \ud1b5\ud574 \uae30\uc874 \ubaa8\ub378\uc5d0 LoRA \ubaa8\ub4c8\uc744 \uc0bd\uc785\ud560 \uc218 \uc788\ub294 API\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. LoRA\ub97c \uc0ac\uc6a9\ud558\uba74 \ud30c\uc778\ud29c\ub2dd\uc2dc \uba54\ubaa8\ub9ac\uc640 \uc5f0\uc0b0\ub7c9\uc744 \ud06c\uac8c \uc808\uac10\ud558\uba74\uc11c\ub3c4 \uc6d0\ub798 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac70\uc758 \uc720\uc9c0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 2023\ub144 \uc81c\uc548\ub41c QLoRA\ub294 \uc774\ub97c \ud55c \ub2e8\uacc4 \ubc1c\uc804\uc2dc\ucf1c \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uc744 4-bit\ub85c \uace0\uc815\ud558\uace0 LoRA\ub85c\ub9cc \uc5c5\ub370\uc774\ud2b8\ub97c \uc218\ud589\ud568\uc73c\ub85c\uc368, 65\uc5b5~130\uc5b5\uae09 \ubaa8\ub378\ub3c4 \ub2e8\uc77c GPU\ub85c \ubbf8\uc138\uc870\uc815 \uac00\ub2a5\ud558\uac8c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. \uc2e4\uc81c\ub85c QLoRA\ub97c \ud1b5\ud574 65B \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uc744 48GB VRAM\uc758 \ub2e8\uc77c GPU\uc5d0\uc11c \ud480 16\ube44\ud2b8 \ud30c\uc778\ud29c\ub2dd\uacfc \ub3d9\ub4f1\ud55c \uc131\ub2a5\uc73c\ub85c \ud559\uc2b5\ud558\ub294 \ub370 \uc131\uacf5\ud588\uc2b5\ub2c8 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA) (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)\u3011. \uc774\ub294 GPU 1\ub300\uc5d0\uc11c 780GB \uba54\ubaa8\ub9ac\uac00 \ud544\uc694\ud588\ub358 \uc791\uc5c5\uc744 48GB\ub85c \uc904\uc778 \uc131\uacfc\ub85c, \ub300\uaddc\ubaa8 \ubaa8\ub378 \ud30c\uc778\ud29c\ub2dd\uc758 \uc811\uadfc\uc131\uc744 \ud601\uc2e0\uc801\uc73c\ub85c \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8 ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv)\u3011. QLoRA\uc758 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub294 NF4 (4-bit NormalFloat) \uc591\uc790\ud654\uc640 \uc774\uc911 \uc591\uc790\ud654(Double Quantization), \uadf8\ub9ac\uace0 Paged Optimizer \ub4f1\uc744 \ub3c4\uc785\ud558\uc5ec \uc131\ub2a5 \uc800\ud558 \uc5c6\uc774 \uba54\ubaa8\ub9ac\ub97c \uadf9\ub2e8\uc801\uc73c\ub85c \uc544\ub080 \uac83\uc785\ub2c8 ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs) ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs)\u3011.</p> <p>Hugging Face \ubc29\ubc95\uc758 \uc7a5\uc810\uc740 \uac04\ud3b8\ud568\uacfc \ubc94\uc6a9\uc131\uc785\ub2c8\ub2e4. \ubc29\ub300\ud55c \uc0ac\uc804\ud559\uc2b5 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c Hugging Face Hub\uc5d0\uc11c \uc989\uc2dc \ubd88\ub7ec\uc640 \ud65c\uc6a9\ud560 \uc218 \uc788\uace0, \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ubd80\ud130 \ud3c9\uac00\uae4c\uc9c0 \ud1b5\ud569\ub41c \uc0dd\ud0dc\uacc4(\ud83e\udd17 Datasets \ub4f1)\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud2b9\ud788 Transformer-Decorder \ubaa8\ub378(\uc608: GPT-2, GPT-3, LLaMA \ub4f1)\uc758 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ud0dc\uc2a4\ud06c\ub97c \uc704\ud55c \ud30c\uc778\ud29c\ub2dd \uc608\uc81c\uac00 \ud48d\ubd80\ud558\uba70, \ud559\uc2b5 loop, \ud1a0\ud06c\ub098\uc774\uc800, \ubaa8\ub378 \ubcd1\ub82c\ud654 \ub4f1\uc774 \uc798 \ucd94\uc0c1\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8 <code>Trainer</code>\ub97c \uc0ac\uc6a9\ud558\ub294 \uc608\uc2dc \ucf54\ub4dc (\uc608: GPT-2\ub97c \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ub370\uc774\ud130\ub85c \ud30c\uc778\ud29c\ub2dd) \ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <pre><code>from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"output\",\n    per_device_train_batch_size=2,\n    num_train_epochs=3,\n    fp16=True,                      # FP16 \ud63c\ud569 \uc815\ubc00\ub3c4 \uc0ac\uc6a9\n    logging_steps=100,\n    save_steps=500,\n    deepspeed=None                  # (DeepSpeed \uc0ac\uc6a9\uc2dc \uc124\uc815 \ud30c\uc77c \uacbd\ub85c \uc9c0\uc815)\n)\ntrainer = Trainer(model=model, args=training_args, \n                  train_dataset=train_ds, eval_dataset=eval_ds, \n                  data_collator=data_collator)\ntrainer.train()\n</code></pre> <p>\ucc38\uace0: \uc0c1\uae30 \ucf54\ub4dc\uc5d0\uc11c <code>deepspeed=None</code>\ub85c \ub450\uba74 Hugging Face\uc758 \uae30\ubcf8 Trainer\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4. DeepSpeed\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 <code>deepspeed=\"ds_config.json\"</code>\ucc98\ub7fc \uc124\uc815 \ud30c\uc77c\uc744 \uc9c0\uc815\ud558\uac70\ub098 \ud83e\udd17 Accelerate\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4 (\uc544\ub798 DeepSpeed \uc139\uc158 \ucc38\uace0).</p> <p>Hugging Face \uae30\ubc18 \uc811\uadfc\uc758 \ub2e8\uc810\uc774\ub77c\uba74, \uc544\uc8fc \ud070 \ubaa8\ub378\uc744 \ub2e4\ub8f0 \ub54c\ub294 \uae30\ubcf8 \ud658\uacbd\uc73c\ub85c\ub294 \ud55c\uacc4\uac00 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc218\uc2ed\uc5b5~\uc218\ubc31\uc5b5 \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uc744 \uc804\uccb4 \ubbf8\uc138\uc870\uc815(full fine-tuning)\ud558\ub824\uba74 \uba40\ud2f0 GPU\uac00 \ud544\uc218\uc774\uba70, \uc774 \uacbd\uc6b0 DeepSpeed\ub098 FSDP \ub4f1\uc758 \ubcf4\uc870\uac00 \ud544\uc694\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc911\uc18c\uaddc\ubaa8 \ubaa8\ub378\uc774\ub098 LoRA\uac19\uc740 \ud30c\ub77c\ubbf8\ud130 \ud6a8\uc728 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uba74 Hugging Face\ub9cc\uc73c\ub85c\ub3c4 \ucda9\ubd84\ud788 \uc2e4\ud5d8\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c Hugging Face\ub294 \uc5f0\uad6c \uac1c\ubc1c\uc758 \ucd9c\ubc1c\uc810\uc73c\ub85c\uc11c \ucd5c\uc2e0 \uae30\ubc95\ub4e4\uc744 \ube60\ub974\uac8c \ubc1b\uc544\ub4e4\uc774\uace0 \uc788\uc5b4, \uc2e4\ubb34\uc5d0\uc11c\ub3c4 \uac00\uc7a5 \ub110\ub9ac \uc4f0\uc774\ub294 LLM \ud30c\uc778\ud29c\ub2dd \ud50c\ub7ab\ud3fc\uc785\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/%08finetuning_frameworks/#deepspeed","title":"DeepSpeed\ub97c \ud65c\uc6a9\ud55c \ub300\uaddc\ubaa8 \ubaa8\ub378 \ud30c\uc778\ud29c\ub2dd","text":"<p>DeepSpeed\ub294 \ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8\uac00 \uac1c\ubc1c\ud55c \ub300\uaddc\ubaa8 \ubd84\uc0b0 \ud559\uc2b5 \ucd5c\uc801\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c, \ud2b9\ud788 \uac70\ub300 \uc5b8\uc5b4\ubaa8\ub378\uc758 \ud559\uc2b5\uc744 \uc18d\ub3c4\uc640 \uc2a4\ucf00\uc77c \uce21\uba74\uc5d0\uc11c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. DeepSpeed\uc758 \ud575\uc2ec\uc5d0\ub294 ZeRO (Zero Redundancy Optimizer) \uc54c\uace0\ub9ac\uc998\uc774 \uc788 (DeepSpeed)10\u3011. ZeRO\ub294 \ub370\uc774\ud130 \ubcd1\ub82c \ud559\uc2b5 \uc2dc \uac01 GPU\uc5d0 \ub3d9\uc77c\ud558\uac8c \ubcf5\uc81c\ub418\ub358 \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc, \uadf8\ub798\ub514\uc5b8\ud2b8, \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130\ub97c shard(\ubd84\ud560)\ud558\uc5ec GPU\ub4e4 \uac04\uc5d0 \ubd84\uc0b0 \uc800\uc7a5 (DeepSpeed)10\u3011. \uc774\ub807\uac8c \ud558\uba74 \uc911\ubcf5\uc73c\ub85c \uba54\ubaa8\ub9ac\ub97c \uc7a1\uc544\uba39\ub294 \uc694\uc18c\uac00 \uc0ac\ub77c\uc838, \ubaa8\ub378 \ud06c\uae30\uac00 \ucee4\uc838\ub3c4 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc801\uc73c\ub85c \ubd84\uc0b0\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. ZeRO\ub294 \ub2e8\uacc4\ubcc4\ub85c \ubc1c\uc804\ub418\uc5b4 Stage 1(\uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc \ubd84\uc0b0), Stage 2(+ gradient \ubd84\uc0b0), Stage 3(+ \ud30c\ub77c\ubbf8\ud130 \uc790\uccb4 \ubd84\uc0b0)\uc73c\ub85c \uad6c\ubd84\ub418\uba70, Stage \uc22b\uc790\uac00 \ub192\uc744\uc218\ub85d GPU \uba54\ubaa8\ub9ac \ubd80\ub2f4\uc774 \uac10\uc18c (DeepSpeed)10\u3011. \ud2b9\ud788 ZeRO-3\ub294 \ubaa8\ub4e0 \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130\ub97c \ubaa8\ub4e0 GPU\uc5d0 \ub098\ub204\uc5b4 \uc62c\ub824\ub193\uace0 \ud544\uc694 \uc2dc \ub3d9\uc801\uc73c\ub85c \ubd88\ub7ec\uc4f0\ub294 \ubc29\uc2dd\uc73c\ub85c, \uac1c\ubcc4 GPU\uc5d0\ub294 \uc804\uccb4 \ubaa8\ub378\uc758 \uc77c\ubd80\ub9cc \uc0c1\uc8fc\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc608\ub97c \ub4e4\uc5b4 70\uc5b5~130\uc5b5 \uac1c \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uc744 \ub2e8\uc77c \ub610\ub294 \uc18c\uc218 GPU\uc5d0\uc11c \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc774 \uac00\ub2a5\ud574\uc84c (ZeRO-Offload - DeepSpeed)09\u3011. DeepSpeed \ud300\uc758 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \ub530\ub974\uba74, ZeRO-Offload \uae30\ub2a5\uae4c\uc9c0 \ud65c\uc6a9\ud558\uba74 10\uc5b5~13\uc5b5 \ud30c\ub77c\ubbf8\ud130 GPT-2 \ubaa8\ub378\ub3c4 \ub2e8\uc77c 32GB GPU\uc5d0\uc11c \ud559\uc2b5\ud560 \uc218 \uc788 (ZeRO-Offload - DeepSpeed)09\u3011. \uc544\ub798\ub294 DeepSpeed\uc758 ZeRO-Offload\ub97c \ud1b5\ud55c \ub2e8\uc77c GPU \ub300\uc6a9\ub7c9 \ubaa8\ub378 \ud559\uc2b5 \uc0ac\ub840\uc785\ub2c8\ub2e4:</p> <ul> <li>\u201cZeRO-Offload\ub294 \uc635\ud2f0\ub9c8\uc774\uc800 \uba54\ubaa8\ub9ac\uc640 \uc5f0\uc0b0\uc744 CPU\ub85c \uc624\ud504\ub85c\ub4dc\ud558\uc5ec, \ucd5c\ub300 130\uc5b5 \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub2ec\ud558\ub294 \ud070 \ubaa8\ub378\ub3c4 \ub2e8\uc77c GPU\uc5d0\uc11c \ud6a8\uc728\uc801\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uac8c \ud574 (ZeRO-Offload - DeepSpeed)09\u3011.\u201d</li> </ul> <p>DeepSpeed\uc758 \ub610 \ub2e4\ub978 \uac15\uc810\uc740 \ubcd1\ub82c\ud654\uc640 \ucd5c\uc801\ud654 \uc804\ub7b5\uc758 \ub2e4\uc591\uc131\uc785\ub2c8\ub2e4. \ubaa8\ub378 \ubcd1\ub82c\ud654, \ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654, mixed precision \uc5f0\uc0b0, Gradient Accumulation \ub4f1\uc758 \uae30\ubc95\uc744 \ud1b5\ud569\uc801\uc73c\ub85c \uc9c0\uc6d0\ud558\uc5ec GPU \uc5ec\ub7ec \ub300\ub97c \ucd5c\ub300\ud55c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c CPU Offloading(ZeRO-Offload)\uacfc NVMe Offloading(ZeRO-Infinity)\uc744 \ud1b5\ud574, GPU \uba54\ubaa8\ub9ac\uac00 \ubd80\uc871\ud560 \uacbd\uc6b0 \uc77c\ubd80 \ub370\uc774\ud130(\uc608: \ubaa8\ub378 \uac00\uc911\uce58\ub098 \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc)\ub97c CPU RAM\uc774\ub098 SSD\ub85c \ubd84\uc0b0\uc2dc\ud0b4\uc73c\ub85c\uc368 \uc0ac\uc2e4\uc0c1 \ubb34\uc81c\ud55c\uc5d0 \uac00\uae4c\uc6b4 \ubaa8\ub378 \uc0ac\uc774\uc988\uae4c\uc9c0\ub3c4 \ud559\uc2b5\uc744 \uc2dc\ub3c4\ud560 \uc218 \uc788 (ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research) (ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research)94\u3011. \uc774\ub7ec\ud55c \uadf9\ub2e8\uc801 \ud655\uc7a5\uc131 \ub355\ubd84\uc5d0 DeepSpeed\ub294 GPT-3(175B) \uae09 \ubaa8\ub378 \ud559\uc2b5\uc774\ub098 \uc218\uc870 \uac1c \ud30c\ub77c\ubbf8\ud130 \uc2e4\ud5d8\ucc98\ub7fc \ucd5c\ucca8\ub2e8 \uc2a4\ucf00\uc77c\uc758 \uc5f0\uad6c\uc5d0 \ud544\uc218\uc801\uc778 \ub3c4\uad6c\ub85c \uc790\ub9ac\uc7a1\uc558\uc2b5\ub2c8\ub2e4.</p> <p>DeepSpeed\ub97c \uc2e4\ubb34\uc5d0 \ud65c\uc6a9\ud558\ub824\uba74 \uc124\uc815 \ud30c\uc77c\uacfc \ub7f0\ucc98\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc774 \uc77c\ubc18\uc801\uc785\ub2c8\ub2e4. Hugging Face Trainer\uc5d0\ub3c4 <code>deepspeed</code> \uc778\uc790\ub97c \ud1b5\ud574 DeepSpeed\ub97c \ud1b5\ud569\ud560 \uc218 \uc788\uc73c\uba70, \ud83e\udd17 Accelerate \ud234\uc744 \uc4f0\uba74 \ub300\ud654\ud615\uc73c\ub85c \uc124\uc815 \ud30c\uc77c\uc744 \uc0dd\uc131\ud560 \uc218  (DeepSpeed) (DeepSpeed)164\u3011. \uc608\ub97c \ub4e4\uc5b4, \uc544\ub798\uc640 \uac19\uc740 DeepSpeed \uc124\uc815(<code>ds_config.json</code>)\uc744 \uc900\ube44\ud558\uc5ec Trainer\uc5d0 \uc804\ub2ec\ud558\uba74 ZeRO \uae30\ubc18 \ud6c8\ub828\uc774 \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4:</p> <pre><code>{\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"offload_param\": {\n      \"device\": \"cpu\"\n    }\n  },\n  \"fp16\": {\n    \"enabled\": true\n  }\n}\n</code></pre> <p>\uc704 \uc124\uc815\uc740 ZeRO-3 \ub2e8\uacc4\uc5d0\uc11c \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130\ub97c CPU\ub85c \uc624\ud504\ub85c\ub4dc\ud558\ub3c4\ub85d \uc9c0\uc815\ud55c \uc608\uc2dc\uc785\ub2c8\ub2e4. <code>TrainingArguments(..., deepspeed=\"ds_config.json\")</code> \ucc98\ub7fc \uc124\uc815\ud558\uba74 Hugging Face Trainer\uac00 \ub0b4\ubd80\uc801\uc73c\ub85c DeepSpeed \uc5d4\uc9c4\uc744 \ucd08\uae30\ud654\ud558\uc5ec \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \ub610\ub294 <code>deepspeed.init</code> API\ub97c \uc9c1\uc811 \uc0ac\uc6a9\ud574 \ubaa8\ub378, \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uac10\uc2fc \ub4a4 <code>deepspeed.run</code>\uc73c\ub85c \ud6c8\ub828 loop\uc744 \uad6c\ud604\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc5b4\ub5a4 \ubc29\ubc95\uc774\ub4e0, \uae30\uc874 PyTorch \ucf54\ub4dc\ub97c \ud06c\uac8c \ubcc0\uacbd\ud558\uc9c0 \uc54a\uc73c\uba74\uc11c DeepSpeed\uc758 \uc774\uc810\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4\ub294 \uac83\uc774 \uc7a5\uc810\uc785\ub2c8\ub2e4.</p> <p>DeepSpeed\uc640 Hugging Face PEFT\ub97c \uc870\ud569\ud558\uc5ec \uc0ac\uc6a9\ud558\ub294 \uac83\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 LoRA \uc801\uc6a9 \ubaa8\ub378\uc744 DeepSpeed ZeRO-3\ub85c \ubd84\uc0b0 \ud559\uc2b5\ud558\uac70\ub098, QLoRA(4\ube44\ud2b8 + LoRA)\ub97c DeepSpeed\uc640 \ud568\uaed8 \ud65c\uc6a9\ud558\uc5ec \ub2e4\uc911 GPU\uc5d0\uc11c \ucd08\uac70\ub300 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \uc2e4\ud5d8\ub4e4\uc774 \ubcf4\uace0\ub418 (DeepSpeed) (DeepSpeed)142\u3011. Hugging Face \uac00\uc774\ub4dc\uc5d0\uc11c\ub294 8x H100 (80GB) GPU\ub85c LLaMA-70B \ubaa8\ub378\uc744 LoRA+ZeRO-3 \uc124\uc815\uc73c\ub85c SFT(\uc9c0\ub3c4\ud30c\uc778\ud29c\ub2dd)\ud558\ub294 \uc608\uc2dc\ub97c \uc81c\uacf5\ud558\uace0  (DeepSpeed) (DeepSpeed)164\u3011. \uc774\ucc98\ub7fc DeepSpeed\ub294 Hugging Face \uc0dd\ud0dc\uacc4\uc640\ub3c4 \uc798 \ub9de\ubb3c\ub824 \ub3d9\uc791\ud558\uba70, \ud30c\uc778\ud29c\ub2dd \uc18d\ub3c4 \ubc0f \ud655\uc7a5\uc131\uc744 \ub192\uc774\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.</p> <p>\uc815\ub9ac\ud558\uba74, DeepSpeed\uc758 \ud2b9\uc9d5\uacfc \uc7a5\uc810\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <ul> <li>ZeRO \uc54c\uace0\ub9ac\uc998\uc744 \ud1b5\ud55c \uba54\ubaa8\ub9ac \ucd5c\uc801\ud654 \ubc0f \ubaa8\ub378 \ubd84\uc0b0: \ub3d9\uc77c \uc790\uc6d0\uc73c\ub85c \ub354 \ud070 \ubaa8\ub378 \ud559 (DeepSpeed)110\u3011 </li> <li>CPU/NVMe \uc624\ud504\ub85c\ub4dc\ub85c \ub2e8\uc77c GPU \uba54\ubaa8\ub9ac \ud55c\uacc4 (ZeRO-Offload - DeepSpeed)109\u3011 </li> <li>\uace0\ub3c4\ud654\ub41c \ubd84\uc0b0 \ubcd1\ub82c \ud559\uc2b5 \uc9c0\uc6d0: \uc218\uc2ed~\uc218\ubc31 GPU\uae4c\uc9c0 \ud6a8\uc728\uc801 \uc2a4\ucf00\uc77c \uc544\uc6c3</li> <li>\uc131\ub2a5 \ucd5c\uc801\ud654 \ucee4\ub110 \uc81c\uacf5: DeepSpeed\uc758 CPU Adam \uc635\ud2f0\ub9c8\uc774\uc800\ub294 \uae30\ubcf8 PyTorch \ub300\ube44 5~7 (ZeRO-Offload - DeepSpeed)119\u3011 \ub4f1</li> </ul> <p>\ub2e8\uc810\uc73c\ub85c\ub294 \ud658\uacbd \uc124\uc815\uc758 \ubcf5\uc7a1\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc124\uc815 \ud30c\uc77c \uc791\uc131, \ub7f0\ucc98 \uba85\ub839 \ub4f1 \ucc98\uc74c \uc0ac\uc6a9\uc2dc \uc9c4\uc785\uc7a5\ubcbd\uc774 \uc788\uc73c\uba70, \uc791\uc740 \uaddc\ubaa8 \uc2e4\ud5d8\uc5d0\ub294 \uacfc\ud55c \uce21\uba74\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ub3d9\uc77c\ud55c \uc5f0\uc0b0\uc774\ub77c\ub3c4 \uc57d\uac04\uc758 \uc624\ubc84\ud5e4\ub4dc(\ud1b5\uc2e0 \ub300\uae30 \ub4f1)\uac00 \uc874\uc7ac\ud558\ubbc0\ub85c, \ubaa8\ub378\uc774 \ucda9\ubd84\ud788 \ud06c\uac70\ub098 \ubd84\uc0b0\uc774 \ud544\uc694\ud55c \uacbd\uc6b0\uc5d0 \uac00\uc7a5 \ud070 \ud6a8\uacfc\ub97c \ubd05\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4, \uc2e4\ubb34\uc5d0\uc11c \uc218\uc2ed\uc5b5~\uc218\ucc9c\uc5b5 \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\uc744 \ub2e4\ub904\uc57c \ud55c\ub2e4\uba74 DeepSpeed\ub294 \uc0ac\uc2e4\uc0c1 \ud45c\uc900 \ub3c4\uad6c\ub85c \uc790\ub9ac\uc7a1\uc558\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/%08finetuning_frameworks/#unsloth-llm","title":"Unsloth\ub97c \ud65c\uc6a9\ud55c \uace0\uc18d LLM \ud30c\uc778\ud29c\ub2dd","text":"<p>Unsloth\ub294 2023\ub144 \ucee4\ubba4\ub2c8\ud2f0\uc5d0\uc11c \ub4f1\uc7a5\ud55c \uacbd\ub7c9\ud654 LLM \ud30c\uc778\ud29c\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c, \u201cHugging Face \ud638\ud658\u201d\uc744 \ud45c\ubc29\ud558\uba74\uc11c\ub3c4 \ud559\uc2b5 \uc18d\ub3c4\ub97c 2\ubc30 \uc774\uc0c1 \ub192\uc774\uace0 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc744 40~70% \uc904\uc774\ub294 \ud601\uc2e0\uc744 \ubcf4\uc5ec\uc8fc\uace0  (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L92\u3011. Unsloth\uc758 \uc811\uadfc\ubc95\uc740 \uae30\uc874 Hugging Face <code>Transformers</code> \ubaa8\ub378\uc758 \uc77c\ubd80 \uc5f0\uc0b0\uc744 Triton \uae30\ubc18\uc758 \ub9de\ucda4 \ucee4\ub110\ub85c \ub300\uccb4\ud558\uc5ec PyTorch \uc218\uc900\uc5d0\uc11c\uc758 \ube44\ud6a8\uc728\uc744 \uc81c\uac70\ud558\ub294  (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L72\u3011. \uad6c\uccb4\uc801\uc73c\ub85c, Self-Attention, FFN \ub4f1 Transformer \ud575\uc2ec \ubaa8\ub4c8\uc758 backward \uc5f0\uc0b0\uc744 \uc218\uc2dd\uc73c\ub85c \uc9c1\uc811 \uc720\ub3c4\ud558\uc5ec Triton\uc73c\ub85c \uad6c\ud604\ud568\uc73c\ub85c\uc368, \uac19\uc740 \uc791\uc5c5\uc744 \ud558\uba74\uc11c\ub3c4 \uba54\ubaa8\ub9ac \ubcf5\uc0ac\ub098 \uc911\uac04 \uc5f0\uc0b0 overhead\ub97c \ucd5c\uc18c\ud654 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L72\u3011. \uc774\ub7f0 \uc218\ub3d9 \ucd5c\uc801\ud654(manual backprop) \uae30\ubc95 \ub355\ubd84\uc5d0, \ub3d9\uc77c\ud55c QLoRA \ud30c\uc778\ud29c\ub2dd\uc774\ub77c\ub3c4 Unsloth \uc0ac\uc6a9 \uc2dc \ud559\uc2b5 \uc18d\ub3c4\uac00 \uc57d 2\ubc30\ub85c \ud5a5\uc0c1\ub418\uace0 GPU VRAM \uc0ac\uc6a9\uc740 \uc808\ubc18 \uc774\ud558\ub85c \uac10\uc18c\ud558\ub294 \uacb0\uacfc\ub97c \uc5bb (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L88\u3011. \ub180\ub78d\uac8c\ub3c4 \ubaa8\ub378\uc758 \ucd5c\uc885 \uc131\ub2a5 \uc800\ud558\uac00 0%\uc784\uc774 \uac80\uc99d\ub418\uc5c8\ub294\ub370, \uc774\ub294 Unsloth\uc758 \ucee4\ub110 \ucd5c\uc801\ud654\uac00 \uadfc\ubcf8\uc801\uc73c\ub85c \ub3d9\uc77c\ud55c \uacc4\uc0b0\uc744 \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \uad6c\ud604\ud55c \uac83\uc774\ubbc0\ub85c \uc815\ud655\ub3c4\uac00 \ubcf4\uc874\ub418\uae30 \ub54c (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L68\u3011.</p> <p>Unsloth\ub294 Hugging Face\uc640\uc758 \ud638\ud658\uc131\uc744 \uac15\uc870\ud569\ub2c8\ub2e4. \uc0ac\uc6a9\ubc95\ub3c4 \ub9e4\uc6b0 \ube44\uc2b7\ud558\uc5ec, <code>FastLanguageModel.from_pretrained()</code> \ud568\uc218\ub85c \ubaa8\ub378\uc744 \ubd88\ub7ec\uc624\uba74 \ub0b4\ubd80\uc801\uc73c\ub85c <code>transformers</code> \ubaa8\ub378\uc744 \ub798\ud551\ud55c Unsloth \ubaa8\ub378 \uac1d\uccb4\uc640 \ud1a0\ud06c\ub098\uc774\uc800\ub97c \ubc18 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)L22\u3011. \uc608\ub97c \ub4e4\uc5b4 \ub2e4\uc74c\uacfc \uac19\uc774 LLaMA \uacc4\uc5f4 \ubaa8\ub378\uc744 Unsloth\ub85c \ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/mistral-7b-bnb-4bit\",  # HF \ud5c8\ube0c \ubaa8\ub378\uba85 (4-bit \uc591\uc790\ud654\ub41c Mistral 7B)\n    max_seq_length=2048                       # \ucd5c\ub300 \uc2dc\ud000\uc2a4 \uae38\uc774 (RoPE \uc2a4\ucf00\uc77c\ub9c1 \uc790\ub3d9\uc801\uc6a9)\n)\n</code></pre> <p>\ubd88\ub7ec\uc628 \ubaa8\ub378\uc740 Hugging Face <code>transformers</code>\uc640 \uac70\uc758 \ub3d9\uc77c\ud55c \uc778\ud130\ud398\uc774\uc2a4\ub97c \uc81c\uacf5\ud558\ubbc0\ub85c, <code>transformers.Trainer</code>\ub098 \ud83e\udd17 TRL\uc758 <code>SFTTrainer</code> \ub4f1\uc5d0 \uadf8\ub300\ub85c \ub123\uc5b4\uc11c \uc0ac\uc6a9\ud560 \uc218 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L66\u3011. Unsloth\ub294 \ud604\uc7ac LLaMA \uacc4\uc5f4(Llama-2, CodeLlama \ub4f1)\uacfc Mistral, Qwen \ub4f1 GPT \uc720\uc0ac \uc544\ud0a4\ud14d\ucc98\ub97c \uc9c0\uc6d0\ud558\uba70, \ub2e4\uc591\ud55c NVIDIA GPU(\uc608: GTX 16GB\uae09\ubd80\ud130 A100/H100\uae4c\uc9c0)\uc5d0\uc11c  (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L63\u3011. \ud2b9\ud788 FP16, BF16 \ud63c\ud569\uc815\ubc00\ub3c4\ub3c4 \uc635\uc158\uc73c\ub85c \ucf24 \uc218 \uc788\uace0, \uc591\uc790\ud654\ub41c \ubaa8\ub378(<code>bnb-4bit</code>)\ub3c4 \uc9c1\uc811 \ub85c\ub4dc\ud560 \uc218 \uc788\uc5b4 (bitsandbytes \ub77c\uc774\ube0c\ub7ec\ub9ac \ud544\uc694), Hugging Face\uc5d0\uc11c \ud558\ub358 4-bit QLoRA \ud30c\uc778\ud29c\ub2dd\uc744 \uac70\uc758 \uadf8\ub300\ub85c \uc9c4\ud589\ud558\uba74\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ub204\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Unsloth\uc758 \ud2b9\uae30\ud560 \ub9cc\ud55c \uae30\ub2a5 \uc911 \ud558\ub098\ub294 RoPE Scaling\uc744 \uc790\ub3d9 \ucc98\ub9ac\ud558\ub294 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L15\u3011. RoPE(Rotary Positional Embedding)\ub294 GPT \uacc4\uc5f4\uc5d0\uc11c \uc4f0\uc774\ub294 \uc704\uce58\uc778\ucf54\ub529 \uae30\ubc95\uc778\ub370, Unsloth \ubaa8\ub378 \ub85c\ub4dc\uc2dc <code>max_seq_length</code>\ub97c \ud06c\uac8c \uc9c0\uc815\ud558\uba74 \ud559\uc2b5 \uc2dc \ub354 \uae34 \ubb38\ub9e5\uae38\uc774\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ub0b4\ubd80\uc801\uc73c\ub85c \uc8fc\ud30c\uc218\ub97c \uc2a4\ucf00\uc77c\ub9c1\ud574 \uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ubcf8\ub798 2048 \ud1a0\ud070\uae4c\uc9c0\uc600\ub358 LLaMA-2 \ubaa8\ub378\ub3c4 \ucd5c\ub300 4\ubc30 \uc774\uc0c1 \uae34 \ucee8\ud14d\uc2a4\ud2b8\uae4c\uc9c0 \ud30c\uc778\ud29c\ub2dd\ud560 (Fine-tuning Guide | Unsloth Documentation)L163\u3011, \uc77c\ubd80 \ucd5c\uc2e0 \ubaa8\ub378(Llama-3.3 70B \ub4f1)\uc740 Unsloth\ub85c 8\ub9cc~3\uc2ed\ub9cc \ud1a0\ud070 \uc774\uc0c1\uc758 \ubb38\ub9e5 \ud559\uc2b5\ub3c4 \uc2dc\ub3c4\ub418\uace0 (GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1 &amp; Reasoning LLMs 2x faster with 70% less memory! )L308\u3011. \uae34 \ubb38\ub9e5 \ub300\uc751\uc740 Decoder-Only \ubaa8\ub378\uc758 \uc2e4\uc81c \ud65c\uc6a9\ub3c4\ub97c \ub192\uc774\ub294 \uc911\uc694\ud55c \ucd5c\uc801\ud654\uc778\ub370, Unsloth\uac00 \uc774\ub97c \ud3b8\ub9ac\ud558\uac8c \uc9c0\uc6d0\ud558\ub294 \uc810\uc740 \uc2e4\uc6a9\uc801 \uc7a5\uc810\uc774\ub77c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc694\uc57d\ud558\uba74 Unsloth\uc758 \ud2b9\uc9d5\uacfc \uc7a5\uc810:</p> <ul> <li>Triton \ucee4\ub110 \uae30\ubc18 \ucd5c\uc801\ud654\ub85c \ud559\uc2b5\uc18d\ub3c4 ~2\ubc30 \ud5a5\uc0c1, **\uba54\ubaa8\ub9ac ~50% (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L88\u3011 (\ub3d9\uc77c \ud558\ub4dc\uc6e8\uc5b4/\ubaa8\ub378 \ub300\ube44)</li> <li>Hugging Face **Transformers/PEFT\uc640 \uc644\uc804 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L63\u3011 \u2013 \uce5c\uc219\ud55c API\ub85c \uc0ac\uc6a9 \uac00\ub2a5</li> <li>QLoRA(4-bit + LoRA) \uc9c0\uc6d0 \u2013 \uc800\uc790\ub4e4\uc774 \uc81c\uacf5\ud55c \ub2e4\uc774\ub098\ubbf9 4\ube44\ud2b8 \uc591\uc790\ud654\ub85c QLoRA\uc758 \ubbf8\uc138 \uc131\ub2a5 \uc800\ud558 (Fine-tuning Guide | Unsloth Documentation) (Fine-tuning Guide | Unsloth Documentation)L174\u3011</li> <li>RoPE \ub4f1 Decoder\uc6a9 \ucd94\uac00 \uae30\ub2a5 \u2013 \ubb38\ub9e5\uae38\uc774 \ud655\uc7a5 \ub4f1 \ub514\ucf54\ub354 Transformer\uc5d0 \uc720\uc6a9\ud55c \ucd5c\uc801\ud654 \uc81c\uacf5</li> <li>\uc624\ud508\uc18c\uc2a4 \uac1c\ubc1c \ud65c\uc131\ud654 \u2013 \ucf5c\ub7a9 \ub178\ud2b8\ubd81, \ubca4\uce58\ub9c8\ud06c \uc2a4\ud06c\ub9bd\ud2b8 \uacf5\uac1c \ub4f1\uc73c\ub85c \uc7ac\ud604\uc131\uacfc \uc811 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L92\u3011</li> </ul> <p>Unsloth\uc758 \ud604\uc7ac \ud55c\uacc4\ub85c\ub294 \uc9c0\uc6d0 \uc544\ud0a4\ud14d\ucc98\uac00 \uc81c\ud55c\uc801\uc774\ub77c\ub294 \uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc8fc\ub85c Meta\uc758 Llama \uacc4\uc5f4\uacfc \uadf8 \ud30c\uc0dd\ubaa8\ub378\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\uace0, Transformer \uad6c\uc870\uac00 \ub2e4\ub978 T5(Encoder-Decoder)\ub098 GLM \uc591\ubc29\ud5a5 \ubaa8\ub378 \ub4f1\uc740 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ubd84\uc0b0 \ud559\uc2b5(\uba40\ud2f0 GPU)\uc5d0 \ub300\ud55c \uc5b8\uae09\uc774 \uc801\uc740\ub370, \uc8fc\ub85c \ub2e8\uc77c GPU\uc5d0\uc11c\uc758 \uadf9\ud55c \ucd5c\uc801\ud654\uc5d0 \ucd08\uc810\uc774 \ub9de\ucdb0\uc838 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc544\uc8fc \ud070 \ubaa8\ub378\uc744 \uc5ec\ub7ec GPU\uc5d0 \ub098\ub204\uc5b4 \ud559\uc2b5\ud558\ub294 \uc6a9\ub3c4\ub294 DeepSpeed\ub9cc\ud07c \uc8fc\uc548\uc810\uc740 \uc544\ub2d0 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ub2e8\uc77c/\uc18c\uc218 GPU\ub85c LLM\uc744 \ucd5c\ub300\ud55c \ube60\ub974\uac8c \ud29c\ub2dd\ud574\uc57c \ud558\ub294 \uc2e4\ubb34 \uc0c1\ud669\uc5d0\uc11c Unsloth\ub294 \ub300\ub2e8\ud788 \ub9e4\ub825\uc801\uc778 \uc120\ud0dd\uc9c0\uc785\ub2c8\ub2e4. \uc608\ucee8\ub300, 1\uc7a5\uc758 A100\uc73c\ub85c \ud558\ub8e8 \uac78\ub9ac\ub358 \ud30c\uc778\ud29c\ub2dd \uc791\uc5c5\uc744 Unsloth\ub85c \ubc18\ub098\uc808\uc5d0 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L77\u3011, \uac19\uc740 GPU\uc5d0\uc11c \ub354 \ud070 \ubc30\uce58 \uc0ac\uc774\uc988\ub098 \ub354 \uae34 \ubb38\ub9e5\uc744 \uc2e4\ud5d8\ud560 \uc5ec\uc720\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \uace7 \uac1c\ubc1c \uc0dd\uc0b0\uc131\uacfc \uc2e4\ud5d8 \ubc94\uc704\uc758 \ud655\ub300\ub85c \uc774\uc5b4\uc9c0\ubbc0\ub85c, \uc55e\uc73c\ub85c Unsloth\uc640 \uac19\uc740 \ucd5c\uc801\ud654 \ud234\uc758 \ud65c\uc6a9\ub3c4\ub294 \ub354\uc6b1 \ub192\uc544\uc9c8 \uc804\ub9dd\uc785\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/%08finetuning_frameworks/#_1","title":"\uc131\ub2a5 \ube44\uad50 \ubc0f \ud3c9\uac00 \ubc29\ubc95","text":"<p>LLM \ud30c\uc778\ud29c\ub2dd \uae30\ubc95\ub4e4\uc744 \ud3c9\uac00\ud560 \ub54c\uc5d0\ub294 \ubaa8\ub378\uc758 \ucd5c\uc885 \uc131\ub2a5 \ubfd0 \uc544\ub2c8\ub77c \ud559\uc2b5 \ud6a8\uc728 \uc9c0\ud45c\ub4e4\ub3c4 \uc911\uc694\ud569\ub2c8\ub2e4. \uc8fc\uc694 \ube44\uad50 \uae30\uc900\uc740 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9(VRAM), \ud559\uc2b5 \uc18d\ub3c4(throughput), \ud559\uc2b5 \uc548\uc815\uc131 \ubc0f \ud6a8\uc728\uc131 \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \ud45c\ub294 Hugging Face \uae30\ubcf8 \ubc29\ubc95, DeepSpeed, Unsloth\uc758 \uc8fc\uc694 \ud2b9\uc9d5\uacfc \uc131\ub2a5 \uc0c1\uc758 \uc7a5\ub2e8\uc810\uc744 \uc815\ub9ac\ud55c \uac83\uc785\ub2c8\ub2e4:</p> \uc811\uadfc\ubc95 \uc8fc\uc694 \ud2b9\uc9d5 \ubc0f \ucd5c\uc801\ud654 \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9 \ud559\uc2b5 \uc18d\ub3c4 \ube44\uace0 (\uc7a5\ub2e8\uc810 \uc694\uc57d) Hugging Face \uae30\ubcf8 - Pretrained \ubaa8\ub378/\ub370\uc774\ud130 \uc5d0\ucf54\uc2dc\uc2a4\ud15c- Trainer/Accelerate \ud1b5\ud55c \uc190\uc26c\uc6b4 \uad6c\ud604- PEFT: LoRA, P-Tuning \ub4f1 \uc9c0\uc6d0- 8/4-bit \uc591\uc790\ud654 \ub85c\ub4dc \uc9c0\uc6d0 \uae30\uc900 (100%) \uae30\uc900 (1\u00d7) \uc26c\uc6b4 \uad6c\ud604\uacfc \ucee4\ubba4\ub2c8\ud2f0 \uc9c0\uc6d0\uc774 \uac15\uc810. \ub300\ud615 \ubaa8\ub378\uc740 \ucd94\uac00 \ucd5c\uc801\ud654 \ud544\uc694 (\uc608: DeepSpeed \ud1b5\ud569 \uac00\ub2a5). DeepSpeed (ZeRO) - ZeRO-1/2/3 \uc635\ud2f0\ub9c8 (DeepSpeed)L110\u3011- CPU/NVMe Offlo (ZeRO-Offload - DeepSpeed)L109\u3011- \ubcd1\ub82c\ud654 \ucd5c\uc801 \ud29c\ub2dd (\uc77c\uad04 \ud1b5\uc2e0, One-bit Adam \ub4f1)- \ubd84\uc0b0 \ud6c8\ub828\uc5d0 \ud2b9\ud654 \ub9e4\uc6b0 \uc801\uc74c (\ud30c\ub77c\ubbf8\ud130/\uadf8\ub798\ub514\uc5b8\ud2b8 \ubd84\uc0b0\uc73c\ub85c GPU\ub2f9 \ubd80 (DeepSpeed)L110\u3011 \ub192\uc74c (\uba40\ud2f0 GPU\ub85c \uc120\ud615 \uc2a4\ucf00\uc77c\ub9c1, \ub2e8\uc77c GPU\uc5d0\uc120 \ub2e4\uc18c \uc624\ubc84\ud5e4\ub4dc) \ucd08\ub300\ud615 \ubaa8\ub378 \ud559\uc2b5 \uac00\ub2a5 (\uc218\uc2ed\uc5b5~\uc218\ucc9c\uc5b5\u2191  (ZeRO-Offload - DeepSpeed)L109\u3011. \ucd08\uae30 \uc124\uc815 \ubcf5\uc7a1\ud558\uc9c0\ub9cc, \ub300\uaddc\ubaa8 \uc2e4\ud5d8\uc5d4 \ud544\uc218 \ub3c4\uad6c. Unsloth (QLoRA \uae30\ubc18) - Triton \ucee4\ub110\ub85c \ubaa8\ub378 \uc5f0 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L72\u3011- \uc218\ub3d9 backprop\uc73c\ub85c \uba54\ubaa8\ub9ac \uc808\uc57d- RoPE \uc2a4\ucf00\uc77c\ub9c1\uc73c\ub85c \ubb38\ub9e5 \ud655\uc7a5- HF Transformers\uc640 \ud638\ud658 API \uc801\uc74c (\ub3d9\uc77c QLoRA \ub300\ube44 VRAM \ucd5c\ub300 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L92\u3011) \ub9e4\uc6b0 \ub192\uc74c (\ub3d9\uc77c QLoRA \ub300\ube44 ~ (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L77\u3011) \ub2e8\uc77c/\uc18c\uc218 GPU \ud658\uacbd\uc5d0 \ucd5c\uc801\ud654. \uc815\ud655\ub3c4 \uc190\uc2e4 \uc5c6\uc774  (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L68\u3011. \uc9c0\uc6d0 \ubaa8\ub378 \ud55c\uc815\uc801\uc774\ub098 \ube60\ub974\uac8c \ud655\ub300 \uc911. <p>\ud45c: Hugging Face vs. DeepSpeed vs. Unsloth\uc758 \ud2b9\uc9d5 \ubc0f \ud6a8\uc728 \ube44\uad50</p> <p>\uc704 \ube44\uad50\uc5d0\uc11c \ubcf4\ub4ef\uc774, Hugging Face + \uae30\ubcf8 PyTorch\ub294 \uad6c\ud604 \ud3b8\uc758\uc131 \uce21\uba74\uc5d0\uc11c \ub6f0\uc5b4\ub098\ub098 \ub300\ud615 \ubaa8\ub378 \ud559\uc2b5 \uc2dc \uba54\ubaa8\ub9ac \ubcd1\ubaa9\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. DeepSpeed\ub294 \uc774\ub97c \ud574\uc18c\ud558\uc5ec \ubaa8\ub378 \uc0ac\uc774\uc988 \ud55c\uacc4\ub97c \ud06c\uac8c \ub192\uc5ec\uc8fc\uc9c0\ub9cc, \uad6c\uc131 \ubcf5\uc7a1\uc131\uacfc \ud1b5\uc2e0 \uc624\ubc84\ud5e4\ub4dc\uac00 \uc57d\uac04 \uc874\uc7ac\ud569\ub2c8\ub2e4. Unsloth\ub294 \ub0ae\uc740 \uc218\uc900\uc758 \ucee4\uc2a4\ud130\ub9c8\uc774\uc9d5\uc744 \ud1b5\ud574 \uac00\uc7a5 \ub9ce\uc774 \uc4f0\uc774\ub294 \uc2dc\ub098\ub9ac\uc624(\uc608: LLaMA \uacc4\uc5f4\uc758 SFT)\uc5d0\uc11c \ucd5c\ub300\uc758 \uc18d\ub3c4/\uba54\ubaa8\ub9ac \ud6a8\uc728\uc744 \ub04c\uc5b4\uc62c\ub9b0 \uc0ac\ub840\uc785\ub2c8\ub2e4. \ud2b9\ud788 QLoRA\ucc98\ub7fc 4-bit \uc591\uc790\ud654\ub85c \uc778\ud55c 16-bit \ub300\ube44 \uc57d\uac04\uc758 \uc18d\ub3c4 \uc800\ud558\uac00 \uc6d0\ub798 (Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of ...) ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv)-L57\u3011, Unsloth \ucd5c\uc801\ud654\ub85c \uc774\ub7ec\ud55c \uc591\uc790\ud654 \uc624\ubc84\ud5e4\ub4dc\uae4c\uc9c0 \uc0c1\uc1c4\ud55c \uac83\uc774 \ud070 \uc7a5\uc810\uc785\ub2c8\ub2e4.</p> <p>\ud3c9\uac00 \ubc29\ubc95\uc73c\ub85c, \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \ud6c8\ub828 \uc911 \ucd5c\ub300 GPU VRAM \uc810\uc720\ub97c \uce21\uc815\ud569\ub2c8\ub2e4 (\uc608: <code>nvidia-smi</code> \ubaa8\ub2c8\ud130\ub9c1). DeepSpeed\uc758 \uacbd\uc6b0 ZeRO-3\ub97c \uc4f0\uba74 \uac01 GPU\uac00 \ubaa8\ub378 \uc77c\ubd80\ub9cc \uac16\uace0 \uc788\uc73c\ubbc0\ub85c \uac1c\ubcc4 GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc774 \ud06c\uac8c \uc904\uace0, \ub098\uba38\uc9c0\ub294 CPU/NVMe \uc0ac\uc6a9\ub7c9\uc73c\ub85c  (DeepSpeed) (DeepSpeed)-L20\u3011. \ucc98\ub9ac \uc18d\ub3c4\ub294 \ubcf4\ud1b5 \ucd08\ub2f9 \ucc98\ub9ac \ud1a0\ud070 \uc218 (tokens per second) \ub610\ub294 \uc2a4\ud15d\ub2f9 \uc2dc\uac04\uc73c\ub85c \uc0b0\ucd9c\ud569\ub2c8\ub2e4. \uac19\uc740 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \ubc30\uce58\ub2f9 \ud1a0\ud070 throughput\uc744 \ube44\uad50\ud558\uba74 \ucd5c\uc801\ud654 \ud6a8\uacfc\ub97c \uc815\ub7c9\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ucee8\ub300, Unsloth \ud300\uc740 \ub2e4\uc591\ud55c \ubaa8\ub378/\ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \ucd08\ub2f9 \ud1a0\ud070 \ucc98\ub9ac\ub7c9\uc744 \uce21\uc815\ud558\uc5ec Hugging Face \ub300\ube44 1.5\u00d7~2.7\u00d7 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ubcf4\uace0 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L87\u3011.</p> <p>\ud559\uc2b5 \ud6a8\uc728 \uc774\uc678\uc5d0, \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uac00 \ub610\ud55c \ud544\uc218\uc785\ub2c8\ub2e4. \ubaa8\ub378\uc774 \uc9c0\ub3c4\ud30c\uc778\ud29c\ub2dd\uc744 \ud1b5\ud574 \ubaa9\ud45c \uc791\uc5c5\uc5d0 \uc5bc\ub9c8\ub098 \ud5a5\uc0c1\ub418\uc5c8\ub294\uc9c0, \ub610\ub294 \ud639\uc2dc \uae30\uc874 \uc9c0\uc2dd\uc744 \ud6fc\uc190\ud558\uc9c0 \uc54a\uc558\ub294\uc9c0 \ub4f1\uc744 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4. Decoder-Only LLM\uc758 \uacbd\uc6b0 \uc77c\ubc18\uc801\uc73c\ub85c \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ud488\uc9c8\uc774\ub098 \ub2e4\uc591\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \ud0dc\uc2a4\ud06c \uc131\ub2a5\uc73c\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc9c0\ub3c4\ud559\uc2b5\uc73c\ub85c \ub300\ud654\ud615 \ubaa8\ub378\uc744 \ud29c\ub2dd\ud588\ub2e4\uba74 ChatGPT\uc640 \uc720\uc0ac\ud55c \ubca4\uce58\ub9c8\ud06c(Vicuna Benchmark \ub4f1)\uc5d0\uc11c \ub300\ud654 \ud488\uc9c8\uc744 \uce21\uc815\ud558\uac70\ub098, Human \ud3c9\uac00 \ud639\uc740 GPT-4\ub97c \ud65c\uc6a9\ud55c \ube44\uad50 \ud3c9\uac00\ub97c \uc218\ud589\ud560 \uc218 ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs) ()L107\u3011. QLoRA \ub17c\ubb38\uc5d0\uc11c\ub294 GPT-4 \uae30\ubc18 \uc790\ub3d9 \ud3c9\uac00\ub97c \ud1b5\ud574, 65B \ubaa8\ub378\uc744 QLoRA\ub85c \ubbf8\uc138\uc870\uc815\ud55c Guanaco\uac00 ChatGPT \ub300\ube44 99.3% \uc218\uc900\uc5d0 \ub3c4\ub2ec\ud588\uc74c\uc744  ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs)-L18\u3011. \uc774\ucc98\ub7fc \ubaa8\ub378 \ucd9c\ub825\uc758 \uc815\ub7c9\u00b7\uc815\uc131 \ud3c9\uac00\ub97c \ud1b5\ud574 \ud30c\uc778\ud29c\ub2dd\uc758 \ud6a8\uacfc\ub97c \uac80\uc99d\ud574\uc57c \ud569\ub2c8\ub2e4. \ub610\ud55c perplexity(\uc5b8\uc5b4\ubaa8\ub378\uc758 \ub85c\uadf8\ud655\ub960 \uc9c0\ud45c)\ub3c4 \uc0ac\uc6a9\ub418\ub294\ub370, \uc6d0\ub798 \ubaa8\ub378 \ub300\ube44 \ud37c\ud50c\ub809\uc11c\ud2f0 \ubcc0\ud654\ub85c \uacfc\uc801\ud569 \uc5ec\ubd80\ub098 \uc77c\ubc18\ud654 \uc131\ub2a5\uc744 \uac00\ub2a0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uc2e0 \uc5f0\uad6c\uc5d0 \ub530\ub974\uba74 \ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130\uc758 \ud488\uc9c8\uc774 \ub370\uc774\ud130\ub7c9\ubcf4\ub2e4 \uc911\uc694\ud558\uba70, \uace0\ud488\uc9c8 \uc18c\ub7c9 \ub370\uc774\ud130\ub85c\ub3c4 \uac15\ub825\ud55c \uc131\ub2a5\uc744 \ub0bc \uc218 () ()L142\u3011. Meta\uc758 LIMA \uc5f0\uad6c(2023)\uc5d0\uc11c\ub294 LLaMA 65B \ubaa8\ub378\uc744 \uc5c4\uc120\ub41c 1000\uac1c\uc758 \uc608\uc2dc\ub9cc\uc73c\ub85c \uc9c0\ub3c4\ud559\uc2b5 \ud30c\uc778\ud29c\ub2dd \ud558\uc600\uc744 \ub54c GPT-4 \ub4f1 \uac70\ub300 \ubaa8\ub378\uc5d0 \ud544\uc801\ud558\ub294 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30\ub3c4 (Paper page - LIMA: Less Is More for Alignment - Hugging Face)-L18\u3011. \uc774\ub294 \uc0ac\uc804\ud559\uc2b5\ub41c \uac70\ub300 LM\uc758 \uc7a0\uc7ac\ub825\uc744 \ub04c\uc5b4\ub0b4\ub294 \ub370 \uc788\uc5b4, \ubc29\ub300\ud55c \uc591\uc758 \ubbf8\uc138\uc870\uc815 \ub370\uc774\ud130\ubcf4\ub2e4 \uc778\uac04 \uc804\ubb38\uac00\uac00 \uace0\ub978 \ud575\uc2ec \ub370\uc774\ud130\uac00 \ud6a8\uacfc\uc801\uc77c \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.</p> <p>\ub9c8\uc9c0\ub9c9\uc73c\ub85c, Decoder-Only Transformer \ucd5c\uc801\ud654 \uae30\ubc95\ub4e4\uc744 \uc815\ub9ac\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <ul> <li>\uc591\uc790\ud654(Quantization): 16-bit \ub300\uc2e0 8-bit, 4-bit\ub85c \ubaa8\ub378 \uac00\uc911\uce58\ub97c \ud45c\ud604\ud574 \uba54\ubaa8\ub9ac \uac10\uc18c (\uc608: QLoRA\uc758 4-bit NF (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)L205\u3011. \uc801\uc808\ud55c \uc591\uc790\ud654\ub294 \uc131\ub2a5 \uc720\uc9c0\ud558\uba74\uc11c \uba54\ubaa8\ub9ac 4\ubc30 \uc808\uc57d \uac00\ub2a5.</li> <li>\ud30c\ub77c\ubbf8\ud130 \ud6a8\uc728 \uae30\ubc95(PEFT): (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)L210\u3011, Adaptor, Prefix-Tuning \ub4f1\uc73c\ub85c \uc18c\uc218\uc758 \ud30c\ub77c\ubbf8\ud130\ub9cc \ud559\uc2b5\ud558\uc5ec \uc5f0\uc0b0/\uba54\ubaa8\ub9ac \ud6a8\uc728 \uac1c\uc120.</li> <li>Flash Attention \ub4f1 \uba54\ubaa8\ub9ac \ud6a8\uc728 Attention: \uc2dc\ud000\uc2a4 \uae38\uc774\uac00 \uae38\uc5b4\uc9c8 \ub54c \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc744 \uc904\uc774\uace0 \uc18d\ub3c4\ub97c \ub192\uc774\ub294 \ucd5c\uc801\ud654 Attention \uc54c\uace0\ub9ac\uc998. PyTorch 2.x\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c SDPA(Scaled Dot-Product Attention)\uac00 \uae30\ubcf8 \ud1b5\ud569\ub418\uc5b4 \uc788\uc5b4 \uc131\ub2a5 \ud5a5 (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L92\u3011.</li> <li>Gradient Checkpointing: \uc911\uac04 \ud65c\uc131\uac12\uc744 \uc800\uc7a5\ud558\uc9c0 \uc54a\uace0 \uc7ac\uacc4\uc0b0\ud558\ub294 \uae30\ubc95\uc73c\ub85c, GPU \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc744 \ud070 \ud3ed\uc73c\ub85c \uc808\uac10 (\ub300\uc2e0 \uacc4\uc0b0\ub7c9 \uc99d\uac00). \ub300\ud615 \ubaa8\ub378 \ud30c\uc778\ud29c\ub2dd\uc5d0 \uac70\uc758 \ud544\uc218\uc801\uc73c\ub85c \uc4f0\uc785\ub2c8\ub2e4.</li> <li>Mixed Precision Training: FP32 \ub300\uc2e0 FP16/BF16 \ub4f1\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5f0\uc0b0 \uc18d\ub3c4\uc640 \uba54\ubaa8\ub9ac \uc0ac\uc6a9 \ucd5c\uc801\ud654. \ucd5c\uadfc GPU\ub294 BF16/FP16 \uc131\ub2a5\uc774 \ub6f0\uc5b4\ub098\ubbc0\ub85c, \uc815\ud655\ub3c4\uc5d0 \ud070 \ubb38\uc81c\uc5c6\uc774 \ud65c\uc6a9.</li> <li>\ubd84\uc0b0 \ubcd1\ub82c\ud654: \ubaa8\ub378 \ubcd1\ub82c\ud654(\ub808\uc774\uc5b4\ub97c \uc5ec\ub7ec GPU\uc5d0 \ubd84\ud560), \ub370\uc774\ud130 \ubcd1\ub82c\ud654, \ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654 \ub4f1 \uc870\ud569\uc73c\ub85c \ud558\ub4dc\uc6e8\uc5b4 \uc790\uc6d0 \ud65c\uc6a9 \uadf9\ub300\ud654. DeepSpeed, FSDP, Megatron-LM \ub4f1\uc774 \uc9c0\uc6d0.</li> <li>\ub3d9\uc801 \uc7a5\ube44 \uba54\ubaa8\ub9ac \ud65c\uc6a9: GPU\uc640 CPU, \ub514\uc2a4\ud06c\ub97c \ubaa8\ub450 \ud65c\uc6a9\ud558\uc5ec \uacc4\uc0b0 \uc790\uc6d0 \ub300\ube44 \ucd5c\ub300 \uba54\ubaa8\ub9ac \ud65c\uc6a9 (ZeRO-Offload/Infinit (ZeRO-Offload - DeepSpeed)L109\u3011.</li> <li>\ucd5c\uc2e0 \uc635\ud2f0\ub9c8\uc774\uc800 \uc0ac\uc6a9: AdamW \uc678\uc5d0 LAMB, Lion \ub4f1\uc758 \ub300\uc548 \uc635\ud2f0\ub9c8\uc774\uc800\ub098, DeepSpeed\uc758 One-bit Adam\ucc98\ub7fc \ud1b5\uc2e0\ub7c9\uc744 \uc904\uc778 \ubd84\uc0b0 \uc635\ud2f0\ub9c8\uc774\uc800\ub85c \ud6a8\uc728 \uac1c\uc120.</li> <li>\uc815\uaddc\ud654 \ubc0f \uc548\uc815\ud654 \uae30\ubc95: \ub300\uaddc\ubaa8 LM \ud30c\uc778\ud29c\ub2dd \uc2dc \ub7ec\ub2dd\ub808\uc774\ud2b8 \uc6cc\ubc0d\uc5c5, \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904, Gradient Clipping \ub4f1\uc73c\ub85c \uc548\uc815\uc801 \uc218\ub834\uc744 \ub3c4\ubaa8. \uc774\ub294 \uac04\uc811\uc801\uc73c\ub85c \ud6a8\uc728(\uc7ac\uc2dc\ub3c4 \uac10\uc18c \ub4f1)\uc5d0 \uae30\uc5ec.</li> <li>Continuous Pretraining\uacfc SFT \uacb0\ud569: \uacbd\uc6b0\uc5d0 \ub530\ub77c \uc0ac\uc804\ud559\uc2b5 \uc5f0\uc7a5(Continued Pretraining) \ud6c4 SFT\ub97c \ud558\uba74 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \uc5bb\uac70\ub098, SFT \ub3c4\uc911 \uae30\uc874 \uc9c0\uc2dd \uc720\uc9c0\ub97c \uc704\ud55c \u6df7\u5408 \uc0ac\uc804\ud559\uc2b5 \ub370\uc774\ud130 \uc0ac\uc6a9 \ub4f1\uc758 \uae30\ubc95\ub3c4 \uc5f0\uad6c\ub418\uace0 (Fine-tuning Guide | Unsloth Documentation)L174\u3011.</li> </ul>"},{"location":"tuning_techniques/%08finetuning_frameworks/#_2","title":"\ucd5c\uc2e0 \uc5f0\uad6c \ub3d9\ud5a5 \ubc0f \uacb0\ub860","text":"<p>\ucd5c\uadfc 2\ub144\uac04 LLM \ud30c\uc778\ud29c\ub2dd \ubd84\uc57c\ub294 \u201c\ub354 \uc801\uc740 \uc790\uc6d0\uc73c\ub85c \ub354 \ud070 \ubaa8\ub378\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ub2e4\ub8e8\ub294 \ubc95\u201d\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc654\uc2b5\ub2c8\ub2e4. QL ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs)-L18\u3011\uc758 \ub4f1\uc7a5\uc73c\ub85c \ucd09\ubc1c\ub41c \uc800\ube44\ud2b8 \uc591\uc790\ud654 + \uc5b4\ub311\ud130 \ud559\uc2b5 \ud328\ub7ec\ub2e4\uc784\uc740 \ud604\uc7ac \uc5c5\uacc4 \ud45c\uc900\uc73c\ub85c \uc790\ub9ac\uc7a1\uc558\uace0, \uc774\ub97c \ub118\uc5b4 \uc544\uc9c1 \uc2e4\ud5d8 \ub2e8\uacc4\uc778 3\ube44\ud2b8, 2\ube44\ud2b8 \ubbf8\uc138\ud29c\ub2dd \uc5f0\uad6c\ub3c4 \uc9c4\ud589\uc911\uc785\ub2c8\ub2e4. \ub610\ud55c LORA\uc758 \ubcc0\ud615\uc73c\ub85c\uc11c \uc911\uc694\ub3c4\uac00 \ub192\uc740 \ub808\uc774\uc5b4\uc5d0 \uac00\uc911\uce58\ub97c \ub354 \ud560\ub2f9\ud558\ub294 AdaLoRA \ub4f1\uc758 \uae30\ubc95\ub3c4 \uc81c\uc548\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud55c\ud3b8, \ud30c\uc778\ud29c\ub2dd \ub370\uc774\ud130 \ud655\ubcf4 \uce21\uba74\uc5d0\uc11c\ub294 Stanford\uc758 Alpaca \ud504\ub85c\uc81d\ud2b8\ucc98\ub7fc \uae30\uc874 \ubaa8\ub378(\uc608: GPT-3)\ub97c \uc774\uc6a9\ud55c Self-Instruct \ub370\uc774\ud130 \uc0dd\uc131\uc774 \uc720\ud589\ud558\uc5ec, \ube44\uad50\uc801 \uc800\ub834\ud558\uac8c \uc9c0\ub3c4\ud559\uc2b5 \ub370\uc774\ud130\ub97c \ubaa8\uc73c\ub294 \ud750\ub984\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ud0c4\uc0dd\ud55c Vicuna, WizardLM, OpenAssistant \ub4f1\uc758 \uc624\ud508\uc18c\uc2a4 \ub300\ud654\ud615 \ubaa8\ub378\ub4e4\uc740 \ubaa8\ub450 \uacf5\uac1c \ub370\uc774\ud130\ub098 \uc0dd\uc131 \ub370\uc774\ud130\ub85c SFT\ub41c \uc0ac\ub840\ub4e4\uc785\ub2c8\ub2e4. \uc131\ub2a5 \uba74\uc5d0\uc11c, \uc55e\uc11c \uc5b8\uae09\ud55c LIMA (Less is More for Alignme (Paper page - LIMA: Less Is More for Alignment - Hugging Face)-L18\u3011 \uc5f0\uad6c\ub294 \uace0\ud488\uc9c8 \uc18c\uaddc\ubaa8 \ub370\uc774\ud130\uc758 \uc704\ub825\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uace0, OpenAI\ub3c4 InstructGPT \ub17c\ubb38(2022)\uc5d0\uc11c \uc778\uac04 \ud53c\ub4dc\ubc31 \uc678\uc5d0 \ucd08\uae30 \ub2e8\uacc4\uc758 \uc288\ud37c\ubc14\uc774\uc988\ub4dc \ud30c\uc778\ud29c\ub2dd(SFT)\uc774 \ud575\uc2ec\uc801\uc73c\ub85c \uc911\uc694\ud568\uc744 \ubc1d\ud78c \ubc14 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uadfc\uc5d0\ub294 RLHF(\uac15\ud654\ud559\uc2b5 \ud734\uba3c \ud53c\ub4dc\ubc31) \ub300\uc2e0 DPO(Direct Preference Optimization)\ub098 RLAIF(AI \ud53c\ub4dc\ubc31) \ub4f1 \uc21c\uc218 \uc9c0\ub3c4 \uc2e0\ud638\ub9cc\uc73c\ub85c \uc120\ud638\ub3c4\ub97c \ud559\uc2b5\ud558\ub824\ub294 \uc2dc\ub3c4\ub3c4 \ub098\uc624\uace0 \uc788\uc5b4, \uc9c0\ub3c4 \ud30c\uc778\ud29c\ub2dd\uc758 \ubc94\uc704\uac00 \ud655\uc7a5\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc815\ub9ac\ud558\uba74, Decoder-Only LLM\uc758 \uc9c0\ub3c4 \ud30c\uc778\ud29c\ub2dd\uc740 \uc5ec\uc804\ud788 \ubaa8\ub378 \uc131\ub2a5 \uac1c\uc120\uacfc \ud6a8\uc728\uc801 \ud559\uc2b5\uc744 \uc591\ub9bd\ud558\uae30 \uc704\ud55c \ub2e4\uc591\ud55c \uc5f0\uad6c\ub85c \ud65c\ubc1c\ud788 \uc9c4\ud654\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. Hugging Face, DeepSpeed, Unsloth\uc640 \uac19\uc740 \ub3c4\uad6c\ub4e4\uc740 \uc774\ub7ec\ud55c \uc5f0\uad6c \uc131\uacfc\ub97c \ud604\uc5c5\uc5d0 \uc801\uc6a9\ud558\ub294 \ub2e4\ub9ac \uc5ed\ud560\uc744 \ud558\uba70, \uac01\uae30 \uc0ac\uc6a9\uc790 \uc694\uad6c\uc640 \ud658\uacbd\uc5d0 \ub9de\ub294 \uc194\ub8e8\uc158\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc2e4\ubb34\uc5d0\uc11c\ub294 \uc138 \uac00\uc9c0 \uc811\uadfc\ubc95\uc744 \uc0c1\ud669\uc5d0 \ub530\ub77c \uc870\ud569\ud558\uae30\ub3c4 \ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc911\uac04 \uaddc\ubaa8 \ubaa8\ub378\uc740 Unsloth\ub85c \uc2f1\uae00 GPU \ube60\ub974\uac8c \ud29c\ub2dd\ud558\uace0, \ucd08\uac70\ub300 \ubaa8\ub378\uc740 DeepSpeed\ub85c \uba40\ud2f0 GPU \ubd84\uc0b0 \ud559\uc2b5\ud558\uba70, \uc804\ubc18\uc801\uc778 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub294 Hugging Face \uc5d0\ucf54\uc2dc\uc2a4\ud15c\uc73c\ub85c \uad00\ub9ac\ud558\ub294 \uc2dd\uc785\ub2c8\ub2e4. \uc911\uc694\ud55c \uac83\uc740 \ubaa8\ub378\uc758 \ubaa9\ud45c\uc640 \uc81c\uc57d\uc5d0 \ub9de\ucdb0 \ucd5c\uc801\uc758 \uae30\ubc95\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc55e\uc73c\ub85c\ub3c4 \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc54c\uace0\ub9ac\uc998 \uce21\uba74\uc758 \ubc1c\uc804\uc73c\ub85c LLM \ud30c\uc778\ud29c\ub2dd\uc740 \ub354\uc6b1 \ucd5c\uc801\ud654\ub418\uace0 \ub300\uc911\ud654\ub420 \uac83\uc774\uba70, \u201c\ub354 \ub0ae\uc740 \ube44\uc6a9\uc73c\ub85c \ub354 \ub611\ub611\ud55c \ubaa8\ub378\u201d\uc744 \ub9cc\ub4dc\ub294 \ubc29\ud5a5\uc73c\ub85c \ub098\uc544\uac08 \uac83\uc785\ub2c8\ub2e4.</p> <p>\ucc38\uace0 \ubb38\ud5cc \ubc0f \ub9c1\ud06c: \ucd5c\uc2e0 \ud30c\uc778\ud29c\ub2dd \uae30\ubc95\uacfc \uc0ac\ub840\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Hugging Face \ube14\ub85c\uadf8 \ubc0f \uac01 \ub17c\ubb38\uc758 \uc6d0\ubb38\uc744 \ucc38\uace0\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. \uc544\ub798\ub294 \ubcf8 \ubb38\uc11c\uc5d0\uc11c \uc5b8\uae09\ub41c \uc790\ub8cc\ub4e4\uc758 \ucd9c\ucc98\uc785\ub2c8\ub2e4.</p> <ul> <li>Hugging Face \ube14\ub85c\uadf8: *Making LLMs even more accessible with 4-bit quantization and Q (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA) (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA)L222\u3011</li> <li>Hugging Face \ube14\ub85c\uadf8: *Make LLM fine-tuning 2x faster with Unsloth and (Make LLM Fine-tuning 2x faster with Unsloth and  TRL) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L80\u3011</li> <li>Hugging Face Docs: *DeepSpeed &amp; Accelerate Integration G (DeepSpeed) (ZeRO-Offload - DeepSpeed)L109\u3011</li> <li>Unsloth \uacf5\uc2dd \ubb38 (Fine-tuning Guide | Unsloth Documentation) (Make LLM Fine-tuning 2x faster with Unsloth and  TRL)-L77\u3011</li> <li>QLoRA \ub17c\ubb38 (Dettmers et al.,  ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs) ([2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs)-L27\u3011</li> <li>LIMA \ub17c\ubb38 (Zhou et al.,  (Paper page - LIMA: Less Is More for Alignment - Hugging Face)-L18\u3011</li> </ul>"},{"location":"tuning_techniques/continual_pretraining/","title":"Continued Pre-Training (CPT)","text":"<p>Countinuous, Continual, Continued \ub2e4\uc591\ud558\uac8c \ubd88\ub9ac\ub294  Pre-Training \uae30\ubc95\uc785\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/continual_pretraining/#cpt","title":"CPT \uac1c\uc694","text":"<p>Continued Pre-Training(CPT)\uc740 \uc774\ubbf8 Pre-trained \ub41c \ubaa8\ub378, \uadf8\ub807\uc9c0\ub9cc Instruction tuining \ub418\uae30\uc804 \uc0c1\ud0dc\uc758 Base Model\uc5d0 \ub370\uc774\ud130\ub97c \ucd94\uac00 \ud559\uc2b5\uc2dc\ud0a4\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4.</p> <ul> <li>\ubaa9\uc801: \ubaa8\ub378\uc744 \uc0c8\ub85c\uc6b4 \ub3c4\uba54\uc778, \uc5b8\uc5b4, \uc9c0\uc2dd\uc73c\ub85c \ud655\uc7a5 \uc2dc\ud0b5\ub2c8\ub2e4.  </li> <li>\ubc29\uc2dd: \uae30\uc874 \uc0ac\uc804\ud559\uc2b5\uacfc \ub3d9\uc77c\ud55c \ubb38\uc81c \uc815\uc758 (Next Token Prediction)\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc, \ud2b9\uc815 \ub3c4\uba54\uc778\uc774\ub098 \uc5b8\uc5b4\uc5d0 \uc9d1\uc911\ud558\uc5ec \ub370\uc774\ud130\ub97c \ubaa8\uc544\ub450\ub294 \uac83\uc774 \uc77c\ubc18\uc801\uc785\ub2c8\ub2e4. </li> </ul> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac \uc65c Instruction Model \uc774 \uc544\ub2cc Base Model \uc5d0 \ub370\uc774\ud130\ub97c \ucd94\uac00 \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac78\uae4c\uc694? Instruction Model \uc5d0 corpus \ub97c \ucd94\uac00\ud558\ub2e4\ubcf4\uba74, Insturction Following \ub2a5\ub825\uc744 \ud3ec\ud568\ud574 \ubaa8\ub378\uc774 \ub9ce\uc774 \ub9dd\uac00\uc9d1\ub2c8\ub2e4 (Catastrophic Forgetting)"},{"location":"tuning_techniques/continual_pretraining/#pre-train-cpt","title":"\uadf8\ub0e5 Pre-Train\uacfc CPT\uc758 \ucc28\uc774\uc810","text":"<ul> <li>\uc2dc\uc791\uc810: Random init \ub41c \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\ub290\ub0d0, \uc544\ub2c8\uba74 \uc774\ubbf8 \ud559\uc2b5\ub41c \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\ub290\ub0d0 \uac00 \ucc28\uc774\uc785\ub2c8\ub2e4. </li> <li>\ub370\uc774\ud130 \uaddc\ubaa8: \uc694\uc998 Pre-Train\uc740 \ubcf4\ud1b5  Trillon \ub2e8\uc704\uc758 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc, CPT\ub294 \uc77c\ubc18\uc801\uc73c\ub85c Billion \ub2e8\uc704\uc758 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. </li> <li>\ubaa9\ud45c: \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \uc2b5\ub4dd\ud558\uba74\uc11c \uae30\uc874 \ub2a5\ub825\uc744 \uc720\uc9c0\ud558\ub294 \uade0\ud615\uc774 \uc911\uc694\ud569\ub2c8\ub2e4, \uc0dd\uac01\ubcf4\ub2e4 \uc5b4\ub824\uc6cc\uc694... \u3160\u3160</li> </ul>"},{"location":"tuning_techniques/continual_pretraining/#cpt_1","title":"\ub3c4\uba54\uc778 \uc801\uc751\uc744 \uc704\ud55c CPT","text":"<p>\ud2b9\uc815 \ub3c4\uba54\uc778\uc774\ub098 \uc5b8\uc5b4\uc5d0 \ubaa8\ub378\uc744 \ud2b9\ud654\uc2dc\ud0a4\uae30 \uc704\ud574 CPT\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/continual_pretraining/#cpt_2","title":"\ud2b9\uc815 \ub3c4\uba54\uc778 \ub370\uc774\ud130\ub85c CPT \uc131\uacf5 \uc0ac\ub840","text":"<ul> <li>\uae08\uc735 \ub3c4\uba54\uc778: Amazon\uc758 FinPythia\ub294 Pythia \ubaa8\ub378\uc744 \uae08\uc735 \ud14d\uc2a4\ud2b8\ub85c \ucd94\uac00 \ud559\uc2b5\uc2dc\ucf1c \uae08\uc735 NLP \uc791\uc5c5\uc5d0\uc11c 10% \uc131\ub2a5 \ud5a5\uc0c1<ul> <li>\ucc38\uace0: AWS Machine Learning Blog - Efficient continual pre-training LLMs for financial domains, \uc544\uc8fc \uc790\uc138\ud558\uac8c \uc798 \ub098\uc640\uc788\uc5b4\uc694, CPT \ud558\uc2e4 \ubd84\ub4e4\uc740 \uaf2d \ud55c\ubc88 \ubcf4\uc138\uc694. </li> </ul> </li> <li>\ucf54\ub4dc \ub3c4\uba54\uc778: Code Llama\ub294 LLaMA-2\ub97c 5\ucc9c\uc5b5 \ud1a0\ud070\uc758 \ucf54\ub4dc \ub370\uc774\ud130\ub85c \ucd94\uac00 \ud559\uc2b5\uc2dc\ucf1c \ucf54\ub529 \ub2a5\ub825 \uac15\ud654, \uc774 \uc678\uc5d0\ub3c4 \ucf54\ub4dc \ud2b9\ud654 \ubaa8\ub378\ub4e4\uc740 \ub108\ubb34 \ub9ce\uace0 \uc131\uacf5 \uc0ac\ub840\uac00 \ub9ce\uc544\uc11c \uc0dd\ub7b5\ud569\ub2c8\ub2e4.  </li> </ul>"},{"location":"tuning_techniques/continual_pretraining/#cpt_3","title":"\uc5b8\uc5b4 \ud655\uc7a5\uc744 \uc704\ud55c CPT (\ud55c\uad6d\uc5b4)","text":"<p>Upstage\uc758 Solar \ubc0f Solar Pro \ubaa8\ub378\uc740 \ud55c\uad6d\uc5b4 \ud655\uc7a5\uc744 \uc704\ud55c CPT\uc758 \uc131\uacf5\uc801\uc778 \uc0ac\ub840\uc785\ub2c8\ub2e4.</p> <p>\ucc38\uace0: Solar Pro \uc18c\uac1c</p> <ul> <li>Solar 10.7B (Solar Mini): Mistral 7B\ub97c \uae30\ubc18\uc73c\ub85c Depth Up-Scaling(DUS) \uae30\ubc95\uc744 \uc801\uc6a9\ud574 48\ub808\uc774\uc5b4(10.7B \ud30c\ub77c\ubbf8\ud130)\ub85c \ud655\uc7a5\ud55c \ud6c4, \ucd94\uac00 Corpus\ub85c CPT\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4, \uadf8\ub9ac\uace0 \uaf64\ub098 \uc88b\uc740 \ud55c\uad6d\uc5b4 \uc131\ub2a5\uc744 \ubcf4\uc600\uc8e0. </li> <li>Solar Pro (22B): Phi-3 Medium(14B)\uc744 \uae30\ubc18\uc73c\ub85c 22B \uaddc\ubaa8\ub85c \ud655\uc7a5\ud558\uace0, \"1 Trillion Token Club\"\uc774\ub77c \ubd88\ub9ac\ub294 \ub300\uaddc\ubaa8 \uc601\uc5b4\u00b7\ud55c\uad6d\uc5b4 \ub9d0\ubb49\uce58(\uc57d 1\uc870 \ud1a0\ud070)\ub85c CPT\ub97c \uc218\ud589\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc5d0\uc11c \ud55c\uad6d\uc5b4 \uc774\ud574\ub825\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ud55c\uad6d\uc5b4 CPT \uc801\uc6a9 \ubc29\uc2dd:</p> <ol> <li>\ud1a0\ud06c\ub098\uc774\uc800 \ud655\uc7a5: \ud55c\uad6d\uc5b4 \ub2e8\uc5b4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub3c4\ub85d \ud1a0\ud06c\ub098\uc774\uc800\ub97c \ud655\uc7a5\ud558\uac70\ub098 \uc7ac\ud559\uc2b5</li> <li>\ub300\uaddc\ubaa8 \ud55c\uad6d\uc5b4 \ub370\uc774\ud130: Pretrain \ud560 Corpus \uc5d0 \ud55c\uad6d\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \ub300\ub7c9 \ud3ec\ud568</li> <li>\ud55c\uad6d\uc5b4 instruction \ud30c\uc778\ud29c\ub2dd: \ud55c\uad6d\uc5b4 \uc9c8\uc758\uc751\ub2f5, \uc694\uc57d \ub4f1\uc758 \ud0dc\uc2a4\ud06c\uc5d0 \ub300\ud55c \ucd94\uac00 \ud559\uc2b5</li> </ol> <p>\uc131\ub2a5 \ud5a5\uc0c1:</p> <ul> <li>Solar 10.7B\ub294 CPT \uc774\ud6c4 \ub3d9\uae09 \ubaa8\ub378\ub4e4\uc744 \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \ub54c\ub85c\ub294 30B \uaddc\ubaa8 \ubaa8\ub378\uacfc \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131</li> <li>Solar Pro\ub294 HuggingFace Open LLM \ub9ac\ub354\ubcf4\ub4dc(70B \ubbf8\ub9cc \ubd80\ubb38)\uc5d0\uc11c 1\uc704\ub97c \uae30\ub85d\ud558\uba70, \ub2e8\uc77c GPU\ub85c \uad6c\ub3d9 \uac00\ub2a5\ud55c \ubaa8\ub378 \uc911 \ucd5c\uace0 \uc218\uc900\uc758 \uc9c0\ub2a5\uc744 \uc785\uc99d</li> </ul> <p>\uc774\ub7ec\ud55c CPT \uacfc\uc815\uc744 \ud1b5\ud574 Solar \uc2dc\ub9ac\uc988\ub294 \ud55c\uad6d\uc5b4\uc640 \uc601\uc5b4 \ubaa8\ub450\uc5d0\uc11c \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc774\uba70, \ud2b9\ud788 Solar Pro \uc815\uc2dd \ubc84\uc804\uc740 \ud55c\uad6d\uc5b4 \uc9c8\uc758\uc5d0 \uc720\ucc3d\ud558\uac8c \ub2f5\ubcc0\ud558\uace0 \ub2e4\uc591\ud55c \ud55c\uad6d\uc5b4 NLP \uacfc\uc81c\uc5d0\uc11c \uc6b0\uc218\ud55c \uc131\uacfc\ub97c \ubcf4\uc77c \uac83\uc73c\ub85c \uae30\ub300\ub429\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/continual_pretraining/#cpt_4","title":"CPT \ucd5c\uc801\ud654 \uae30\ubc95","text":"<p>\ud6a8\uc728\uc801\uc778 CPT\ub97c \uc704\ud55c \uc54c\ub824\uc9c4 \ub2e4\uc591\ud55c \uae30\uc220\uc801 \ud301\uc774 \uc788\uc2b5\ub2c8\ub2e4, \uc8fc\uc694 \ubaa9\ud45c\ub294 \uae30\uc874\uc5d0 \uc774\ubbf8 \ud559\uc2b5\ub41c pre-trained \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uae30\ub2a5\uc744 \ud655\uc7a5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/continual_pretraining/#_1","title":"\ud6a8\uacfc\uc801\uc778 \ud559\uc2b5\ub960 \ubc0f \ubc30\uce58 \ud06c\uae30 \uc120\ud0dd","text":"<ul> <li>\ub0ae\uc740 Learning Rate: \uc0ac\uc804\ud559\uc2b5\ubcf4\ub2e4 \ub0ae\uc740 \ud559\uc2b5\ub960 \uc0ac\uc6a9 (\ubaa8\ub378\uc774 \uc774\ubbf8 \uc88b\uc740 \uc601\uc5ed\uc5d0 \uc788\uae30 \ub54c\ubb38)</li> <li>\ubd84\ub9ac\ub41c Learning Rate: \uc784\ubca0\ub529 \ub808\uc774\uc5b4\uc640 \ucd9c\ub825 \ub808\uc774\uc5b4\uc5d0 \ub354 \uc791\uc740 \ud559\uc2b5\ub960 \uc801\uc6a9\ud558\uc5ec \uc548\uc815\uc131 \ud655\ubcf4</li> <li>Warmup \ub2e8\uacc4: \uc9e7\uc740 \uc6cc\ubc0d\uc5c5 \ub2e8\uacc4\ub97c \ud1b5\ud574 \ud559\uc2b5\ub960\uc744 \uc548\uc804\ud558\uac8c \ub192\uc774\ub294 \ubc29\ubc95 \ud65c\uc6a9</li> <li>\ub370\uc774\ud130 \ud63c\ud569: \uc0c8 \ub3c4\uba54\uc778 \ub370\uc774\ud130\uc640 \uc77c\ubd80 \uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \ud63c\ud569\ud558\uc5ec \uae30\uc874 \ub2a5\ub825 \uc720\uc9c0</li> </ul>"},{"location":"tuning_techniques/continual_pretraining/#_2","title":"\uc0ac\uc804\ud559\uc2b5 \ud14d\uc2a4\ud2b8 \uc120\ubcc4 \uae30\uc900","text":"<ul> <li>\ub3c4\uba54\uc778 \uad00\ub828\uc131: \ubaa9\ud45c \ub3c4\uba54\uc778\uacfc \uc9c1\uc811 \uad00\ub828\ub41c \uace0\ud488\uc9c8 \ud14d\uc2a4\ud2b8\ub97c \uc120\ud0dd\ud558\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4.</li> <li>\ub2e4\uc591\uc131: \ub3c4\uba54\uc778 \ub0b4 \ub2e4\uc591\ud55c \ud558\uc704 \uc8fc\uc81c\uc640 \uc2a4\ud0c0\uc77c\uc744 \ud3ec\ud568\ud558\ub294 \uade0\ud615 \uc7a1\ud78c \ucf54\ud37c\uc2a4\ub97c \uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4.</li> <li>\ucd5c\uc2e0\uc131: \ubaa8\ub378 \uc9c0\uc2dd \uc5c5\ub370\uc774\ud2b8\uac00 \ubaa9\uc801\uc778 \uacbd\uc6b0 \ucd5c\uc2e0 \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\ub294 \ud14d\uc2a4\ud2b8\ub97c \uc120\ud0dd\ud558\ub294 \uac83\uc774 \ud6a8\uacfc\uc801\uc785\ub2c8\ub2e4.</li> <li>\ud569\uc131 \ub370\uc774\ud130: \uc2e4\uc81c \ub370\uc774\ud130\uac00 \ubd80\uc871\ud55c \uacbd\uc6b0 \uc9c0\uc2dd \uadf8\ub798\ud504\ub098 \ub2e4\ub978 \ubaa8\ub378\uc744 \ud1b5\ud574 \ud569\uc131 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\uc5ec \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/continual_pretraining/#_3","title":"\uc801\uc6a9 \uc0ac\ub840","text":"<p>\uae30\uc5c5\uacfc \uc5f0\uad6c\uc18c\uc5d0\uc11c\ub294 \ub2e4\uc591\ud55c \ubaa9\uc801\uc73c\ub85c CPT\ub97c \ud65c\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\ub3c4\uba54\uc778 \uc9c0\uc2dd \ud2b9\ud654: Amazon\uc758 FinPythia, Bloomberg\uc758 BloombergGPT \ub4f1 \uae08\uc735 \ud2b9\ud654 \ubaa8\ub378\uc774 \uc88b\uc740 \uc608\uc2dc\uc785\ub2c8\ub2e4. </li> <li>\ucd5c\uc2e0 \uc815\ubcf4 \ubc18\uc601: \uc815\uae30\uc801\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uc6f9 \ub370\uc774\ud130\ub85c CPT\ub97c \uc218\ud589\ud558\uc5ec \ubaa8\ub378 \uc9c0\uc2dd\uc744 \uac31\uc2e0\ud569\ub2c8\ub2e4.</li> <li>\ub0b4\ubd80 \uc9c0\uc2dd \ud1b5\ud569: \uae30\uc5c5 \ub0b4\ubd80 \ubb38\uc11c, \ub9e4\ub274\uc5bc, \ud2b8\ub79c\uc2a4\ud06c\ub9bd\ud2b8 \ub4f1\uc73c\ub85c CPT\ub97c \uc218\ud589\ud558\uc5ec \uc870\uc9c1 \uc9c0\uc2dd\uc744 \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4.</li> <li>\uc9c0\uc18d\uc801 \ud559\uc2b5 \uc2dc\uc2a4\ud15c: \uc8fc\uae30\uc801\uc73c\ub85c \uc0c8 \ub370\uc774\ud130\ub97c \uc218\uc9d1\ud558\uace0 \ubaa8\ub378\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud569\ub2c8\ub2e4.</li> </ul> <p>\ucc38\uace0 \uc790\ub8cc:</p> <ul> <li>Unsloth - Continued LLM Pretraining</li> <li>AWS Machine Learning Blog - Efficient continual pre-training LLMs for financial domains</li> <li>Databricks Blog - Characterizing Datasets and Building Better Models with Continued Pre-Training</li> <li>RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining </li> </ul>"},{"location":"tuning_techniques/korean_tuning/","title":"\ud55c\uad6d\uc5b4 \ud655\uc7a5 \ud29c\ub2dd","text":""},{"location":"tuning_techniques/korean_tuning/#llm","title":"\ud55c\uad6d\uc5b4 LLM \ud604\ud669","text":"<ul> <li>\ud55c\uad6d\uc5b4 \ud2b9\ud654 \ubaa8\ub378 \uc18c\uac1c</li> <li>\uae30\uc874 \ub2e4\uad6d\uc5b4 \ubaa8\ub378\uc758 \ud55c\uad6d\uc5b4 \uc131\ub2a5</li> </ul>"},{"location":"tuning_techniques/korean_tuning/#cpt","title":"\ud55c\uad6d\uc5b4 CPT","text":"<ul> <li>\ud55c\uad6d\uc5b4 \ucf54\ud37c\uc2a4 \uad6c\ucd95</li> <li>\ud55c\uad6d\uc5b4 \ud1a0\ud06c\ub098\uc774\uc800 \ucd5c\uc801\ud654</li> <li>\ud55c\uad6d\uc5b4 CPT \uc804\ub7b5</li> </ul>"},{"location":"tuning_techniques/korean_tuning/#sft","title":"\ud55c\uad6d\uc5b4 SFT","text":"<ul> <li>\ud55c\uad6d\uc5b4 \uc9c0\uc2dc \ub370\uc774\ud130 \uc900\ube44</li> <li>\ubc88\uc5ed \ub370\uc774\ud130 vs. \uc6d0\uc5b4 \ub370\uc774\ud130</li> <li>\ud55c\uad6d\uc5b4 SFT \ucd5c\uc801\ud654 \ubc29\ubc95</li> </ul>"},{"location":"tuning_techniques/korean_tuning/#_2","title":"\ud55c\uad6d\uc5b4 \ud2b9\ud654 \ud3c9\uac00","text":"<ul> <li>\ud55c\uad6d\uc5b4 \ubca4\uce58\ub9c8\ud06c \uc18c\uac1c</li> <li>\ud3c9\uac00 \uba54\ud2b8\ub9ad \uc124\uc815</li> <li>\ud55c-\uc601 \uc131\ub2a5 \uade0\ud615 \uc720\uc9c0 </li> </ul>"},{"location":"tuning_techniques/peft_methods/","title":"Parameter-Efficient Fine-Tuning (PEFT)","text":"<p>Parameter-Efficient Fine-Tuning (PEFT)\ub294 LLM\uc758 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uc9c0 \uc54a\uace0\ub3c4 \ubaa8\ub378\uc744 \ud29c\ub2dd\ud558\ub294 \ubc29\ubc95\uc744 \uce6d\ud569\ub2c8\ub2e4. \uc2e4\uc804\uc5d0\uc11c\ub294 LLM \uc744 Full Tuning \ud558\uae30\uac00 \ub108\ubb34 \ube44\uc2f8\uc694...... </p>"},{"location":"tuning_techniques/peft_methods/#_1","title":"\ud544\uc694\uc131","text":"<p>\ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \uc804\uccb4 \ud30c\ub77c\ubbf8\ud130\ub97c \ubbf8\uc138 \uc870\uc815(full fine-tuning)\ud558\ub294 \uac83\uc740 \ub9c9\ub300\ud55c \ucef4\ud4e8\ud305 \uc790\uc6d0\uc744 \ud544\uc694\ub85c \ud569\ub2c8\ub2e4.</p> <p>2025\ub144 3\uc6d4 \uae30\uc900, \uc5f0\uad6c\uc6a9\uc73c\ub85c \uac00\uc7a5 \ub9ce\uc774 \uc0ac\uc6a9\ub418\ub294 32B \ubaa8\ub378\uc744 Full Tuning \ud558\ub824\uba74 \ub300\ub7b5 300GB \uc758 GPU \uba54\ubaa8\ub9ac\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. (\uc5ec\ub7ec \ud14c\ud06c\ub2c9\uc744 \ud1b5\ud574 \uc904\uc774\ub294 \uac83\uc774 \ub2f9\uc5f0\ud788 \uac00\ub2a5\ud558\uae34 \ud569\ub2c8\ub2e4)</p> <p>300GB VRAM \ud655\ubcf4\ud558\ub824\uba74, H100 (80GB) 4\uc7a5... \uad6c\uc785\ud558\ub824\uba74 2~3\uc5b5\uc740 \ud560 \uac83 \uac19\ub124\uc694. \uc800\ub3c4 \uadf8\ub807\uace0 \ub9ce\uc774\ub4e4 \ub300\uc5ec\ud574\uc11c \uc0ac\uc6a9\ud558\ub294\ub370\uc694 \uadf8\ub798\ub3c4 \ube44\uc309\ub2c8\ub2e4.   </p> \ubc29\ubc95 \uc815\ubc00\ub3c4 7B 13B 30B 70B 110B Full 16\ube44\ud2b8 67GB 125GB 288GB 672GB 1056GB LoRA 16\ube44\ud2b8 15GB 28GB 63GB 146GB 229GB QLoRA 8\ube44\ud2b8 9GB 17GB 38GB 88GB 138GB QLoRA 4\ube44\ud2b8 5GB 9GB 20GB 46GB 72GB <p>BF16, AdamW \uae30\uc900 \ub300\ub7b5\uc801\uc778 \uacc4\uc0b0\uc785\ub2c8\ub2e4 VRAM \uc694\uad6c\uc0ac\ud56d \ucc38\uace0</p> <p>PEFT\ub294 \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc758 \uadf9\ud788 \uc77c\ubd80 \ud30c\ub77c\ubbf8\ud130\ub9cc \uc870\uc815\ud558\uac70\ub098 \uc801\uc740 \uc591\uc758 \uc2e0\uaddc \ud30c\ub77c\ubbf8\ud130\ub97c \ucd94\uac00 \ud559\uc2b5\ud558\ub294 \uc811\uadfc \uc785\ub2c8\ub2e4.  </p> <p>\ud559\uc2b5 \ube44\uc6a9\uc774 \uc808\uac10\ub418\ub294 \uac83 \ubfd0 \uc544\ub2c8\ub77c, \uc5ec\ub7ec \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \uc791\uc5c5\uc5d0 \ub300\ud574 \uae30\ubcf8 \ubaa8\ub378\uc740 \uacf5\uc720\ud558\uace0 \uc791\uc5c5\ubcc4 \uc5b4\ub311\ud130\ub9cc \uad50\uccb4\ud558\ub294 \ud6a8\uc728\uc801 \ubc30\ud3ec\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. </p> <p>\uc774\ub807\uac8c \ube44\uc6a9\uc744 \uc904\uc600\ub294\ub370 Full Tuning \ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc744\uae4c\uc694? \ud558\uba74. \uc74c \uadf8\ub7f4 \uc218\ub3c4 \uc788\uace0 \uc544\ub2d0 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ucc28\ucc28 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/peft_methods/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>LoRA\ub294 \uc0ac\uc804\ud6c8\ub828\ub41c \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \ub3d9\uacb0\ud55c \ucc44, low-rank decomposition\ub97c \ud1b5\ud574 \ud6a8\uc728\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4, \uc0ac\uc2e4\uc0c1 \ucd5c\uace0 \uc778\uae30 \uae30\ubc95\uc785\ub2c8\ub2e4. LLM \uc774 \uc544\ub2cc Diffusion Model \uc5d0\uc11c\ub3c4 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/peft_methods/#_2","title":"\uc791\ub3d9 \uc6d0\ub9ac","text":"<p>LoRA\uc758 \ud575\uc2ec \uc544\uc774\ub514\uc5b4\ub294 weight \uc5c5\ub370\uc774\ud2b8\ub97c low-rank \ud589\ub82c\uc758 \uacf1\uc73c\ub85c \uadfc\uc0ac\ud558\ub294 \uac83\uc785\ub2c8\ub2e4, \uc0ac\uc2e4 \ub2e8\uc21c\ud55c \uc120\ud615\ub300\uc218 \ubb38\uc81c\uc785\ub2c8\ub2e4.  </p> <ul> <li> <p>\uae30\uc874 Weight Matrix $$W \\in \\mathbb{R}^{d \\times k}$$</p> </li> <li> <p>LoRA \uc5c5\ub370\uc774\ud2b8  $$ \\Delta W = A \\times B $$  $$A \\in \\mathbb{R}^{d \\times r}$$ $$B \\in \\mathbb{R}^{r \\times k}$$ $$r \\ll \\min(d, k)$$</p> </li> <li> <p>\ucd5c\uc885 \uc801\uc6a9 $$W' = W + \\alpha \\cdot \\Delta W$$  (\uc5ec\uae30\uc11c alpha\ub294 \uc2a4\ucf00\uc77c\ub9c1 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc785\ub2c8\ub2e4.)</p> </li> </ul> <p>\uc774 \ubc29\uc2dd\uc740 \ud559\uc2b5 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130 \uc218\ub97c \ud06c\uac8c \uc904\uc785\ub2c8\ub2e4. \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\ud558\uae30 \ub098\ub984\uc778\ub370\uc694, 1% \ubbf8\ub9cc\uc758 weight\ub9cc \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \uac83\uc774 \uc77c\ubc18\uc801\uc778 \uac83 \uac19\uc544\uc694.</p> <p>\ucd9c\ucc98 - LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</p>"},{"location":"tuning_techniques/peft_methods/#_3","title":"\uc8fc\uc694 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130","text":"<p>Unlosth\uc758 LoRA \uc124\uc815 \ucf54\ub4dc\ub97c \ubc1c\ucdcc\ud588\uc2b5\ub2c8\ub2e4. </p> <pre><code>model = FastLanguageModel.get_peft_model(\n    model,\n\n    r=16,  # Choose any positive number! Recommended values include 8, 16, 32, 64, 128, etc.\n    # Rank parameter for LoRA. The smaller this value, the fewer parameters will be modified.\n\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n    # Specify the modules to which LoRA will be applied\n\n    lora_alpha=32,\n    # Alpha parameter for LoRA. This value determines the strength of the applied LoRA.\n\n    ...\n)\n</code></pre> <ul> <li>\ub7ad\ud06c(r): \uc800\ub7ad\ud06c \ubd84\ud574\uc758 \ucc28\uc6d0\uc73c\ub85c, \uc77c\ubc18\uc801\uc73c\ub85c 4, 8, 16 \ub4f1\uc758 \uac12 \uc0ac\uc6a9, \uc5bc\ub9c8\ub098 \ub9ce\uc740 parameter\ub97c \uc5c5\ub370\uc774\ud2b8 \ud560\uc9c0 \uacb0\uc815 \ud569\ub2c8\ub2e4, 16 \uc774\uc0c1\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4.  </li> <li>\ud0c0\uac9f \ubaa8\ub4c8: LoRA\ub97c \uc801\uc6a9\ud560 \ubaa8\ub378 \ub0b4 \ud2b9\uc815 \ubaa8\ub4c8 (\uc608: attention\uc758 query, key, value \ud589\ub82c) \ub4e4\uc744 \uacb0\uc815\ud569\ub2c8\ub2e4, \uc5c5\ub370\uc774\ud2b8\ud560 layer \ub4e4\uc744 \uc124\uc815\ud558\ub294 \uac83\uc774\uc8e0.</li> <li>\uc54c\ud30c(\u03b1): \uc5c5\ub370\uc774\ud2b8 \uc2a4\ucf00\uc77c\ub9c1 \ud329\ud130, \ubcf4\ud1b5 r\uc758 2\ubc30 \uc815\ub3c4\ub85c \uc124\uc815\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4, LoRA \uc758 \uac15\ub3c4\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/peft_methods/#_4","title":"\uc7a5\uc810","text":"<ul> <li>\uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \ud06c\uac8c \uc904\uc784, update \ub418\ub294 weight\ub294 1% \uc774\ud558 \uc218\uc900 </li> <li>\ucd94\ub860 \uc2dc \uc9c0\uc5f0 \uc2dc\uac04 \uc99d\uac00 \uc5c6\uc74c (LoRA \uac00\uc911\uce58\ub97c \uc6d0\ubcf8\uacfc merge \uac00\ub2a5)</li> <li>\ub2e4\uc591\ud55c \uc791\uc5c5\uc5d0 \ub300\ud574 \uae30\ubcf8 \ubaa8\ub378\uc740 \uacf5\uc720\ud558\uace0 \uc791\uc740 LoRA \uac00\uc911\uce58\ub9cc \uad50\uccb4 \uac00\ub2a5</li> <li>\ub9ce\uc740 \uacbd\uc6b0\uc5d0 full-tuning \uacfc \ube44\uc2b7\ud55c \uc131\ub2a5 (?!!!!) </li> </ul>"},{"location":"tuning_techniques/peft_methods/#lora-vs-full-tuning","title":"LoRA VS Full Tuning","text":"<p>LoRA \ub294 \ud30c\ub77c\ubbf8\ud130\uc758 \uc544\uc8fc \uc77c\ubd80\ubd84\ub9cc \uc5c5\ub370\uc774\ud2b8\ub97c \ud558\ub294\ub370... \uc815\ub9d0 \uad1c\ucc2e\uc744\uae4c\uc694? \uc774 \ubd80\ubd84\uc5d0 \uc758\ubb38\uc774 \ub9ce\uc774 \ub4dc\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4, \uc800\ub3c4 \uadf8\ub7ac\uace0\uc694. </p> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac &amp; \uacbd\ud5d8\ub2f4 \uc81c \uac1c\uc778\uc801\uc778 \uacbd\ud5d8\uc5d0 \uc758\ud558\uba74, Fine-Tuning\uc73c\ub85c \uc6b0\ub9ac\uac00 \ud558\uace0 \uc2f6\uc740 \uc77c\uc774 \ub300\ub2e8\ud55c \uc77c\uc774 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 LoRA \ucda9\ubd84\ud55c \uac83 \uac19\uc2b5\ub2c8\ub2e4. \ub9d0\ud22c\ub97c \uad50\uc815\ud558\uac70\ub098 \uc77c\ubd80\ubd84\uc758 \ub3c4\uba54\uc778 \uc9c0\uc2dd\uc744 \ucd94\uac00\ud558\uac70\ub098... \uc804\uccb4 LLM \uc774 \uac00\uc9c4 \ub2a5\ub825\uc5d0 \ube44\ud558\uba74 \uc544\uc8fc \ubbf8\ube44\ud55c \uc218\uc900\uc774\ub77c\uace0 \ub290\uaef4\uc9d1\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc544\uc8fc \ud070 \ucc28\uc6d0\uc758 \ud589\ub82c\uc5d0\uc11c \ubf51\uc544\ub0b8 dominant \ud55c \ud30c\ub77c\ubbf8\ud130\ub294 \uc22b\uc790\uc758 \uac2f\uc218\uc5d0 \ube44\ud574 \ud070 \uc601\ud5a5\ub825\uc744 \uac00\uc9c0\uae30\ub3c4 \ud558\uc8e0. \uc5b4\uca0c\ub4e0 \uc800\ub294 LoRA \ub85c \ud30c\uc778\ud29c\ub2dd\uc2dc, \ubaa9\uc801\uc744 \ub300\ubd80\ubd84 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. <p>\ub0a8\ub4e4\uc758 \uc8fc\uc7a5\ub4e4\ub3c4 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> <p>LoRA vs Full Tuning</p> <ul> <li>LoRA Learns Less and Forgets Less (24.05) \uc5d0 \ub530\ub974\uba74, \ub35c \ubc30\uc6b0\uace0 \ub35c \uc78a\ub294\ub2e4\uace0 \ud569\ub2c8\ub2e4, \ub2f9\uc5f0\ud788 \uadf8\ub7f4 \uac83 \uac19\uc2b5\ub2c8\ub2e4. </li> <li>Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2 (23.09) \uc5d0 \ub530\ub974\uba74, 95% vs 97% \uc758 \uc131\ub2a5 \ucc28\uc774\ub85c \uc2e4\uc0ac\uc6a9\uc5d0\uc11c \ubb38\uc81c\uac00 \ub418\uc9c0 \uc54a\ub294 \uc218\uc900\uc774\ub77c\uace0 \ud588\uc2b5\ub2c8\ub2e4. </li> <li>LoRA vs Full Fine-tuning: An Illusion of Equivalence (24.10) \uc5d0 \ub530\ub974\uba74, <ul> <li>LoRA\ub85c \uc5c5\ub370\uc774\ud2b8\ub41c \uac00\uc911\uce58 \ud589\ub82c\uc758 \ud2b9\uc774\uac12 \ubd84\ud3ec\ub97c \ubd84\uc11d\ud588\ub294\ub370, LoRA\uc758 \uacbd\uc6b0 \uae30\uc874 \uc0ac\uc804\ud559\uc2b5 \ud2b9\uc131 \uacf5\uac04\uc5d0 \uc5c6\ub358 \uc0c8\ub85c\uc6b4 \uace0\ucc28\uc6d0 \ud2b9\uc774\ubca1\ud130\ub4e4 (\"Intruder Dimension\")\uc774 \ub4f1\uc7a5\ud568\uc744 \ubc1c\uacac\ud588\uc2b5\ub2c8\ub2e4\u200b, \ubc18\uba74 Full Fine-tuning \ubaa8\ub378\uc740 \uc0ac\uc804\ud559\uc2b5\ub41c \ud2b9\uc131 \uacf5\uac04\uc744 \ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \uc720\uc9c0\ud588\ub2e4\uace0 \ud569\ub2c8\ub2e4\u200b</li> <li>rsLORA (rank-stabilized LoRA) \ub77c\ub294 \ud655\uc7a5 \uae30\ubc95\uc774 \uc788\ub294\ub370, \uc774 \uacbd\uc6b0\uc5d0\ub294 Full Tuning \uacfc \ube44\uc2b7\ud55c \ubd84\ud3ec\ub97c \ubcf4\uc778\ub2e4\uace0 \ud558\ub294 \uad70\uc694. </li> <li>\uc218\ud559\uc801 \ucc28\uc774\uc810\uc774 \uc774\uc81c \ubc1c\uacac\ub418\uc5b4 \uac00\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4, \uc694\uc57d\ud558\uc790\uba74 \uc131\ub2a5\uc774 \ube44\uc2b7\ud558\ub354\ub77c\ub3c4 \ubaa8\ub378\uc774 \uc774\ud574\ud558\ub294 \ub0b4\uc6a9\uc740 \ud45c\ud604\uc774 \ub2e4\ub974\ub2e4\ub294 \uac83\uc774\uace0\uc694, \uadf8\ub798\uc11c \ud2b9\uc815\ud55c \uc0c1\ud669\uc5d0\uc11c\ub294 \ucc28\uc774\ub97c \ubcf4\uc77c \uc218 \uc788\uaca0\ub124\uc694.</li> </ul> </li> </ul> <p>\uc694\uc57d\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\ubc94\uc6a9\uc801\uc778 SFT \uc5d0\uc11c\ub294 \ud070 \ucc28\uc774\uac00 \uc5c6\uc744 \uac83\uc785\ub2c8\ub2e4\ud654</li> <li>\ubcf5\uc7a1\ud55c \uacfc\uc81c (\uc218\ud559\uc774\ub098 \ucf54\ub529, \ud639\uc740 CPT\uc640 \uac19\uc740 \ud070 \ub3c4\uba54\uc778 \ubcc0\ud654) \uc5d0\uc11c\ub294 LoRA \uac00 \ubd88\ub9ac\ud55c \uacbd\uc6b0\uac00 \uaf64 \uc788\uc2b5\ub2c8\ub2e4. </li> <li>Fine Tuning \uc758 \ubd80\uc791\uc6a9 \uc911 \ud558\ub098\ub294 \uc6d0 \ubaa8\ub378\uc758 \uc9c0\uc2dd\uc774\ub098 \ub2a5\ub825\uc744 \uc78a\uc5b4\uba39\ub294 \uac83\uc778\ub370 (Catastrophic Forgetting), \uc774 \ubb38\uc81c\ub294 LoRA \uc5d0\uc11c\ub294 \ub35c \ubc1c\uc0dd\ud569\ub2c8\ub2e4. </li> </ul>"},{"location":"tuning_techniques/peft_methods/#qlora-quantized-lora","title":"QLoRA (Quantized LoRA)","text":"<p>QLoRA\ub294 LoRA\uc758 \ud655\uc7a5\uc73c\ub85c, 4\ube44\ud2b8 \uc591\uc790\ud654(quantization)\uc640 LoRA\ub97c \uacb0\ud569\ud558\uc5ec \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc744 \uadf9\ub300\ud654\ud55c \uae30\ubc95\uc785\ub2c8\ub2e4. Quantization \uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc740 \uc591\uc790\ud654 \ubb38\uc11c \ub97c \ucc38\uc870\ud558\uc138\uc694. </p>"},{"location":"tuning_techniques/peft_methods/#_5","title":"\uc791\ub3d9 \uc6d0\ub9ac","text":"<ol> <li>Pre-Trained \ubaa8\ub378 weights\ub97c 4\ube44\ud2b8 \uc815\ubc00\ub3c4\ub85c \uc591\uc790\ud654\ud558\uc5ec \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc785\ub2c8\ub2e4.</li> <li>\uc591\uc790\ud654\ub41c weights\ub294 \ub3d9\uacb0\ud558\uace0 LoRA \uc5b4\ub311\ud130\ub9cc \ud559\uc2b5\ud569\ub2c8\ub2e4. </li> <li>forward/backward \uacc4\uc0b0 \uc2dc \ud544\uc694\ud55c \ubd80\ubd84\ub9cc \ub2e4\uc2dc \uace0\uc815\ubc00\ub3c4 (ex. bf16) \uc73c\ub85c \uc62c\ub824\uc11c \uc5f0\uc0b0 \uc218\ud589\ud569\ub2c8\ub2e4. </li> </ol> <p>QLoRA\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ud601\uc2e0\uc801 \uae30\uc220\uc744 \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4:</p> <ul> <li>4\ube44\ud2b8 NormalFloat (NF4): \uc815\uaddc \ubd84\ud3ec\ub41c \uac00\uc911\uce58\uc5d0 \ucd5c\uc801\ud654\ub41c \ucee4\uc2a4\ud140 4\ube44\ud2b8 \ub370\uc774\ud130 \ud0c0\uc785</li> <li>Double Quantization: \uc591\uc790\ud654 \uc0c1\uc218\ub3c4 \ub2e4\uc2dc \uc591\uc790\ud654\ud558\uc5ec \ucd94\uac00 \uba54\ubaa8\ub9ac \uc808\uc57d</li> <li>Paged Optimizers: \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c CPU\uc640 GPU \uc0ac\uc774\uc5d0 \ud6a8\uc728\uc801\uc73c\ub85c \uad00\ub9ac</li> </ul>"},{"location":"tuning_techniques/peft_methods/#_6","title":"\uc7a5\uc810","text":"<ul> <li>\uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 FP16 \ub300\ube44 \ucd5c\ub300 4\ubc30 \uc774\uc0c1 \uc808\uac10\ud574\uc11c, 70B \uaddc\ubaa8 \ubaa8\ub378\ub3c4 \ub2e8\uc77c 48GB GPU\uc5d0\uc11c \ubbf8\uc138 \uc870\uc815 \uac00\ub2a5\ud558\ub2e4\uace0 \ud569\ub2c8\ub2e4.</li> <li>QLoRA\uc5d0 \ub530\ub974\uba74 Guanaco \uc2dc\ub9ac\uc988\uc5d0\uc11c 16bit \uae30\ubc18 fine-tuned \ubaa8\ub378\uacfc \uc131\ub2a5\uc744 \ub611\uac19\uc774 \ubcf4\uc600\ub2e4\uace0 \ud569\ub2c8\ub2e4. </li> </ul> <p>\uc131\ub2a5 \uc800\ud558 \ubd80\ubd84\uc5d0 \ub300\ud574\uc11c\ub294 \ub17c\ub780\uc758 \uc5ec\uc9c0\uac00 \ub9ce\uc740\ub370\uc694... \ub2f9\uc5f0\ud788 \uc5b8\uc81c\ub098 \ube44\uc2b7\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0 \ubabb\ud569\ub2c8\ub2e4. \uc704 \ub17c\ubb38\uc758 \uc8fc\uc7a5\uc740 \ubca0\uc2a4\ud2b8 \ucf00\uc774\uc2a4\uc8e0. \uc81c \uacbd\ud5d8\uc0c1 QLoRA \ub2f9\uc5f0\ud788 LoRA \ubcf4\ub2e4 \uc548 \uc88b\uc740 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \ube60\ub974\uac8c \uac00\ub2a5\uc131\uc744 \ud14c\uc2a4\ud2b8\ud574\ubcf4\uace0 \uc2f6\uc744 \ub54c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.  </p>"},{"location":"tuning_techniques/peft_methods/#qlora","title":"QLoRA \uc801\uc6a9 \uc608\uc2dc","text":"<pre><code>import torch\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4\ube44\ud2b8 \uc591\uc790\ud654 \uc124\uc815\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                   # 4\ube44\ud2b8 \ub85c\ub529 \ud65c\uc131\ud654\n    bnb_4bit_quant_type=\"nf4\",           # NF4 \uc591\uc790\ud654 \uc0ac\uc6a9\n    bnb_4bit_use_double_quant=True,      # \uc774\uc911 \uc591\uc790\ud654 \ud65c\uc131\ud654\n    bnb_4bit_compute_dtype=torch.bfloat16  # \uc5f0\uc0b0 \uc2dc BF16 \uc0ac\uc6a9\n)\n\n# \uc591\uc790\ud654\ub41c \ubaa8\ub378 \ub85c\ub4dc\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# LoRA \uad6c\uc131 \ubc0f \uc801\uc6a9\nfrom peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"])\nqlora_model = get_peft_model(model_4bit, lora_config)\n</code></pre>"},{"location":"tuning_techniques/peft_methods/#peft","title":"\uae30\ud0c0 PEFT \uae30\ubc95","text":"<ul> <li>\uc5b4\ub311\ud130(Adapter): \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uac01 \uce35\uc5d0 \uc791\uc740 \uc2e0\uacbd\ub9dd \ubaa8\ub4c8\uc744 \uc0bd\uc785\ud558\uc5ec \ud559\uc2b5</li> <li>\ud504\ub9ac\ud53d\uc2a4 \ud29c\ub2dd(Prefix Tuning): \uac01 \uce35\uc5d0 \ud6c8\ub828 \uac00\ub2a5\ud55c \uac00\uc9dc \ud1a0\ud070 \ubca1\ud130(prefix)\ub97c \ucd94\uac00</li> <li>\ud504\ub86c\ud504\ud2b8 \ud29c\ub2dd(Prompt Tuning): \uc785\ub825 \uc784\ubca0\ub529 \uacf5\uac04\uc5d0\uc11c \ud559\uc2b5 \uac00\ub2a5\ud55c \uc18c\ud504\ud2b8 \ud504\ub86c\ud504\ud2b8 \ucd94\uac00</li> <li>BitFit: \ubaa8\ub378\uc758 \ubc14\uc774\uc5b4\uc2a4 \ud30c\ub77c\ubbf8\ud130\ub9cc \uc5c5\ub370\uc774\ud2b8</li> </ul> <p>PEFT Methods \uc5d0 \ub610 \ub2e4\ub978 \ub9ce\uc740 \ubc29\ubc95\ub4e4\uc774 \uc788\uc73c\ub2c8 \ucc38\uc870\ud558\uc2dc\uba74 \ub418\uaca0\uc2b5\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/peft_methods/#_7","title":"\uacb0\ub860","text":"<p>\uc774 \uae00\uc744 \ubcf4\uc2dc\ub294 \ubd84\ub4e4\uc774 \uac00\uc7a5 \ud544\uc694\ud55c \uac83\uc740 \"\uadf8\ub798\uc11c \ubb58 \uc368\uc57c\ud574?\" \ub77c\ub294 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub300\ub2f5\uc774\uaca0\uc8e0.</p> <ol> <li>\ub098\ub294 GPU \uac00 \uc9c4\uc9dc\ub85c \ub109\ub109\ud558\ub2e4! (ex. H100x8)  -&gt; \uadf8\ub0e5 BF16\uc5d0 \ud480\ud29c\ub2dd \ud558\uc138\uc694.</li> <li>\ub098\ub294 GPU \uac00 \uc560\ub9e4\ud558\ub2e4.... -&gt; QLoRA \uc368\ubcf4\uc2dc\uace0, \uc798 \uc548\ub418\uba74 LoRA \uc368\ubcf4\uc138\uc694. \ud55c\uc815\ub41c \uba54\ubaa8\ub9ac\uc5d0\ub294 Quantization \ub41c \ud070 \ubaa8\ub378\uc744 \uc4f0\uc2dc\ub294 \uac78 \ucd94\ucc9c \ub4dc\ub9bd\ub2c8\ub2e4.</li> <li>\ucd94\ub860\uc73c\ub85c \uc801\uc6a9\ud560 \ud0c0\uac9f \ud558\ub4dc\uc6e8\uc5b4 \uba54\ubaa8\ub9ac\uac00 \ub108\ubb34 \ube61\ube61\ud558\ub2e4 -&gt; \uc5b4\ucc28\ud53c \ud070 \ubaa8\ub378 \ubabb \uc4f0\uc2e4\ud14c\ub2c8 LoRA \uc368\ubcf4\uc138\uc694.</li> </ol> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac \uc800\ub294 \ud558\ub4dc\uc6e8\uc5b4 \uc790\uc6d0\uc744 \ud070 \ubaa8\ub378\uc758 \ud480\ud29c\ub2dd\uc5d0 \ud22c\uc790\ud560 \uc2dc\uac04\uc5d0 \ub370\uc774\ud130\uc758 \ud488\uc9c8\uc744 \uc62c\ub9ac\ub294 \uac83\uc5d0 \ud22c\uc790\ud558\ub294 \uac83\uc774 \ud6e8\uc52c \ub354 \uc88b\uc740 \uc120\ud0dd\uc774\ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. \uc81c \uac1c\uc778\uc801 \uacbd\ud5d8\uc5d0 \uae30\uc778\ud55c \ucd94\ucc9c \uac00\uc774\ub4dc\uc785\ub2c8\ub2e4. \ucd5c\uadfc \ub525\uc2dc\ud06c\uac00 FP8 \ud63c\ud569 \ud6c8\ub828 \uae30\ubc95\uc744 \uc544\uc8fc \uc798 \uacf5\uac1c\ud588\ub294\ub370, \uc774 \ubc29\ubc95\uc774 \ub300\uc911\ud654\uac00 \ub41c\ub2e4\uba74 25\ub144\uc5d0\ub294 \uc800\ub3c4 \uc0dd\uac01\uc774 \ubc14\ub014\uc9c0\ub3c4 \ubaa8\ub974\uacd8\ub124\uc694. \uadf8\ub807\ub2e4\uba74 \uc774 \ubb38\uc11c\ub97c \uc5c5\ub370\uc774\ud2b8 \ud558\uaca0\uc2b5\ub2c8\ub2e4."},{"location":"tuning_techniques/peft_methods/#_8","title":"\ucc38\uace0 \ubb38\ud5cc","text":"<ol> <li>Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... &amp; Chen, W. (2021). LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</li> <li>Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023). QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314.</li> <li>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... &amp; Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning (pp. 2790-2799). PMLR.</li> <li>Li, X. L., &amp; Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.</li> <li>Lester, B., Al-Rfou, R., &amp; Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</li> <li>Rasley, J., Rajbhandari, S., Ruwase, O., &amp; He, Y. (2020). DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (pp. 3505-3506).</li> </ol>"},{"location":"tuning_techniques/preference_optimization/","title":"Direct Preference Optimization","text":""},{"location":"tuning_techniques/preference_optimization/#_1","title":"\uac1c\uc694","text":"<p>Preference Optimization (\uc120\ud638\ub3c4 \ucd5c\uc801\ud654)\ub294 LLM\uc744 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc640 \uac00\uce58\uc5d0 \ub9de\uac8c \uc870\uc815\ud558\ub294 \uae30\ubc95\ub4e4\uc744 \ub9d0\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubc29\ubc95\ub4e4\uc740 \ubaa8\ub378\uc774 \ub354 \uc720\uc6a9\ud558\uace0, \uc548\uc804\ud55c, \uadf8\ub9ac\uace0 \uc0ac\ub78c\ub4e4\uc774 \uc88b\uc544\ud560\ub9cc\ud55c \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \ub9cc\ub4dc\ub294 \ub370 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#alignment","title":"Alignment","text":"<p>Alignment \ub294 \ucd94\uc0c1\uc801\uc778 \uac1c\ub150\uc774\ub77c \uc124\uba85\ud558\uac8c\uac00 \uc870\uae08 \uc5b4\ub824\uc6b4\ub370\uc694. LLM \uc758 \ud559\uc2b5\uacfc\uc815\uc5d0\uc11c \uc608\uc2dc\ub97c \ud558\ub098 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> <p>(\ucc38\uace0) Base Model \uacfc Aligned LLM \uc740 Llama-3.1 \uc758 base \ubaa8\ub378\uacfc final \ubaa8\ub378\uc744 \uae30\uc900\uc73c\ub85c \uc9c1\uc811 \ucd94\ub860\uc2dc\ucf1c\ubcf8 \ub0b4\uc6a9\uc774\uace0\uc694, Instruct \ubaa8\ub378\uc758 output \uc740 \ud559\uc2b5 \uc911\uac04 \ub2e8\uacc4\ub77c\uc11c \uc6b0\ub9ac\uac00 \uc9c1\uc811 \ubcfc \uc218 \uc5c6\uae30 \ub584\ubb38\uc5d0 HuggingFace Zephyr\uc758 \uc608\uc2dc\uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4. </p> <p>Alignment \uc758 \ubaa9\uc801\uc740 \ubaa8\ub378\uc774 \uc0ac\ub78c\uc758 \uc758\ub3c4\uc5d0 \ub9de\uac8c \ud589\ub3d9\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uadf8\uac83\uc740 \ub0b4\uc6a9\uc801\uc778 \ubd80\ubd84\uc77c \uc218\ub3c4 \uc788\uace0, \ub9d0\ud22c\ub098 \uc2a4\ud0c0\uc77c \uc77c \uc218\ub3c4 \uc788\uace0, \uc5b4\ub5a4 \ucde8\ud5a5\uc77c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.   \uc704 \uc608\uc2dc\uc560\uc11c\ub294 \"\ud53c\uc790\uc5d0 \ud30c\uc778\uc560\ud50c\uc744 \uc62c\ub9ac\ub294 \uac83\" \uc5d0 \ub300\ud574 \uc911\ub9bd\uc801\uc778 \ub300\ub2f5 \ud639\uc740 \ud68c\ud53c\uc131 \ub300\ub2f5\uc744 \ud558\ub3c4\ub85d \ubaa8\ub378\uc744 align \uc2dc\ucf30\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc774\ud0c8\ub9ac\uc544 \uc0ac\ub78c\uc744 \ub300\uc0c1\uc73c\ub85c \ud55c\ub2e4\uba74, \uacf5\uaca9\uc801\uc73c\ub85c \"\ud53c\uc790\uc5d0 \ud30c\uc778\uc560\ud50c\uc740 \ubc94\uc8c4\uc57c!\" \ub77c\uace0 \ub300\ub2f5\ud558\uac8c \ub9cc\ub4e4 \uc218\ub3c4 \uc788\uace0, \ud558\uc640\uc774 \uc0ac\ub78c\uc774\ub77c\uba74 \"\ud53c\uc790\uc5d0 \ud30c\uc778\uc560\ud50c\uc740 \ub9db\uc788\uc5b4!\" \ub77c\uace0 \ub300\ub2f5\ud558\uac8c \ub9cc\ub4e4 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/preference_optimization/#rlhf-reinforcement-learning-from-human-feedback","title":"RLHF (Reinforcement Learning from Human Feedback)","text":"<p>RLHF\ub294 \uc778\uac04 \ud53c\ub4dc\ubc31\uc744 \ud65c\uc6a9\ud55c \uac15\ud654\ud559\uc2b5\uc73c\ub85c, \ub300\ud654\ud615 AI \ubaa8\ub378\uc744 \uc815\ub82c(alignment)\ud558\ub294 \uc804\ud1b5\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4, ChatGPT \ud0d1\uc7ac\ub41c GPT-3.5 ~ 4 \uc758 \ubaa8\ub378\uc774 \uac00\uc7a5 \ub611\ub611\ud588\uc5c8\ub358 \uc774\uc720\ub85c \ub9ce\uc774 \uc9c0\ubaa9 \ubc1b\uc558\uc2b5\ub2c8\ub2e4.  </p>"},{"location":"tuning_techniques/preference_optimization/#rlhf","title":"RLHF \ud30c\uc774\ud504\ub77c\uc778","text":"<ol> <li>Reward \ubaa8\ub378 \ud559\uc2b5 <ul> <li>\uc778\uac04 \uc120\ud638\ub3c4 \ube44\uad50 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud574 \uc751\ub2f5\uc758 \ud488\uc9c8\uc744 \ud3c9\uac00\ud558\ub294 reward \ubaa8\ub378\uc744 \ud6c8\ub828\ud569\ub2c8\ub2e4.</li> <li>OpenAI \uc758 \uacbd\uc6b0\uc5d0\ub294 ChatGPT \uc11c\ube44\uc2a4 \uc6b4\uc601\uc744 \ud1b5\ud574 \uc218\ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc758 \ud53c\ub4dc\ubc31 \ub370\uc774\ud130\uac00 \uc788\uc5c8\uae30 \ub584\ubb38\uc5d0 \ub2e4\ub978 \ud68c\uc0ac\ub4e4 \ubcf4\ub2e4 reward \uc744 \uc798 \ub9cc\ub4e4 \uc218 \uc788\uc5c8\uc744 \uac81\ub2c8\ub2e4. </li> <li>Reward \ubaa8\ub378 \uc5c6\uc774 \uc0ac\ub78c\uc774 \uc9c1\uc811 \ub2f5\ubcc0\uc5d0 \uc810\uc218\ub97c \ub9e4\uae38 \uc218\ub3c4 \uc788\uae34 \ud55c\ub370, scaling\uc774 \uc548\ub418\ub294 \ubb38\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> <li>\uac15\ud654\ud559\uc2b5(RL)<ul> <li>Reward \ubaa8\ub378\uc758 \ud53c\ub4dc\ubc31\uc744 \ucd5c\ub300\ud654\ud558\ub3c4\ub85d \uac15\ud654\ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \uadf8\ub7ec\uba74 \ubaa8\ub378\uc774 reward \ubaa8\ub378\uc774 \uc88b\uac8c \ud3c9\uac00\ud558\ub294 \ub300\ub2f5\uc744 \ub0b4\ub193\uac8c \ub418\uc8e0.</li> <li>\ubcf4\ub2e4 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 PPO (Proximal Policy Optimization) \uc54c\uace0\ub9ac\uc998\uc744 \ucc38\uace0\ud558\uc138\uc694. </li> <li>\uc774\uc0c1\uc801\uc73c\ub85c\ub294 \uc0ac\ub78c\ub4e4\uc774 \uc2eb\uc5b4\ud558\ub294 \uacf5\uaca9\uc801\uc778 \ub300\ub2f5\ub3c4 \uc548\ud558\uac8c \ub418\uace0, \uc0ac\ub78c\ub4e4\uc774 \uc88b\uc544\ud558\ub294 \uc720\uba38\ub3c4 \uc798 \ud558\uac8c \ub418\uc57c\uaca0\uc8e0.</li> </ul> </li> </ol>"},{"location":"tuning_techniques/preference_optimization/#rlhf_1","title":"RLHF\uc758 \ubb38\uc81c\uc810","text":"<p>Reward \ubaa8\ub378\uc774 \uc644\ubcbd\ud558\ub2e4\uba74, \uc798 \ub418\uc5b4\uc57c\ud560\ud150\ub370\uc694... \uba87\uac00\uc9c0 \ubb38\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4. </p> <ul> <li>Reward \ubaa8\ub378\uc774 \uc790\uafb8 \ub6ab\ub824\ubc84\ub9bd\ub2c8\ub2e4. Reward Hacking \uc774\ub77c\uace0 \ud558\ub294\ub370 \uac15\ud654\ud559\uc2b5 \ubd84\uc57c\uc758 \uc624\ub79c \ubb38\uc81c\uc785\ub2c8\ub2e4. \uc798\ubabb\ub41c \ub300\ub2f5\ub3c4 \ub192\uc740 \uc810\uc218\ub97c \ubc1b\uc744 \uc218\uac00 \uc788\uc5b4\uc11c \ud559\uc2b5\uc774 \uc798 \uc548\ub418\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4.</li> <li>\uc5f0\uc0b0\ub7c9\ub3c4 \ub9ce\uc774 \ud544\uc694\ud558\uace0\uc694, \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc758 \uc5b4\ub824\uc6c0\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. </li> <li>\uc2e4\uc81c\ub85c OpenAI \uc640 \uba87 \ube45\ud14c\ud06c \ud68c\uc0ac\ub4e4\uc744 \uc81c\uc678\ud558\uace0\ub294 RLHF \ub97c \uc131\uacf5\uc801\uc73c\ub85c \uc218\ud589\ud574\ub0b8 \uc0ac\ub840\uac00 \uac70\uc758 \uc5c6\uc2b5\ub2c8\ub2e4. </li> </ul> <p>OpenAI \ub0b4\ubd80\uc5d0 \uc544\uc8fc \uc88b\uc740 Reward \ubaa8\ub378\uc774 \uc788\ub2e4\ub294 \ucd94\uce21\uc774 \ub9ce\uc2b5\ub2c8\ub2e4. \uc544\ubb34\ub798\ub3c4 ChatGPT\uc758 \uc131\uacf5\uc774 \uc778\uac04 \ub370\uc774\ud130\ub97c \uc544\uc8fc \ub9ce\uc774 \uc218\uc9d1\ud560 \uc218 \uc788\uac8c \ud574\uc92c\uaca0\uc8e0. \ub2e4\ub978 \ud68c\uc0ac\ub4e4\uc774 \uc801\uc790\ub97c \ubcf4\ub354\ub77c\ub3c4 \uc11c\ube44\uc2a4\ub97c \uc131\uacf5\uc2dc\ud0a4\uace0 \uc2f6\uc740\ub370 \uc774\uc720\ub77c\uace0\ub3c4 \uc0dd\uac01\uc774 \ub418\ub124\uc694. </p>"},{"location":"tuning_techniques/preference_optimization/#dpo-direct-preference-optimization","title":"DPO (Direct Preference Optimization)","text":"<p>RLHF \uac00 \uadf8\ub807\uac8c \uc5b4\ub835\ub2e4\uba74! \ub300\uc548\uc774 \uc788\uc8e0. \ubc14\ub85c DPO \uc785\ub2c8\ub2e4.  DPO\ub294 2023\ub144 Rafailov \ub4f1\uc774 \uc81c\uc548\ud55c \ubc29\ubc95\uc73c\ub85c, RLHF\uc758 \ubcf5\uc7a1\ud55c \uacfc\uc815\uc744 \ub2e8\uc21c\ud654\ud558\uc5ec \uc778\uac04 \uc120\ud638\ub3c4\ub97c \uc9c1\uc811 \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4. DPO\ub294 Reward \ubaa8\ub378 training, \uc0d8\ud50c \uc0dd\uc131 \ubc0f \ud3c9\uac00 loop, \ubcf5\uc7a1\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd \ub4f1\uc744 \uc5c6\uc574\uc2b5\ub2c8\ub2e4\u200b. \uacb0\uacfc\uc801\uc73c\ub85c DPO\ub294 RLHF \ub300\ube44 \uad6c\ud604\uc774 \uac04\ub2e8\ud558\uace0 \uc548\uc815\uc801\uc785\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#_2","title":"\ud575\uc2ec \uc544\uc774\ub514\uc5b4","text":"<p>DPO\ub294 \"Your Language Model is Secretly a Reward Model\"\uc774\ub77c\ub294 \ubd80\uc81c\uc5d0\uc11c \uc54c \uc218 \uc788\ub4ef\uc774, \uc5b8\uc5b4 \ubaa8\ub378 \uc790\uccb4\uac00 \ubcf4\uc0c1 \ubaa8\ub378\uc758 \uc5ed\ud560\uc744 \ud560 \uc218 \uc788\ub2e4\ub294 \ud1b5\ucc30\uc5d0 \uae30\ubc18\ud569\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740 RLHF\uc758 \ucd5c\uc801 \uc815\ucc45\uc744 \uc9c1\uc811 \uc720\ub3c4\ud558\uc5ec \ub2e8\uc77c \ub2e8\uacc4\uc758 \uc9c0\ub3c4\ud559\uc2b5\uc73c\ub85c \uad6c\ud604\ud569\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#_3","title":"\uc218\ud559\uc801 \uc6d0\ub9ac","text":"<p>DPO\ub294 \ube0c\ub798\ub4e4\ub9ac-\ud14c\ub9ac(Bradley-Terry) \ubaa8\ub378\uacfc \uc720\uc0ac\ud55c \ubc29\uc2dd\uc73c\ub85c, \ub450 \uc751\ub2f5 \uac04\uc758 \uc120\ud638\ub3c4\ub97c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4:</p> <p>$$p_\\theta(y_w \\succ y_l \\mid x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$</p> <p>\uc5ec\uae30\uc11c \ud575\uc2ec\uc740 \ubcf4\uc0c1 \ud568\uc218 $$r_\\theta$$ \ub97c \uc5b8\uc5b4 \ubaa8\ub378\uc758 \ub85c\uadf8 \ud655\ub960\ub85c \uc7ac\ub9e4\uac1c\ubcc0\uc218\ud654\ud558\ub294 \uac83\uc785\ub2c8\ub2e4:</p> <p>$$ r_\\theta(x,y) = \\beta \\Big(\\log \\pi_\\theta(y\\mid x) - \\log \\pi_{\\text{ref}}(y\\mid x)\\Big) $$</p> <p>$$\\pi_{\\text{ref}}$$ \ub294 \ucd08\uae30 \ubaa8\ub378 $$\\pi_\\theta$$ \ub294 \ud559\uc2b5 \uc911\uc778 \ubaa8\ub378 $$\\beta$$\ub294 \uc870\uc808 \uac00\ub2a5\ud55c \uc2a4\ucf00\uc77c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub85c, \ucc38\uc870 \ubaa8\ub378 \ub300\ube44 \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc758 \ud655\ub960\uc744 \uc5bc\ub9c8\ub098 \uac15\uc870\ud560\uc9c0\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/preference_optimization/#dpo","title":"DPO\uc758 \uc7a5\uc810","text":"<ul> <li>\ub2e8\uc21c\uc131: \ubcc4\ub3c4\uc758 Reward \ubaa8\ub378\uc774\ub098 \uac15\ud654\ud559\uc2b5 \uc5c6\uc774 \ub2e8\uc77c \ub2e8\uacc4\uc758 Supervised Learning \uc73c\ub85c \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. </li> <li>\ud6a8\uc728\uc131: \uc0d8\ud50c\ub9c1\uc774\ub098 \ubc18\ubcf5\uc801\uc778 \uac15\ud654\ud559\uc2b5 loop\uc774 \ud544\uc694 \uc5c6\uc5b4 \uacc4\uc0b0 \ube44\uc6a9 \uc808\uac10\ub418\uc5c8\uc2b5\ub2c8\ub2e4. </li> </ul> <p>\uc704 \ub450 \uc694\uc18c\ub97c \ud1b5\ud574, RLHF \ubcf4\ub2e4 \ub354 \uc548\uc815\uc801\uc774\uace0 \ud6a8\uc728\uc801\uc778 \ud559\uc2b5\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.  Optimization\uc774 closed-form \uc774\ub77c \uc548\uc815\uc801\uc774\ub2c8, \uc2e4\uc81c\ub85c \ub354 \uc88b\uc73c\ub2c8 \ubb50\ub2c8 \ub9d0\uc774 \ub9ce\uc9c0\ub9cc\uc694...</p> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac RLHF \ub294 \uc5b4\ub835\uace0 DPO \ub294 \uc27d\uc2b5\ub2c8\ub2e4. \uadf8\uac8c \ub2e4 \ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. \ud070 \uadf8\ub8f9\ub4e4\uc5d0\uc11c\ub294 RLHF \ub97c \uc131\uacf5\uc2dc\ud0a8 \ubc18\uba74\uc5d0  \uc791\uc740 \uadf8\ub8f9\ub4e4(\ud2b9\ud788 \uc624\ud508 \uc9c4\uc601)\uc5d0\uc11c\ub294 RLHF \ub97c \uc131\uacf5\uc2dc\ud0a8 \uc0ac\ub840\uac00 \ucc3e\uae30 \ud798\ub4dc\ub124\uc694. DPO \ub294 \uadf8 \ub300\uc548\uc73c\ub85c\uc11c \uc544\uc8fc \uc88b\uc740 \uc120\ud0dd\uc9c0\uc785\ub2c8\ub2e4."},{"location":"tuning_techniques/preference_optimization/#_4","title":"\ub370\uc774\ud130 \ud615\uc2dd","text":"<p>DPO \ud559\uc2b5\uc744 \uc704\ud574\uc11c\ub294 \uc120\ud638\ub3c4 \ube44\uad50 \ub370\uc774\ud130\uc14b (\ud504\ub86c\ud504\ud2b8 x, \uc120\ud638 \uc751\ub2f5 y_w, \ube44\uc120\ud638 \uc751\ub2f5 y_l) \uc774 \ud544\uc694\ud569\ub2c8\ub2e4.  </p> <p>Intel Orca DPO Pairs \ub370\uc774\ud130\uc14b \uc5d0\uc11c \ubc1c\ucdcc \ud588\uc2b5\ub2c8\ub2e4.</p> \ud504\ub86c\ud504\ud2b8 On a scale of 1-5 (with 1 being least favorable and 5 being most favorable), how would you rate this review? \"Good at all\" \uc120\ud638 \uc751\ub2f5 I would rate this review as a 5, as it is a positive review with no negative comments or criticisms. The reviewer states that the product is \"good at all,\" which suggests that they are very satisfied with their experience. \ube44\uc120\ud638\uc751\ub2f5 2 <p>\uc774 \ub370\uc774\ud130\ub97c \uc608\uc2dc\ub85c \ub4e4\uba74, \ub300\ub2f5\uc744 \ud6e8\uc52c \ub354 \uae38\uac8c\uae38\uac8c \uc124\uba85\uc744 \ud3ec\ud568\ud558\uac8c \ud558\ub3c4\ub85d \ubaa8\ub378\uc774 align\uc774 \ub418\uaca0\uc8e0. </p> <p>\ud559\uc2b5 \uacfc\uc815\uc740 \uc77c\ubc18\uc801\uc778 Supervised Learning \uacfc \uc720\uc0ac\ud558\uace0, \uadf8\ub0e5 SFT \ud558\ub4ef\uc774 \ud558\uba74 \uc54c\uc544\uc11c gradient descent \ub97c \uacc4\uc0b0\ud574\uc11c weight \uac00 \uc5c5\ub370\uc774\ud2b8 \ub429\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/preference_optimization/#zephyr","title":"\uc2e4\uc81c \uc801\uc6a9 \uc0ac\ub840: Zephyr","text":"<p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Zephyr \ubb38\uc11c \ub97c \ucc38\uace0\ud558\uc138\uc694.  </p> <p>\ud5c8\uae45\ud398\uc774\uc2a4\uc758 Zephyr \ubaa8\ub378\uc740 DPO\ub97c \ud65c\uc6a9\ud55c \ub300\ud45c\uc801\uc778 \uc0ac\ub840\uc785\ub2c8\ub2e4:</p> <ol> <li>SFT: Mistral-7B\ub97c \uae30\ubc18\uc73c\ub85c GPT-4\uac00 \uc0dd\uc131\ud55c \uc751\ub2f5\uc73c\ub85c SFT</li> <li>DPO: GPT-4\uac00 \ud3c9\uac00\ud55c \uc120\ud638\ub3c4 \uc30d(UltraFeedback \ub370\uc774\ud130\uc14b)\uc73c\ub85c DPO</li> </ol> <p>Zephyr-7B\ub294 MT-Bench\uc5d0\uc11c LLaMA2-Chat-70B \ub4f1 \ud6e8\uc52c \ud070 \ubaa8\ub378\uc744 \ub2a5\uac00\ud558\ub294 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\uba70, \uc778\uac04\uc758 \uac1c\uc785\uc5c6\uc774 \uc5c6\uc774 GPT-4\uc758 \uc9c0\uc2dd\uacfc \uc120\ud638\ub3c4\ub97c \ud6a8\uacfc\uc801\uc73c\ub85c \uc801\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</p> <p>\ucc38\uace0: \uc774\ub7f0 \ubc29\uc2dd\uc758 \ub2a5\ub825 (\uc5ec\uae30\uc11c\ub294 \uc9c0\uc2dd\uacfc \uc120\ud638\ub3c4) \uc744 \uac00\uc838\uc624\ub294 \ubc29\uc2dd\uc744 Distillation (\uc99d\ub958) \ub77c\uace0 \ud569\ub2c8\ub2e4. </p>"},{"location":"tuning_techniques/preference_optimization/#_5","title":"\uae30\ud0c0 \uc120\ud638\ub3c4 \ucd5c\uc801\ud654 \uae30\ubc95","text":""},{"location":"tuning_techniques/preference_optimization/#orpo-odds-ratio-preference-optimization","title":"ORPO (Odds Ratio Preference Optimization)","text":"<p>2024\ub144 KAIST \uc5d0\uc11c \uc81c\uc548\ud55c DPO\uc758 \ubc1c\uc804\ub41c \ud615\ud0dc\ub85c, \ucc38\uc870 \ubaa8\ub378 \uc5c6\uc774 \uc120\ud638\ub3c4 \ucd5c\uc801\ud654\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \"\ub2e8\uc77c \ub2e8\uacc4\" \uc811\uadfc\ubc95\uc73c\ub85c SFT\uc640 \uc120\ud638\ub3c4 \uc815\ub82c\uc744 \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud558\uba70, \uc9c0\ub3c4\ud559\uc2b5 \uc190\uc2e4\uacfc \uc624\uc988\ube44 \uae30\ubc18 \uc120\ud638\ub3c4 \uc190\uc2e4\uc744 \uacb0\ud569\ud569\ub2c8\ub2e4: Refernece \ubaa8\ub378\uc774 \ud544\uc694 \uc5c6\uc5b4 \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131\uc774 \ub192\uace0, SFT\uc640 \uc120\ud638\ub3c4 \ud559\uc2b5\uc744 \ub2e8\uc77c \ub2e8\uacc4\ub85c \ud1b5\ud569\ud558\uc5ec \uad6c\ud604\uc774 \uc6a9\uc774\ud569\ub2c8\ub2e4.</p> <p>(\ud544\uc790 \uc758\uacac) \uc544\uc9c1\uae4c\uc9c0\ub294 \ud070 \uc0ac\uc774\uc988\uc758 \ubaa8\ub378\uc5d0\uc11c\uc758 \uc131\uacf5\uc0ac\ub840\ub294 \ubcf4\uc9c0 \ubabb\ud588\uc2b5\ub2c8\ub2e4. \uc791\uc740 \uc0ac\uc774\uc988 (&lt; 10B) \uc5d0\uc11c\ub294 \uc131\uacf5\uc0ac\ub840\uac00 \uc788\uc2b5\ub2c8\ub2e4. (ex. Zephyr-ORPO)</p>"},{"location":"tuning_techniques/preference_optimization/#simpo-simple-preference-optimization","title":"SimPO (Simple Preference Optimization)","text":"<p>2024\ub144 Meng \ub4f1\uc774 \uc81c\uc548\ud55c \ubc29\ubc95\uc73c\ub85c, \uc2dc\ud000\uc2a4\uc758 \ud3c9\uade0 \ub85c\uadf8\ud655\ub960\uc744 \ubcf4\uc0c1\uc73c\ub85c \uc0ac\uc6a9\ud558\uace0 \ucc38\uc870 \ubaa8\ub378\uc744 \uc81c\uac70\ud55c \uac04\uc18c\ud654\ub41c DPO\uc785\ub2c8\ub2e4. \uc2b9\uc790\uc640 \ud328\uc790 \uc751\ub2f5\uc758 \ub85c\uadf8\ud655\ub960 \ucc28\uc774\uc5d0 margin\uc744 \uc8fc\uc5b4 \ud559\uc2b5 \ud6a8\uacfc\ub97c \ub192\uc600\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#ipo-identity-preference-optimization","title":"IPO (Identity Preference Optimization)","text":"<p>DPO \uc190\uc2e4\uc5d0 \uc815\uaddc\ud654 \ud56d\uc744 \ucd94\uac00\ud558\uc5ec \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud558\uace0 \uc548\uc815\uc801\uc778 \uc218\ub834\uc744 \ub3c4\ubaa8\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#kto-kahneman-tversky-optimization","title":"KTO (Kahneman-Tversky Optimization)","text":"<p>\uc30d(pair)\uc774 \uc544\ub2cc \uac1c\ubcc4 \uc751\ub2f5\uc758 \uc88b\uace0 \ub098\uc068 \ub808\uc774\ube14\ub9cc\uc73c\ub85c\ub3c4 \ud559\uc2b5\ud560 \uc218 \uc788\ub3c4\ub85d \uc190\uc2e4\uc744 \uc815\uc758\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#_6","title":"\uc120\ud638\ub3c4 \ucd5c\uc801\ud654 \uae30\ubc95 \ube44\uad50","text":"\uae30\ubc95 \ud559\uc2b5 \ubc29\uc2dd \ud544\uc694 \uad6c\uc131 \uc694\uc18c \uc548\uc815\uc131 \ud6a8\uc728\uc131 \uad6c\ud604 \ub09c\uc774\ub3c4 RLHF (PPO) \uac15\ud654\ud559\uc2b5 \ubcf4\uc0c1 \ubaa8\ub378, \ucc38\uc870 \ubaa8\ub378 \ub0ae\uc74c \ub0ae\uc74c \ub192\uc74c DPO \uc9c0\ub3c4\ud559\uc2b5 \ucc38\uc870 \ubaa8\ub378 \uc911\uac04~\ub192\uc74c \uc911\uac04~\ub192\uc74c \uc911\uac04 ORPO \uc9c0\ub3c4\ud559\uc2b5 \uc5c6\uc74c \uc911\uac04~\ub192\uc74c \ub192\uc74c \ub0ae\uc74c"},{"location":"tuning_techniques/preference_optimization/#_7","title":"\uacb0\ub860","text":"<p>\uc120\ud638\ub3c4 \ucd5c\uc801\ud654 \uae30\ubc95\uc740 LLM\uc744 \uc778\uac04\uc758 \uac00\uce58\uc640 \uc120\ud638\uc5d0 \ub9de\uac8c \uc815\ub82c\ud558\ub294 \ud575\uc2ec \ubc29\ubc95\ub860\uc785\ub2c8\ub2e4. DPO\ub97c \ud544\ub450\ub85c \ud55c \"RLHF\uc5d0\uc11c RL\uc744 \uc81c\uac70\"\ud558\ub294 \uc811\uadfc\ubc95\uc740 LLM \uc815\ub82c \uacfc\uc815\uc744 \ud06c\uac8c \ub2e8\uc21c\ud654\ud588\uc73c\uba70, \ub354 \ub9ce\uc740 \uc5f0\uad6c\uc790\uc640 \uac1c\ubc1c\uc790\uac00 \uace0\ud488\uc9c8\uc758 \uc815\ub82c\ub41c \ubaa8\ub378\uc744 \ub9cc\ub4e4 \uc218 \uc788\uac8c \ud588\uc2b5\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/preference_optimization/#_8","title":"\ucc38\uace0 \ubb38\ud5cc","text":"<ol> <li>Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., &amp; Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290.</li> <li>Hong, J., Pang, R., Jeon, H., Kim, J., &amp; Yun, S. (2024). ORPO: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691.</li> <li>Meng, K., Moradshahi, M., Semnani, S. J., Liang, P., &amp; Zou, J. (2024). SimPO: Simple preference optimization with a reference-free reward. arXiv preprint arXiv:2405.14734.</li> <li>Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Bellagente, M., ... &amp; Wolf, T. (2023). Zephyr: Direct distillation of LM alignment. arXiv preprint arXiv:2310.16944.</li> </ol>"},{"location":"tuning_techniques/quantization/","title":"\uc591\uc790\ud654(Quantization)","text":"<p>LLM\uc5d0 \uc801\uc6a9\ub418\ub294 \uc591\uc790\ud654(quantization) \uae30\ubc95\ub4e4\uc740 \ubaa8\ub378\uc758 \uac00\uc911\uce58\uc640/\ub610\ub294 \ud65c\uc131\uac12\uc744 \uc800\uc815\ubc00\ub3c4\ub85c \ud45c\ud604\ud558\uc5ec \uba54\ubaa8\ub9ac\uc640 \uc5f0\uc0b0\ub7c9\uc744 \uc904\uc785\ub2c8\ub2e4. \ucef4\ud4e8\ud130\uc5d0\uc11c \uae30\ubcf8\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uc815\ubc00\ub3c4\ub294 Floating Point 32bit \ub610\ub294 64bit \uc778\ub370, LLM \uc5d0\uc11c\ub294 \uc774\ub9cc\ud07c \uc815\ubc00\ud55c \uac83\uc774 \uaf2d \ud544\uc694\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub274\ub7f4\ub137 \uc54c\uace0\ub9ac\uc998\uc5d0 \ubaa8\ub450 \ud574\ub2f9\ud558\ub294 \uc774\uc57c\uae30\ub85c LLM \ubfd0\ub9cc \uc544\ub2c8\ub77c CNN (Convolutional Neural Network) \uae30\ubc18 \ubaa8\ub378\ub4e4 \uc5d0\uc11c\ub3c4 \uc560\uc6a9\ub418\uc5c8\ub358 \ubc29\ubc95\uc785\ub2c8\ub2e4.  </p>"},{"location":"tuning_techniques/quantization/#1-quantization","title":"1. \uc8fc\uc694 Quantization \uae30\ubc95","text":"<p>\uac00\uc7a5 \uae30\ubcf8\uc774 \ub418\ub294 FP16 \uacfc BF16 \uc5d0 \ub300\ud574\uc11c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. </p> <p>Quantization</p> <ul> <li> <p>FP16 (\ubc18\uc815\ubc00\ub3c4 \ubd80\ub3d9\uc18c\uc218\uc810): 16\ube44\ud2b8 \ubd80\ub3d9\uc18c\uc218\uc810\uc73c\ub85c \ud45c\uc900 32\ube44\ud2b8 \ub300\uc2e0 \uc5f0\uc0b0\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ud604\uc7ac \ub300\uaddc\ubaa8 \ubaa8\ub378 \ud6c8\ub828\uacfc \ucd94\ub860\uc5d0\uc11c \uae30\ubcf8\uc73c\ub85c \ud65c\uc6a9\ub418\ub294 \uae30\ubc95\uc73c\ub85c, \uba54\ubaa8\ub9ac \uc0ac\uc6a9\uc744 \uc808\ubc18\uc73c\ub85c \uc904\uc774\uace0 \uc5f0\uc0b0 \uc18d\ub3c4\ub97c \ub192\uc774\uba74\uc11c\ub3c4 \ubaa8\ub378 \uc131\ub2a5 \uc800\ud558\uac00 \ubbf8\ubbf8\ud569\ub2c8\ub2e4 FP16 \uc0ac\uc6a9 \uc2dc \ud070 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \ucd5c\uc2e0 LLM\ub4e4\uc740 FP32\ubcf4\ub2e4 FP16/BF16(Brain Floating Point)\uc73c\ub85c \ud559\uc2b5/\ucd94\ub860\ub418\ub294 \uac83\uc774 \uc77c\ubc18\uc801\uc785\ub2c8\ub2e4.</p> </li> <li> <p>BF16 (Brain Floating Point 16): \uc694\uc998 GPU \uc5d0\uc11c \uae30\ubcf8\uc73c\ub85c \ucde8\uae09 \ubc1b\ub294 \ud3ec\ub9f7\uc774\uc8e0. BF16\uc740 Google Brain \ud300\uc774 \uac1c\ubc1c\ud55c 16\ube44\ud2b8 \ubd80\ub3d9\uc18c\uc218\uc810 \ud615\uc2dd\uc73c\ub85c, FP32\uc758 \uc9c0\uc218\ubd80(8\ube44\ud2b8)\ub97c \uadf8\ub300\ub85c \uc720\uc9c0\ud558\uace0 \uac00\uc218\ubd80\ub9cc \uc904\uc778 \ud615\ud0dc\uc785\ub2c8\ub2e4. \uc774 \ud2b9\uc9d5 \ub355\ubd84\uc5d0 FP32\uc640 \ub3d9\uc77c\ud55c \ub113\uc740 \uc218 \ubc94\uc704\ub97c \ud45c\ud604\ud558\uba74\uc11c\ub3c4 \uba54\ubaa8\ub9ac\ub294 \uc808\ubc18\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. FP16\uacfc \ub2ec\ub9ac \uc624\ubc84\ud50c\ub85c\uc6b0/\uc5b8\ub354\ud50c\ub85c\uc6b0 \uc704\ud5d8\uc774 \uc801\uc5b4 \ubcc4\ub3c4\uc758 \uc190\uc2e4 \uc2a4\ucf00\uc77c\ub9c1 \uc5c6\uc774\ub3c4 \uc548\uc815\uc801\uc778 \ud559\uc2b5\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. NVIDIA A100/H100, Google TPU v3+ \ub4f1 \ucd5c\uc2e0 AI \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \uae30\ubcf8 \uc9c0\uc6d0\ub418\uba70, FP32 \ub300\ube44 2\ubc30 \uc774\uc0c1\uc758 \ucc98\ub9ac\ub7c9\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc2e4\uc81c\ub85c GPT-4, LLaMA, DeepSeek \ub4f1 \ub300\ubd80\ubd84\uc758 \ucd5c\uc2e0 LLM\ub4e4\uc774 BF16\uc73c\ub85c \ud559\uc2b5\ub418\uc5c8\uc73c\uba70, \ud604\uc7ac \ub525\ub7ec\ub2dd \ud559\uc2b5\uc758 \uc0ac\uc2e4\uc0c1 \ud45c\uc900 \ud3ec\ub9f7\uc73c\ub85c \uc790\ub9ac\uc7a1\uc558\uc2b5\ub2c8\ub2e4. \ucd5c\uadfc\uc5d0\ub294 FP8\uacfc \ud568\uaed8 \ud63c\ud569 \uc815\ubc00\ub3c4 \ud559\uc2b5\uc5d0 \ud65c\uc6a9\ub418\uc5b4 DeepSeek-V3 \uac19\uc740 \ucd08\uac70\ub300 \ubaa8\ub378(671B)\uc758 \ud6a8\uc728\uc801 \ud559\uc2b5\uc744 \uac00\ub2a5\ucf00 \ud588\uc2b5\ub2c8\ub2e4.</p> </li> </ul> <p>\ucc38\uace0 - NVIDIA GPU \uae30\uc900 RTX 30XX \ubd80\ud130 BF16\uc744 \uc9c0\uc6d0\ud558\uae30 \ub584\ubb38\uc5d0, \uc61b\ub0a0 GPU \ub4e4\uc740 \uc774 \ubd80\ubd84\uc5d0\uc11c \ud638\ud658\uc774 \uc548\ub418\uc5b4 \ubb38\uc81c\ub418\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc8fc\uc758\ud558\uc138\uc694!!!</p> <ul> <li>INT8 (8\ube44\ud2b8 \uc815\uc218 \uc591\uc790\ud654): \uac00\uc911\uce58\ub098 \ud65c\uc131\uac12\uc744 8\ube44\ud2b8 \uc815\uc218\ub85c \ud45c\ud604\ud558\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4. Float \uc544\ub2c8\uace0 Int \ub85c\uc694. Float \uacfc Int \uc758 \ucc28\uc774\uc5d0 \ub300\ud574\uc11c\ub294 \uc5ec\uae30\uc11c\ub294 \uc774 \uae00\uc744 \ubcf4\uc2dc\ub294 \ubd84\ub4e4\uc740 \ub2e4 \uc548\ub2e4\uace0 \uac00\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ud558\ub4dc\uc6e8\uc5b4\uc801\uc73c\ub85c\ub3c4 int \ub294 \uc5f0\uc0b0\uc5d0 \uc774\uc810\uc774 \uaf64 \uc788\uc2b5\ub2c8\ub2e4\ub9cc, \ubb38\uc81c\ub294 \ud45c\ud604 \ubc94\uc704\uac00 \uc791\uc8e0. \uadf8\ub798\uc11c \ud2b9\uc815 \uc21c\uac04\uc758 \uac12\ub4e4\uc774 \uc798\ub824\ubc84\ub9b4 \uc218\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc131\ub2a5\uc5d0 \uaf64 \uce58\uba85\uc801\uc77c \uc218 \uc788\uaca0\uc8e0. \uc774\ub97c \ubcf4\uc644\ud558\uae30 \uc704\ud574 \ubca1\ud130\ubcc4 \uc2a4\ucf00\uc77c \uc870\uc815 \ub4f1 \uc5ec\ub7ec \uae30\ubc95\ub4e4\uc774 \uac1c\ubc1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\uc704 \ubc29\ubc95\ub4e4\uc740 Training \ub2e8\uacc4\uc5d0\uc11c\ub3c4 \uc4f0\uc774\ub294 \ubc94\uc6a9\uc801\uc778 \ubc29\ubc95\uc774\ub77c\uba74, GPTQ/AWQ \ub294 \ud559\uc2b5\uc774 \ubaa8\ub450 \uc644\ub8cc\ub41c \ud6c4 (Post Training Quatization) Inference \uc758 \ud6a8\uc728\uc744 \ub192\uc774\uae30 \uc704\ud55c \ubc29\ubc95\ub4e4 \uc785\ub2c8\ub2e4. </p> <ul> <li> <p>GPTQ (Generative Pre-trained Transformer Quantization): 2022\ub144 \ub9d0 \uc81c\uc548\ub41c Post Training Quantization (PTQ) \uc54c\uace0\ub9ac\uc998\uc785\ub2c8\ub2e4. \ubaa8\ub378\uc758 \uac01 \uce35\uc744 \ud55c \ubc88\uc5d0 \ucd5c\uc801\ud654\ud558\uc5ec 4\ube44\ud2b8 \ub610\ub294 3\ube44\ud2b8\ub85c 1\ud68c(pass)\ub9cc\uc5d0 \uc591\uc790\ud654\ud558\ub294 \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4. GPTQ\ub294 2\ucc28 \ucd5c\uc801\ud654 \uc815\ubcf4(Hessian \uadfc\uc0ac)\ub97c \ud65c\uc6a9\ud574 \uc591\uc790\ud654 \uc624\ucc28\ub97c \ucd5c\uc18c\ud654\ud558\uba70, 175B\uac19\uc740 \ucd08\uac70\ub300 \ubaa8\ub378\ub3c4 \uba87 \uc2dc\uac04 \ub0b4 3~4\ube44\ud2b8\ub85c \ubcc0\ud658\ud558\uba74\uc11c\ub3c4 \uc815\ud655\ub3c4 \uc190\uc2e4\uc774 \uac70\uc758 \uc5c6\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c\ub85c GPTQ\ub85c 3~4\ube44\ud2b8\ub85c \uc555\ucd95\ud55c \ubaa8\ub378\uc740 \uc6d0\ubcf8 FP16 \ubaa8\ub378 \ub300\ube44 \uc131\ub2a5 \uc800\ud558\uac00 \ubb34\uc2dc\ud560 \ub9cc\ud07c \uc791\uace0, \ubaa8\ub378 \ud06c\uae30\ub294 16\ubd84\uc758 1 \uc218\uc900\uc73c\ub85c \uc904\uc5b4\ub4ed\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 175B \uaddc\ubaa8 \ubaa8\ub378\ub3c4 \ub2e8\uc77c GPU\uc5d0\uc11c \uc2e4\ud589 \uac00\ub2a5\ud568\uc744 \uc2dc\uc5f0\ud558\uc600\uc2b5\ub2c8\ub2e4.    </p> </li> <li> <p>AWQ (Activation-aware Weight Quantization): 2023\ub144 \ubc1c\ud45c\ub418\uc5c8\uace0, GPTQ\uc640 \uc720\uc0ac\ud558\uac8c \uac00\uc911\uce58\ub9cc \uc800\ube44\ud2b8\ub85c \uc591\uc790\ud654\ud558\ub294 PTQ \ubc29\ubc95\uc774\uc9c0\ub9cc, \ud65c\uc131\uac12 \ubd84\ud3ec\ub97c \ucc38\uace0\ud558\uc5ec \"\uc911\uc694\ud55c\" \uac00\uc911\uce58 \ucc44\ub110 1%\ub9cc \ubcf4\ud638\ud55c\ub2e4\ub294 \uc544\uc774\ub514\uc5b4\uac00 \ud575\uc2ec\uc785\ub2c8\ub2e4. \uc77c\ubd80 \ucc44\ub110\uc758 \uac00\uc911\uce58\ub294 \uc2a4\ucf00\uc77c\uc744 \ud0a4\uc6cc \uc591\uc790\ud654 \uc624\ucc28\ub97c \uc904\uc774\uace0 (\ucd94\ub860 \uc2dc\uc5d0\ub294 \ub2e4\uc2dc \uc2a4\ucf00\uc77c \ubcf4\uc815), \ub098\uba38\uc9c0\ub294 \uade0\uc77c\ud558\uac8c \uc591\uc790\ud654\ud569\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \ud63c\ud569 \uc815\ubc00\ub3c4 \uc5c6\uc774\ub3c4 \uc911\uc694\ud55c \uac00\uc911\uce58\uc758 \uc815\ubcf4\ub97c \uc720\uc9c0\ud560 \uc218 \uc788\uc5b4 \ud558\ub4dc\uc6e8\uc5b4 \ud6a8\uc728\uc131\uc744 \ud574\uce58\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 Instructio \ubaa8\ub378\uc774\ub098 \uba40\ud2f0\ubaa8\ub2ec LLM\uae4c\uc9c0 4\ube44\ud2b8\ub85c \uc591\uc790\ud654\ud574\ub3c4 \uc815\ud655\ub3c4\ub97c \ub192\uac8c \uc720\uc9c0\ud558\ub294 \uc131\uacfc\ub97c \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.</p> </li> <li> <p>\uae30\ud0c0 \uc800\ube44\ud2b8 \uc591\uc790\ud654: \uc0c1\uae30 \uc678\uc5d0\ub3c4 4\ube44\ud2b8 \uc774\ud558 \uc815\ubc00\ub3c4\ub85c\uc758 \ub3c4\uc804\uc774 \uc774\uc5b4\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 INT4 (4\ube44\ud2b8 \uc815\uc218) \uc591\uc790\ud654\ub294 \uba54\ubaa8\ub9ac \uadf9\uc18c\ud654 \uc7a5\uc810 \ub54c\ubb38\uc5d0 \ub9ce\uc774 \uc5f0\uad6c\ub418\uba70, GPTQ/AWQ \uac19\uc740 \ucd5c\uc2e0 \ubc29\ubc95\ub3c4 \ubcf8\uc9c8\uc801\uc73c\ub85c\ub294 INT4\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uc77c\ubd80 \uc5f0\uad6c\ub294 INT3\uc774\ub098 INT2\uae4c\uc9c0 \uc2e4\ud5d8\ud558\uace0 \uc788\uc73c\ub098, 2\ube44\ud2b8 \uc774\ud558\uc5d0\uc11c\ub294 \uc131\ub2a5 \uc800\ud558\uac00 \ub450\ub4dc\ub7ec\uc838 \uc2e4\uc6a9\uc131\uc774 \ub0ae\uc2b5\ub2c8\ub2e4.</p> </li> </ul>"},{"location":"tuning_techniques/quantization/#_1","title":"\uac01 \uc591\uc790\ud654 \ubc29\uc2dd \uac04 \ube44\uad50","text":"<p>\uc804\ubc18\uc801\uc73c\ub85c, FP16\uc740 \ube44\uad50\uc801 \uc548\uc815\uc801\uc778 \uc131\ub2a5\uc744 \uc720\uc9c0\ud558\uba74\uc11c \uba54\ubaa8\ub9ac \uc808\uc57d\uacfc \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc5bb\ub294 \uae30\ubcf8 \uae30\ubc95\uc774\uace0, INT8\uc740 \ucd94\uac00 \uc808\ubc18\uc758 \uba54\ubaa8\ub9ac \uc808\uac10\uc744 \uc81c\uacf5\ud558\uc9c0\ub9cc \uc815\ud655\ub3c4 \uad00\ub9ac\uac00 \uc911\uc694\ud569\ub2c8\ub2e4. GPTQ/AWQ \uac19\uc740 4\ube44\ud2b8 PTQ \uc54c\uace0\ub9ac\uc998\ub4e4\uc740 \ucd5c\uc18c\ud55c\uc758 \uc815\ud655\ub3c4 \uc190\uc2e4\ub85c \ucd5c\ub300 \uba54\ubaa8\ub9ac \uc555\ucd95\uc744 \ub178\ub9ac\ub294 \ucd5c\uc2e0 \ubc29\ubc95\uc73c\ub85c, \uae30\uc874 8\ube44\ud2b8\ubcf4\ub2e4 \ud6e8\uc52c \ub192\uc740 \uc555\ucd95\ub960\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4. GPTQ\ub294 \uac00\uc911\uce58 \uc7ac\uad6c\uc131 \ucd5c\uc801\ud654\ub85c \uc815\ud655\ub3c4\ub97c \ub192\uc778 \ubc18\uba74, AWQ\ub294 \ud65c\uc131\uac12 \uae30\ubc18 \ucc44\ub110 \uc2a4\ucf00\uc77c\ub9c1\uc73c\ub85c \ud558\ub4dc\uc6e8\uc5b4 \ud6a8\uc728\uc744 \uac15\uc870\ud55c \ucc28\uc774\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc591\uc790\ud654 \ub300\uc0c1 \uce21\uba74\uc5d0\uc11c, \uc704 \uae30\ubc95\ub4e4\uc740 \uc8fc\ub85c \uac00\uc911\uce58(weight)\ub97c \uc591\uc790\ud654\ud558\uc9c0\ub9cc, SmoothQuant \ub4f1 \uc77c\ubd80 \uc5f0\uad6c\ub294 \ud65c\uc131\uac12(activation)\uae4c\uc9c0 8\ube44\ud2b8\ub85c \ud568\uaed8 \uc591\uc790\ud654\ud558\uc5ec \uc5d4\ub4dc\ud22c\uc5d4\ub4dc INT8 \ucd94\ub860\uc744 \uac00\ub2a5\ucf00 \ud588\uc2b5\ub2c8\ub2e4 \ud65c\uc131\uac12 \uc591\uc790\ud654\ub294 \ub09c\uc774\ub3c4\uac00 \ub354 \ub192\uc9c0\ub9cc, \uc131\uacf5\ud558\uba74 \ucd94\uac00\uc801\uc778 \uba54\ubaa8\ub9ac/\ub300\uc5ed\ud3ed \uc774\uc810\uc744 \uc90d\ub2c8\ub2e4.</p>"},{"location":"tuning_techniques/quantization/#2-quantization","title":"2. Quantization\uc758 \uc7a5\uc810 (\uc18d\ub3c4, \uba54\ubaa8\ub9ac)","text":"<ul> <li> <p>\uc18d\ub3c4 \uac1c\uc120 \ud6a8\uacfc</p> <ul> <li>\uc591\uc790\ud654\ub294 \uc5f0\uc0b0\ub2f9 \ube44\ud2b8\uc218\uac00 \uc904\uc5b4\ub4e4\uc5b4 \ucc98\ub9ac\ud574\uc57c \ud560 \ub370\uc774\ud130\ub7c9\uc774 \uac10\uc18c\ud558\uace0, \ud558\ub4dc\uc6e8\uc5b4\uc758 \uc800\uc815\ubc00\ub3c4 \uc5f0\uc0b0 \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uc5ec \ucd94\ub860 \uc18d\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 GPTQ\ub85c 4\ube44\ud2b8\ub85c \uc555\ucd95\ub41c \ubaa8\ub378\uc740 \ub3d9\uc77c GPU\uc5d0\uc11c FP16 \ubaa8\ub378 \ub300\ube44 \uc57d 3.25\ubc30 \ube68\ub77c\uc84c\ub2e4\ub294 \ubcf4\uace0\uac00 \uc788\uc2b5\ub2c8\ub2e4 (A100 GPU \uae30\uc900).</li> <li>\ucd5c\uc2e0 AI \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c\ub294 \uc774\uc810\uc774 \ub354 \ucee4\uc9c0\ub294\ub370, FP8\uc758 \uacbd\uc6b0 NVIDIA Hopper \uacc4\uc5f4 GPU\uc5d0\uc11c FP16\uc758 \ub450 \ubc30\uc5d0 \ub2ec\ud558\ub294 \uc5f0\uc0b0 \ucc98\ub9ac\ub7c9(FLOPS)\uc744 \uc81c\uacf5\ud574 \ud559\uc2b5 \uc18d\ub3c4\ub97c \ub300\ud3ed \ud5a5\uc0c1 \uc2dc\ucf30\uc2b5\ub2c8\ub2e4.</li> <li>\uc18d\ub3c4 \uac1c\uc120 \uc815\ub3c4\ub294 \ud558\ub4dc\uc6e8\uc5b4 \uc9c0\uc6d0\uc5d0 \ub530\ub77c \ub9e4\uc6b0(!!!!) \uc0c1\uc774\ud569\ub2c8\ub2e4. \uc77c\ubd80 GPU\uc5d0\uc11c\ub294 4\ube44\ud2b8 \uc5f0\uc0b0\uc744 \uc9c1\uc811 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc544 4\ube44\ud2b8 -&gt; 16\ube44\ud2b8 \ubcc0\ud658 \ud6c4 \uc5f0\uc0b0\ud558\uae30\ub3c4 \ud558\ubbc0\ub85c, \uadf8\ub7f0 \uacbd\uc6b0 \uc774\ub4dd\uc774 \uba54\ubaa8\ub9ac \uc808\uac10\uc5d0 \uad6d\ud55c\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </li> <li>\uba54\ubaa8\ub9ac \ub300\uc5ed\ud3ed bottleneck\uc774 \uc788\ub294 \ub300\uaddc\ubaa8 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc804\uc1a1\ud574\uc57c \ud560 \ub370\uc774\ud130\ub7c9\uc774 \uc904\uc5b4\ub4dc\ub294 \uac83\ub9cc\uc73c\ub85c\ub3c4 throughput \ud5a5\uc0c1 \ud6a8\uacfc\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.</li> <li>\ub2f9\uc5f0\ud788 \uc804\uae30\uac12\ub3c4 \uc801\uac8c \ub098\uac00\uaca0\uc8e0.</li> </ul> </li> <li> <p>\uba54\ubaa8\ub9ac/\uc800\uc7a5 \uacf5\uac04 \uc808\uac10</p> <ul> <li>\uc591\uc790\ud654\uc758 \uac00\uc7a5 \uc9c1\uc811\uc801\uc778 \uc774\uc810\uc740 \ubaa8\ub378 \uba54\ubaa8\ub9ac \ud48b\ud504\ub9b0\ud2b8 \uac10\uc18c\uc785\ub2c8\ub2e4. \uc815\ubc00\ub3c4\ub97c 1/2, 1/4\ub85c \uc904\uc774\uba74 \uadf8\ub9cc\ud07c \ud544\uc694\ud55c \uc800\uc7a5 \uacf5\uac04\uacfc \uba54\ubaa8\ub9ac\ub3c4 \uac10\uc18c\ud569\ub2c8\ub2e4</li> <li>QLoRA \uae30\ubc95\uc740 7~13B\uae09 \ubaa8\ub378\uc744 4\ube44\ud2b8\ub85c \uc555\ucd95\ud558\uc5ec Single GPU \uba54\ubaa8\ub9ac\ub85c\ub3c4 Fine Tuning\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4 (Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA).</li> <li>\uc2e4\uc81c\ub85c 70B \ud30c\ub77c\ubbf8\ud130 LLM\uc744 4\ube44\ud2b8\ub85c \ub85c\ub4dc\ud558\uba74 \uc57d 40~48GB \uc815\ub3c4\ub85c\ub3c4 \ucc98\ub9ac \uac00\ub2a5\ud569\ub2c8\ub2e4. BF16 \uc6d0\ubcf8 \ubaa8\ub378\uc744 \ub85c\ub4dc\ud558\ub824\uba74 130GB \uc774\uc0c1 \ud544\uc694\ud588\ub358 \uac83\uacfc \ucc28\uc774\uac00 \ud06c\uc8e0....</li> <li>\uc774\ucc98\ub7fc \uba54\ubaa8\ub9ac \uac10\uc18c\ub294 \uc5f0\uad6c\uc790\ub098 \uac1c\ubc1c\uc790\uac00 \uc800\ube44\uc6a9 \uc7a5\ube44\uc5d0\uc11c \uc2e4\ud5d8\ud558\uac70\ub098, \ubaa8\ubc14\uc77c/\uc5e3\uc9c0 \ub514\ubc14\uc774\uc2a4\uc5d0\uc11c \ub300\uaddc\ubaa8 \ubaa8\ub378\uc744 \uad6c\ub3d9\ud560 \uc218 \uc788\ub3c4\ub85d \ud574\uc8fc\ub294 \ud575\uc2ec \ubc29\uc548\uc785\ub2c8\ub2e4</li> </ul> </li> </ul>"},{"location":"tuning_techniques/quantization/#3-quatization","title":"3. Quatization \ub2e8\uc810","text":"<p>\uadf8\ub807\ub2e4\uba74, \uc815\ubc00\ub3c4\ub97c \ud76c\uc0dd\ud55c\ub9cc\ud07c \ubaa8\ub378\uc774 \uba4d\uccad\ud574\uc9c8 \ud150\ub370\uc694... \uc5bc\ub9c8\ub098 \uc548 \uc88b\uc744\uae4c\uc694??  Qwen \uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ubca4\uce58\ub9c8\ud06c \uacb0\uacfc\ub97c \ubcf4\uba74 \uc591\uc790\ud654 \ubc29\uc2dd\uc5d0 \ub530\ub978 \uc131\ub2a5 \uc800\ud558\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. </p> \ubaa8\ub378 \uc591\uc790\ud654 \ubc29\uc2dd Average MMLU C-Eval IFEval Qwen2-72B-Instruct BF16 81.3 82.3 83.8 77.6 GPTQ-Int8 80.7 81.3 83.4 77.5 GPTQ-Int4 81.2 80.8 83.9 78.9 AWQ 80.4 80.5 83.9 76.9 Qwen2-7B-Instruct BF16 66.9 70.5 77.2 53.1 GPTQ-Int8 66.2 69.1 76.7 52.9 GPTQ-Int4 64.1 67.8 75.2 49.4 AWQ 64.1 67.4 73.6 51.4 Qwen2-1.5B-Instruct BF16 48.4 52.4 63.8 29.0 GPTQ-Int8 48.1 53.0 62.5 28.8 GPTQ-Int4 45.0 50.7 57.4 27.0 AWQ 46.5 51.6 58.1 29.9 Qwen2-0.5B-Instruct BF16 34.4 37.9 45.2 20.0 GPTQ-Int8 32.6 35.6 43.9 18.1 GPTQ-Int4 29.7 33.0 39.2 16.8 AWQ 31.1 34.4 42.1 16.7 <p>\uc704 \ud45c\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774, \uc591\uc790\ud654\ub85c \uc778\ud55c \uc131\ub2a5 \uc800\ud558\ub294 \ubaa8\ub378 \ud06c\uae30\uc5d0 \ub530\ub77c \ub2e4\ub974\uac8c \ub098\ud0c0\ub0a9\ub2c8\ub2e4:</p> <ul> <li>\ub300\ud615 \ubaa8\ub378(72B): \uc591\uc790\ud654\ub97c \uc801\uc6a9\ud574\ub3c4 \uc131\ub2a5 \uc800\ud558\uac00 \uac70\uc758 \uc5c6\uc73c\uba70, \uc77c\ubd80 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c\ub294 \uc624\ud788\ub824 GPTQ-Int4\uac00 BF16\ubcf4\ub2e4 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uae30\ub3c4 \ud569\ub2c8\ub2e4.</li> <li>\uc911\ud615 \ubaa8\ub378(7B): 8\ube44\ud2b8 \uc591\uc790\ud654\ub294 \ubbf8\ubbf8\ud55c \uc131\ub2a5 \uc800\ud558\ub9cc \ubcf4\uc774\uc9c0\ub9cc, 4\ube44\ud2b8 \uc591\uc790\ud654\uc5d0\uc11c\ub294 2-3% \uc815\ub3c4\uc758 \uc131\ub2a5 \uac10\uc18c\uac00 \uad00\ucc30\ub429\ub2c8\ub2e4.</li> <li>\uc18c\ud615 \ubaa8\ub378(1.5B, 0.5B): \ubaa8\ub378 \ud06c\uae30\uac00 \uc791\uc744\uc218\ub85d \uc591\uc790\ud654\uc758 \uc601\ud5a5\uc774 \ub354 \ucee4\uc9c0\uba70, \ud2b9\ud788 4\ube44\ud2b8 \uc591\uc790\ud654\uc5d0\uc11c\ub294 5% \uc774\uc0c1\uc758 \uc131\ub2a5 \uc800\ud558\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\uc774\ub7ec\ud55c \uacb0\uacfc\ub294 \ub300\ud615 \ubaa8\ub378\uc77c\uc218\ub85d \ud30c\ub77c\ubbf8\ud130 \uc911\ubcf5\uc131\uc774 \ub192\uc544 \uc591\uc790\ud654\uc5d0 \ub354 \uac15\uac74\ud558\ub2e4\ub294 \uac83\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4. \uc2e4\uc81c \uc751\uc6a9\uc5d0\uc11c\ub294 \ubaa8\ub378 \ud06c\uae30, \uc694\uad6c\ub418\ub294 \uc131\ub2a5, \ud558\ub4dc\uc6e8\uc5b4 \uc81c\uc57d \ub4f1\uc744 \uc885\ud569\uc801\uc73c\ub85c \uace0\ub824\ud558\uc5ec \uc801\uc808\ud55c \uc591\uc790\ud654 \ubc29\uc2dd\uc744 \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc5b4\ub5bb\uac8c quatization \uc744 \ud588\ub294\ub370\ub3c4 \uc131\ub2a5\uc774 \ub9ce\uc774 \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc744\uae4c\uc694?</p> <ul> <li>\ud45c\ud604 \ube44\ud2b8\uc218\uac00 \uc904\uc5b4\ub4e4\uba74 weight \ub4e4\uc774 \uc591\uc790\ud654 \uc624\ucc28\ub85c \uc778\ud574 \uc190\uc2e4\ub418\uace0, \uc774\ub294 \ub204\uc801\ub418\uc5b4 \ucd9c\ub825 \ud488\uc9c8 \ud558\ub77d\uc73c\ub85c \uc774\uc5b4\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ud788 LLM\ucc98\ub7fc \ud30c\ub77c\ubbf8\ud130 \uc218\uac00 \ub9e4\uc6b0 \ud070 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc77c\ubd80 \uacc4\uce35\uc758 \uc774\uc0c1\uce58(outlier) \uac12\uc774 \uacb0\uacfc\uc5d0 \ud070 \uc601\ud5a5\uc744 \uc8fc\ub294\ub370, \uc774\ub7ec\ud55c \uac12\uc744 \uc800\uc815\ubc00\ub3c4\ub85c \ud45c\ud604\ud558\uba74 \uc624\ucc28\uac00 \ucee4\uc9d1\ub2c8\ub2e4.</li> <li>\ucd5c\uadfc \uc5f0\uad6c\ub4e4\uc740 \ub2e4\uc591\ud55c \ubcf4\uc644\ucc45\uc744 \ud1b5\ud574 \uc591\uc790\ud654\ub85c \uc778\ud55c \uc131\ub2a5 \uc800\ud558\ub97c \uadf9\ubcf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ubca1\ud130\ubcc4 \uc2a4\ucf00\uc77c \uc870\uc815\uc774\ub098 \ucc44\ub110\ubcc4 \uc911\uc694\ub3c4 \uc2a4\ucf00\uc77c\ub9c1 \uae30\ubc95(LLM.int8(), AWQ \ub4f1)\uc740 \uc591\uc790\ud654 \uc624\ucc28\ub97c \uc904\uc5ec \uc815\ud655\ub3c4 \uc190\uc2e4\uc744 \ucd5c\uc18c\ud654\ud569\ub2c8\ub2e4.</li> </ul> <p>\uc704 Qwen\uc758 \uc2e4\ud5d8 \uacb0\uacfc\ub294 post training quantization \uae30\ubc18\uc758 \uacb0\uacfc \uc778\ub370\uc694, \ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c \uc591\uc790\ud654\ub97c \uc798\ud558\uba74 \uc131\ub2a5\uc800\ud558\ub97c \ub354 \uc904\uc77c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. </p> <ul> <li>QLoRA \uc640 \uac19\uc740 Quantized \ub41c \uc0c1\ud0dc\uc5d0\uc11c\uc758 \ud559\uc2b5\uc740 \uc131\ub2a5 \uc190\uc2e4\uc744 \ubcf4\uc644\ud569\ub2c8\ub2e4. QLoRA \uc5f0\uad6c\uc5d0 \ub530\ub974\uba74, 4\ube44\ud2b8\ub85c \uc591\uc790\ud654\ud558\uc5ec \ubc1c\uc0dd\ud558\ub294 \uc131\ub2a5 \uc800\ud558\ub294 LoRA \ubbf8\uc138\ud29c\ub2dd \ub2e8\uacc4\ub97c \uac70\uce58\uba74 \uc644\uc804\ud788 \ud68c\ubcf5\ud560 \uc218 \uc788\ub2e4\uace0 \uc8fc\uc7a5\ud569\ub2c8\ub2e4 <sup>1</sup></li> <li>Quantization-Aware Training \uc744 \ud1b5\ud574 \ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c\ubd80\ud130 \uc591\uc790\ud654 \uc624\ucc28\uc5d0 \uc801\uc751\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub300\ud45c\uc801\uc73c\ub85c NVIDIA\uc758 FP8 \ud559\uc2b5\uc5d0\uc11c\ub294 \ud559\uc2b5 \uc911\uac04\uc911\uac04 FP8\ub85c \ubcc0\ud658\ud574 \uc5f0\uc0b0\ud558\uba74\uc11c\ub3c4, \uc77c\ubd80 \ubbfc\uac10\ud55c \ubd80\ubd84\uc740 BF16/FP32\ub85c \uc720\uc9c0\ud558\ub294 \ud63c\ud569 \uc815\ubc00 \ud559\uc2b5\uc744 \uc801\uc6a9\ud574 \ud559\uc2b5 \uc548\uc815\uc131\uc744 \ud655\ubcf4\ud588\uc2b5\ub2c8\ub2e4<sup>2</sup><sup>3</sup>, \uc774\ub294 LLM \uc2dc\ub300 \uc774\uc804\uc5d0 CNN \uae30\ubc18\uc758 \ub525\ub7ec\ub2dd \ubaa8\ub378\ub4e4\uc5d0\uc11c\ub3c4 \uc790\uc8fc \uc0ac\uc6a9\ub418\uc5c8\ub358 \ubc29\ubc95\uc774\uc8e0.</li> </ul> \ud83d\udca1 \ud544\uc790\uc758 \uc758\uacac Quantization \uc740 \uc544\uc8fc \uc88b\uc740 \uc120\ud0dd\uc9c0 \uc785\ub2c8\ub2e4. \ud2b9\ud788\ub098 GPU \uba54\ubaa8\ub9ac \uc81c\uc57d\uc73c\ub85c \uc778\ud574 \uc9c4\uc785 \uc7a5\ubcbd\uc774 \ub192\uc740 \uc774 \uc2dc\uc810\uc5d0\uc11c\uc694. Inferece \uae30\uc900, 80GB \uac00 \uc788\ub2e4\uba74 32B BF16 \ubaa8\ub378 \ubcf4\ub2e4 70B Quatized \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc81c \uacbd\ud610\uc0c1 \ud56d\uc0c1 \uc88b\uc558\uc2b5\ub2c8\ub2e4. SFT \ud559\uc2b5 \uae30\uc900\uc73c\ub85c\ub3c4 \ub3d9\uc77c GPU \uae30\uc900 \ubaa8\ub378\uc744 \ucd5c\ub300\ud55c \ud0a4\uc6b0\uace0 QLoRA \ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ub300\ubd80\ubd84 \uc81c\uc77c \uc88b\uc740 \uc120\ud0dd\uc774\uc5c8\uc2b5\ub2c8\ub2e4."},{"location":"tuning_techniques/quantization/#3-2023","title":"3. \ucd5c\uadfc \uc5f0\uad6c \ub3d9\ud5a5 (2023\ub144\uc744 \uc911\uc2ec\uc73c\ub85c)","text":"<p>\uc9c0\ub09c 2\ub144\uac04 LLM \uc591\uc790\ud654 \ubd84\uc57c\ub294 \uae09\uc18d\ud788 \ubc1c\uc804\ud588\uc2b5\ub2c8\ub2e4. \uc8fc\uc694 \ub3d9\ud5a5\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <ul> <li> <p>2022\ub144: 8\ube44\ud2b8 \uc591\uc790\ud654 \uc2dc\ub300 \u2013 LLM.int8()\uc774 \ubca1\ud130 \ub2e8\uc704 \uc591\uc790\ud654\uc640 \uc774\uc0c1\uce58 16\ube44\ud2b8 \ucc98\ub9ac\ub85c \uc131\ub2a5 \uc800\ud558 \uc5c6\ub294 8\ube44\ud2b8 \uc5f0\uc0b0\uc744 \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. SmoothQuant\ub294 \ud65c\uc131\uac12 outlier \uc644\ud654\ub97c \ud1b5\ud574 \uac00\uc911\uce58\uc640 \ud65c\uc131\uac12 \ubaa8\ub450 INT8\ub85c \uc591\uc790\ud654\ud558\uc5ec 2\ubc30 \uba54\ubaa8\ub9ac \uc808\uac10, 1.5\ubc30 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4.</p> </li> <li> <p>2023\ub144: 4\ube44\ud2b8 \uc591\uc790\ud654 \ud655\uc0b0 \u2013 GPTQ\ub294 \ud5e4\uc2dc\uc548 \uadfc\uc0ac \uae30\ubc18 one-shot \uc591\uc790\ud654\ub85c 175B \ubaa8\ub378\uc744 4\ube44\ud2b8\ub85c \uc555\ucd95\ud558\uba74\uc11c\ub3c4 \uc131\ub2a5 \uc800\ud558\ub97c \ucd5c\uc18c\ud654\ud588\uc2b5\ub2c8\ub2e4. QLoRA\ub294 4\ube44\ud2b8 \uace0\uc815 \ubaa8\ub378\uc5d0 LoRA \uc5b4\ub311\ud130 \ud6c8\ub828\uc744 \uc801\uc6a9\ud574 \uc591\uc790\ud654 \uc190\uc2e4\uc744 \uc644\uc804\ud788 \ud68c\ubcf5\ud588\uc73c\uba70, 65B \ubaa8\ub378\uc744 \ub2e8\uc77c GPU\uc5d0\uc11c \ud6a8\uc728\uc801\uc73c\ub85c \ud29c\ub2dd\ud588\uc2b5\ub2c8\ub2e4. AWQ\ub294 \uc911\uc694 \uac00\uc911\uce58 1%\ub9cc \uc2a4\ucf00\uc77c \ubcf4\uc815\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ub3c4\uba54\uc778 \ubb34\uad00\ud558\uac8c 4\ube44\ud2b8 \uc591\uc790\ud654\ub97c \uc801\uc6a9\ud588\uace0, \uc9c0\uc2dc\ud615 LLM\uacfc \uba40\ud2f0\ubaa8\ub2ec \ubaa8\ub378\uc5d0\uc11c\ub3c4 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.</p> </li> <li> <p>2024\ub144: \ud559\uc2b5 \ub2e8\uacc4 \uc591\uc790\ud654 \u2013 DeepSeek-V3\ub294 FP8 \uc815\ubc00\ub3c4\ub97c \ud559\uc2b5\uc5d0 \ub3c4\uc785\ud55c \ucd5c\ucd08\uc758 \uc624\ud508\uc18c\uc2a4 LLM\uc73c\ub85c, 128\u00d7128 \ube14\ub85d \ub2e8\uc704 fine-grained \uc591\uc790\ud654\uc640 E4M3 \ud3ec\ub9f7\uc744 \ud65c\uc6a9\ud574 BF16 \ub300\ube44 0.25% \uc774\ub0b4 \uc190\uc2e4\ub85c 6710\uc5b5 \ud30c\ub77c\ubbf8\ud130 MoE \ubaa8\ub378\uc744 \ud559\uc2b5\ud588\uc2b5\ub2c8\ub2e4. \uc774\ub85c\uc368 \uc591\uc790\ud654\ub294 \ubaa8\ub378 \ud559\uc2b5 \ub2e8\uacc4\uc640 \ucd08\uac70\ub300 \ubaa8\ub378\ub85c\uae4c\uc9c0 \ud655\uc7a5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.</p> </li> </ul>"},{"location":"tuning_techniques/quantization/#4-20232025","title":"4. \ucd5c\uadfc \uc591\uc790\ud654 \uc801\uc6a9 \uc0ac\ub840 (2023~2025)","text":"<p>\ucd5c\uadfc \uacf5\uac1c\ub41c LLM\uc5d0\uc11c \uc591\uc790\ud654 \uae30\uc220\uc774 \uc2e4\uc81c\ub85c \uc544\uc8fc \ud65c\ubc1c\ud788! \ud65c\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li> <p>DeepSeek-V3 \ubc0f R1 (2024~2025)</p> <ul> <li>DeepSeek-V3\ub294 671B \ub9e4\uac1c\ubcc0\uc218\uc758 MoE \ubaa8\ub378\ub85c, \ud559\uc2b5\uacfc \ucd94\ub860\uc5d0 FP8 \uc815\ubc00\ub3c4\ub97c \uc804\uba74 \ud65c\uc6a9\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc774 \ubaa8\ub378\uc740 128\u00d7128 \ube14\ub85d\ubcc4 \uc2a4\ucf00\uc77c\ub9c1\uacfc E4M3 \ud3ec\ub9f7\uc744 \uc0ac\uc6a9\ud574 outlier \ubb38\uc81c\ub97c \ud574\uacb0\ud588\uace0, \uacb0\uacfc\uc801\uc73c\ub85c BF16 \ub300\ube44 0.25% \uc774\ub0b4 \uc190\uc2e4\ub85c \uc548\uc815\uc801\uc778 \ud559\uc2b5\uc744 \uc644\ub8cc\ud588\uc2b5\ub2c8\ub2e4.</li> <li>DeepSeek-R1\ub3c4 \ub3d9\uc77c\ud55c FP8 + MoE \uad6c\uc870\ub97c \ucc44\ud0dd\ud588\uc73c\uba70, \ub450 \ubaa8\ub378 \ubaa8\ub450 TensorRT-LLM\uc774\ub098 vLLM \ub4f1 \uc11c\ube59 \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ud1b5\ud574 FP8\ub85c \ucd94\ub860\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. </li> <li>DeepSeek\uc740 \uadf8\ub0e5 \ubaa8\ub450\uac00 \uc778\uc815\ud558\ub294 \uc555\ub3c4\uc801\uc744 \uc131\ub2a5\uc744 \ubcf4\uc600\uae30 \ub54c\ubb38\uc5d0, 2025\ub144 \ubd80\ud130\ub294 \ub2e4\ub4e4 Pre-Train \ub2e8\uacc4\uc5d0\uc11c \ubd80\ud130 FP8 \ud63c\ud569 \ud559\uc2b5\uc744 \ud558\uac8c \ub420 \uac83 \uac19\ub124\uc694. </li> </ul> </li> <li> <p>Meta LLaMA-2 \ubc0f \uc624\ud508 LLM\ub4e4 (2023): LLaMA-2 \ubaa8\ub378\uc740 \uacf5\uac1c \uc9c1\ud6c4\ubd80\ud130 \ub2e4\uc591\ud55c \uc591\uc790\ud654 \ubc84\uc804\uc774 \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4. HuggingFace Transformers\ub294 <code>load_in_8bit</code>/<code>load_in_4bit</code> \uc635\uc158\uc73c\ub85c bitsandbytes \uae30\ubc18 \uc591\uc790\ud654\ub97c \uc9c0\uc6d0\ud558\uba70, AWQ, GPTQ \uae30\ubc18 4\ube44\ud2b8 LLaMA-2 70B \ubaa8\ub378\uc774 \uacf5\uac1c\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ucee4\ubba4\ub2c8\ud2f0\uc5d0\uc11c\ub294 GGML/GGUF \ud3ec\ub9f7\uc73c\ub85c 8bit\ubd80\ud130 3bit\uae4c\uc9c0 \ub2e4\uc591\ud55c \uc591\uc790\ud654 \ubaa8\ub378\uc744 \ubc30\ud3ec\ud558\uace0 \uc788\uc73c\uba70, \uc774\ub294 LLM \uc591\uc790\ud654\uac00 \uc2e4\ud5d8\uc2e4 \ub2e8\uacc4\ub97c \ub118\uc5b4 \uc2e4\uc81c \ubc30\ud3ec\uc640 \ud65c\uc6a9 \ub2e8\uacc4\ub85c \uc9c4\uc785\ud588\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</p> </li> <li> <p>\uacbd\ub7c9 \ub514\ubc14\uc774\uc2a4/\uc628\ub514\ubc14\uc774\uc2a4 \uc0ac\ub840: \uc774\uc81c LLM \uc740 \uc628\ub514\ubc14\uc774\uc2a4 \uc5d0\uc11c\uc758 \uc0c1\uc6a9\ud654 \ub2e8\uacc4\ub97c \uc2dc\uc791\ud558\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4. CNN \uc774 \uadf8\ub7ec\ud588\ub4e0 Quatization \uc740 \ud544\uc218\uc801\uc73c\ub85c \ud560 \uac83\uc785\ub2c8\ub2e4. \ucd5c\uadfc 1-2\ub144\uac04\uc740 \"\uac00\ub2a5\ud55c \ud55c \uc591\uc790\ud654\ud574\uc11c \uc0ac\uc6a9\ud558\ub77c\"\uac00 \ub300\uaddc\ubaa8 \ubaa8\ub378 \ubc30\ud3ec\uc758 \uc554\ubb35\uc801\uc778 \ubaa8\ud1a0\uac00 \ub420 \uc815\ub3c4\ub85c, \uc591\uc790\ud654\ub294 \ud544\uc218\uc801\uc778 \uc694\uc18c\ub85c \uc790\ub9ac\uc7a1\uc740 \uac83 \uac19\ub124\uc694.</p> </li> </ul>"},{"location":"tuning_techniques/quantization/#5-quantization","title":"5. Quantization \uad00\ub828 \uc8fc\uc694 \ub3c4\uad6c \ubc0f \ub77c\uc774\ube0c\ub7ec\ub9ac","text":""},{"location":"tuning_techniques/quantization/#bitsandbytes","title":"bitsandbytes \ub77c\uc774\ube0c\ub7ec\ub9ac","text":"<ul> <li>bitsandbytes\ub294 Tim Dettmers\uac00 \uac1c\ubc1c\ud55c PyTorch\uc6a9 \uacbd\ub7c9\ud654 \uc5f0\uc0b0 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c, LLM\uc758 \uc591\uc790\ud654\uc5d0 \uc0ac\uc2e4\uc0c1 \ud45c\uc900\ucc98\ub7fc \uac00\uc7a5 \ub9ce\uc774 \ud65c\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>8\ube44\ud2b8 Adam \ub4f1\uc758 8\ube44\ud2b8 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \uc720\uba85\ud574\uc84c\uace0, \uc774\ud6c4 8-bit \ubc0f 4-bit \ud589\ub82c \uacf1\uc148 \uc9c0\uc6d0\uc744 \ucd94\uac00\ud558\uc5ec \ubaa8\ub378 Weight\uc758 \ud6a8\uc728\uc801 \uc591\uc790\ud654\ub97c \uac00\ub2a5\ucf00 \ud588\uc2b5\ub2c8\ub2e4. HuggingFace Transformers\uc5d0 \ud1b5\ud569\ub418\uc5b4, \uc608\ub97c \ub4e4\uc5b4 <code>AutoModel.from_pretrained(..., load_in_8bit=True)</code>\ucc98\ub7fc \ud638\ucd9c\ud558\uba74 \ub0b4\ubd80\uc801\uc73c\ub85c bitsandbytes\ub97c \uc774\uc6a9\ud574 \ubaa8\ub378\uc744 INT8 \uc591\uc790\ud654 \ubaa8\ub4dc\ub85c \ub85c\ub4dc\ud569\ub2c8\ub2e4.</li> <li>bitsandbytes\uc758 8bit \uad6c\ud604\uc740 LLM.int8() \uae30\ubc95(\ubca1\ud130 \ub2e8\uc704 \uc2a4\ucf00\uc77c\ub9c1 + Outlier 16bit \uc5f0\uc0b0)\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc5b4, \ub300\ubd80\ubd84\uc758 \uc5f0\uc0b0\uc744 INT8\ub85c \ucc98\ub9ac\ud558\uba74\uc11c\ub3c4 \uc815\ud655\ub3c4\ub97c \uac70\uc758 \uc190\uc0c1\uc2dc\ud0a4\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. </li> <li>4bit \ub85c\ub4dc (<code>load_in_4bit=True</code>)\ub3c4 \uc9c0\uc6d0\ud558\ub294\ub370, \uc774\ub294 QLoRA \uc5f0\uad6c\uc5d0\uc11c \ub3c4\uc785\ub41c NF4 (NormalFloat4) \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. NF4\ub294 \uac00\uc911\uce58 \ubd84\ud3ec\uc758 \ubd84\uc704\uc218\uc5d0 \uae30\ubc18\ud55c 4\ube44\ud2b8 \uc591\uc790\ud654\ub85c, \uade0\uc77c quantization\ubcf4\ub2e4 \uc815\ubcf4\ub97c \ub354 \uc798 \ubcf4\uc874\ud569\ub2c8\ub2e4. bitsandbytes\ub97c \ud1b5\ud574 \ubaa8\ub378\uc744 4bit\ub85c \ub85c\ub4dc\ud558\uba74 \ub0b4\ubd80\uc801\uc73c\ub85c \ube14\ub85d\ubcc4 (\uae30\ubcf8 64\uac1c \uc694\uc18c)\ub85c \uac00\uc911\uce58\ub97c quantize\ud558\uace0, \uc591\uc790\ud654 \uc2a4\ucf00\uc77c \uac12\ub4e4\uc740 \ubcc4\ub3c4 \ud14c\uc774\ube14\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \uc808\uac10 \ud6a8\uacfc\ub294 \ub9e4\uc6b0 \ucee4\uc11c, \uc608\ub97c \ub4e4\uc5b4 130\uc5b5 \ud30c\ub77c\ubbf8\ud130 \ubaa8\ub378\ub3c4 4bit\ub85c\ub294 \uc57d 6.5GB \uc815\ub3c4\uba74 \uc801\uc7ac\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4.</li> <li>\uc131\ub2a5 \uce21\uba74\uc5d0\uc11c bitsandbytes \uc591\uc790\ud654\ub294 \uc8fc\ub85c \uba54\ubaa8\ub9ac\uc640 GPU \ub300\uc5ed\ud3ed \uc774\ub4dd\uc744 \ud1b5\ud574 \uac04\uc811\uc801\uc778 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc8fc\uba70, H100\ucc98\ub7fc 8bit \uc5f0\uc0b0\uc774 \ud2b9\ud654\ub41c \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c\ub294 \ucd94\ub860 \uc18d\ub3c4\uac00 FP16 \ub300\ube44 \ud5a5\uc0c1\ub418\uae30\ub3c4 \ud569\ub2c8\ub2e4. \ub2e4\ub9cc 4bit\uc758 \uacbd\uc6b0 \uc544\uc9c1 low-level GPU \uc9c0\uc6d0\uc774 \uc5c6\uc73c\ubbc0\ub85c, 8bit \uc5f0\uc0b0\uc73c\ub85c \uc2dc\ubbac\ub808\uc774\uc158\ud558\ub294 \ud615\ud0dc\ub77c \uc18d\ub3c4 \uc774\uc810\uc740 \ud06c\uc9c0 \uc54a\uace0 \uba54\ubaa8\ub9ac \uc774\uc810\uc774 \uc8fc\ud6a8\ud569\ub2c8\ub2e4.</li> <li>bitsandbytes\ub294 \uc624\ub298\ub0a0 \uc218\ub9ce\uc740 LLM \uc555\ucd95 \ud504\ub85c\uc81d\ud2b8\uc758 \uae30\ubc18\uc73c\ub85c \uc4f0\uc774\uace0 \uc788\uc73c\uba70, \uc624\ud508\uc18c\uc2a4 LLM\uc744 \ucde8\uae09\ud558\ub294 HuggingFace, LangChain \ub4f1\uc758 \uc0dd\ud0dc\uacc4\uc5d0\uc11c \ud45c\uc900 \uad6c\uc131\uc694\uc18c\ub85c \uc790\ub9ac\uc7a1\uc558\uc2b5\ub2c8\ub2e4.</li> <li> <p>\ud55c \uac00\uc9c0 \uc720\uc758\ud560 \uc810\uc740, bitsandbytes\ub85c \uc591\uc790\ud654\ud55c \ubaa8\ub378\uc744 \ub2e4\ub8f0 \ub54c \uc5f0\uc0b0 \uc790\uccb4\ub294 \uc5ec\uc804\ud788 FP16\uc73c\ub85c \uc218\ud589\ub418\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ucee8\ub300 4bit\ub85c \ub85c\ub4dc\ub41c \uac00\uc911\uce58\ub294 \uacc4\uc0b0 \uc2dc BF16\uc73c\ub85c \ub514\ud000\ud0c0\uc774\uc988\ub418\uc5b4 \uacf1\uc148\uc774 \uc774\ub8e8\uc5b4\uc9c0\ubbc0\ub85c, \uba54\ubaa8\ub9ac\uc5d0\ub294 \uc774\ub4dd\uc774 \uc788\uc9c0\ub9cc \uacc4\uc0b0\uc801 \uc815\ubc00\ub3c4\uac00 4bit\uc778 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. \uc774\ub294 \uc815\ud655\ub3c4\ub97c \ubcf4\uc874\ud558\uae30 \uc704\ud55c \ud2b8\ub808\uc774\ub4dc\uc624\ud504\ub85c \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc804\uccb4\uc801\uc73c\ub85c, bitsandbytes\ub294 \uc0ac\uc6a9\uc758 \ud3b8\uc758\uc131, \uc548\uc815\uc131, \uc131\ub2a5 \uce21\uba74\uc5d0\uc11c LLM \uc591\uc790\ud654\ub97c \uc2e4\uc6a9\ud654\ud558\ub294\ub370 \ud070 \uae30\uc5ec\ub97c \ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4.</p> </li> <li> <p>\uc801\uc6a9 \ubc29\ubc95: HuggingFace Transformers\uc5d0\uc11c <code>from_pretrained</code> \ud638\ucd9c \uc2dc <code>load_in_8bit</code> \ub610\ub294 <code>load_in_4bit</code>\ub97c \uc9c0\uc815\ud558\uac70\ub098, <code>BitsAndBytesConfig</code>\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc138\ubd80 \uc124\uc815(\uc591\uc790\ud654 \uc784\uacc4\uac12 \ub4f1)\uc744 \uc870\uc808\ud569\ub2c8\ub2e4. \ub610\ub294 GPTQ\ucc98\ub7fc \uc0ac\ud6c4 \uc591\uc790\ud654\ub41c \ubaa8\ub378 \uac00\uc911\uce58\ub97c \ubd88\ub7ec\uc62c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc608\uc2dc:   <pre><code>model = AutoModelForCausalLM.from_pretrained(\n    \"... (ANY MODEL)\",\n    load_in_8bit=True,\n    ...\n)\n</code></pre>   \uc774\ub807\uac8c \ud558\uba74 weights\uac00 8bit\ub85c \uc555\ucd95\ub418\uc5b4 \ub85c\ub4dc\ub418\uace0, \ucd94\ub860 \uc2dc \ub0b4\ubd80\uc801\uc73c\ub85c bitsandbytes\uc758 \ucd5c\uc801\ud654\ub41c 8bit \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. 4bit\ub3c4 \ub3d9\uc77c\ud569\ub2c8\ub2e4.</p> </li> <li> <p>\uae30\ub2a5 \uc694\uc57d: </p> <ul> <li>8-bit \uc635\ud2f0\ub9c8\uc774\uc800 \u2013 \ud559\uc2b5 \uc2dc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc \uba54\ubaa8\ub9ac\ub97c 8bit\ub85c \uc904\uc5ec \ub300\uc6a9\ub7c9 \ubaa8\ub378 \ud559\uc2b5 \uba54\ubaa8\ub9ac \uc808\uac10</li> <li>INT8 \ubaa8\ub378 \ub85c\ub4dc \u2013 \ucd94\ub860 \uc2dc \uac00\uc911\uce58 8bit \uc591\uc790\ud654 \ubc0f outlier \ucc98\ub9ac</li> <li>INT4 \ubaa8\ub378 \ub85c\ub4dc \u2013 4bit \uc591\uc790\ud654 (NF4) \uc9c0\uc6d0</li> <li>\uae30\ud0c0 \u2013 GPU \uba54\ubaa8\ub9ac \ud560\ub2f9 \ucd5c\uc801\ud654, \uc7a5\uce58\uac04 \uba54\ubaa8\ub9ac \uc774\ub3d9 \ucd5c\uc801\ud654 \ub4f1</li> </ul> </li> </ul>"},{"location":"tuning_techniques/quantization/#_2","title":"\ucc38\uace0 \ubb38\ud5cc","text":"<ol> <li> <p>QLoRA: Efficient Finetuning of Quantized LLMs, https://ar5iv.labs.arxiv.org/html/2305.14314\u00a0\u21a9</p> </li> <li> <p>DeepSeek v3 and R1 Model Architecture: Why it's powerful and economical, https://fireworks.ai/blog/deepseek-model-architecture\u00a0\u21a9</p> </li> <li> <p>deepseek-ai/DeepSeek-V3, https://huggingface.co/deepseek-ai/DeepSeek-V3\u00a0\u21a9</p> </li> <li> <p>Understanding LLM.int8() Quantization, https://picovoice.ai/blog/understanding-llm-int8/\u00a0\u21a9</p> </li> <li> <p>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, https://arxiv.org/abs/2211.10438\u00a0\u21a9</p> </li> <li> <p>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, https://arxiv.org/abs/2210.17323\u00a0\u21a9</p> </li> <li> <p>Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA, https://huggingface.co/blog/4bit-transformers-bitsandbytes\u00a0\u21a9</p> </li> <li> <p>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, https://arxiv.org/abs/2306.00978\u00a0\u21a9</p> </li> <li> <p>Quantization, https://huggingface.co/docs/transformers/main/en/main_classes/quantization\u00a0\u21a9</p> </li> <li> <p>DeepSeek-V3/README_WEIGHTS.md, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/README_WEIGHTS.md\u00a0\u21a9</p> </li> </ol>"},{"location":"tuning_techniques/supervised_finetuning/","title":"Supervised Fine-Tuning (SFT)","text":""},{"location":"tuning_techniques/supervised_finetuning/#sft","title":"SFT \uac1c\uc694","text":"<ul> <li>SFT(Supervised Fine-Tuning)\ub294 \uc0ac\uc804\ud559\uc2b5\ub41c \uc5b8\uc5b4 \ubaa8\ub378\uc744 \ud2b9\uc815 \uc791\uc5c5\uc774\ub098 \uc0ac\uc6a9\uc790 \uc9c0\uc2dc\uc5d0 \ub9de\uac8c \uc870\uc815\ud558\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4.</li> <li>PreTrained Base \ubaa8\ub378\uc774 \uac00\uc9c4 \ub2e8\uc21c\ud55c \"Next Token Prediction\" \ub2a5\ub825\uc744 \ub118\uc5b4, \uc0ac\uc6a9\uc790 \uc9c8\ubb38\uc5d0 \uc720\uc6a9\ud558\uac8c \ub2f5\ubcc0\ud558\ub294 \ub2a5\ub825 (Instruction Following) \uc744 \ubd80\uc5ec\ud569\ub2c8\ub2e4.</li> <li>\uc9c0\uc2dc-\uc751\ub2f5 \uc30d\uc73c\ub85c \uad6c\uc131\ub41c \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud574 \ubaa8\ub378\uc774 \uc0ac\ub78c\uc758 \uc758\ub3c4\uc5d0 \ub9de\ub294 \ucd9c\ub825\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \ud6c8\ub828\ud569\ub2c8\ub2e4.</li> <li>\uc774 \uc678\uc5d0\ub3c4 knowledge distillation \uc774\ub098 style tuning, \ucd5c\uadfc\uc5d0\ub294 Reasoning \ub2a5\ub825\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub4f1 \ub2e4\uc591\ud558\uac8c \ud65c\uc6a9\ub429\ub2c8\ub2e4. </li> </ul> <p>\ud639\uc2dc\ub77c\ub3c4 SFT \uc5d0 \ub300\ud574\uc11c \uc775\uc219\ud558\uc9c0 \uc54a\uc73c\uc2dc\ub2e4\uba74 \uc544\ub798 \uae00\uc744 \ubcf4\uace0 \uc624\uc2dc\ub294 \uac83\uc744 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4. </p> \ud83d\udca1 \ucd94\ucc9c \uc790\ub8cc Supervised Fine-Tuning \uc774\ud574\ud558\uae30 - \uc81c\uac00 \uc791\uc131\ud588\ub358 SFT\uc758 \uae30\ubcf8 \uac1c\ub150\uacfc \uc608\uc2dc\ub97c \uc124\uba85\ud55c \uae00\uc785\ub2c8\ub2e4."},{"location":"tuning_techniques/supervised_finetuning/#sft_1","title":"SFT\uc758 \uc911\uc694\uc131","text":""},{"location":"tuning_techniques/supervised_finetuning/#_1","title":"\uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378\uc758 \ud55c\uacc4 \uadf9\ubcf5","text":"<ul> <li>\uc0ac\uc804\ud559\uc2b5\ub41c \ubca0\uc774\uc2a4 \ubaa8\ub378\uc740 \ud14d\uc2a4\ud2b8 \uc0dd\uc131 \ub2a5\ub825\uc740 \uc788\uc9c0\ub9cc, \uc0ac\uc6a9\uc790 \uc758\ub3c4\ub97c \uc815\ud655\ud788 \ud30c\uc545\ud558\uac70\ub098 \uc9c0\uc2dc\ub97c \ub530\ub974\ub294 \ub2a5\ub825\uc774 \ubd80\uc871\ud569\ub2c8\ub2e4.</li> <li>SFT\ub294 \ubaa8\ub378\uc774 \uc0ac\uc6a9\uc790 \uc9c0\uc2dc\ub97c \uc774\ud574\ud558\uace0 \uc720\uc6a9\ud55c \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \ud589\ub3d9\uc744 \uad50\uc815\ud569\ub2c8\ub2e4.</li> <li>\ub300\ubd80\ubd84\uc758 \uc0ac\uc6a9\uc790 \ubd84\ub4e4\uc774 \uc775\uc219\ud55c ChatGPT \ub294 \ub2f9\uc5f0\ud788 \uc774\ubbf8 SFT \ub41c (\uadf8\ub9ac\uace0 \ub354 \ub9ce\uc740 \uac1c\uc120\uc774 \ub41c) \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4.  </li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_2","title":"\uc2e4\uc6a9\uc131 \ud655\ub300","text":"<ul> <li>SFT\ub97c \ud1b5\ud574 \ubaa8\ub378\uc740 \uc9c8\ubb38 \uc751\ub2f5, \uc694\uc57d, \ucf54\ub4dc \uc0dd\uc131 \ub4f1 \ub2e4\uc591\ud55c \uc791\uc5c5\uc744 \uc218\ud589\ud560 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.</li> <li>\ud2b9\uc815 \ub3c4\uba54\uc778(\uc758\ub8cc, \ubc95\ub960, \uae08\uc735 \ub4f1)\uc5d0 \ud2b9\ud654\ub41c \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \uc870\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#alignment","title":"Alignment","text":"<ul> <li>SFT\ub294 \ubaa8\ub378\uc774 \uc778\uac04\uc758 \uac00\uce58\uc640 \uc758\ub3c4\uc5d0 \ub9de\uac8c \ud589\ub3d9\ud558\ub3c4\ub85d Align\ud558\ub294 \uccab \ub2e8\uacc4\uc785\ub2c8\ub2e4.</li> <li>\uc720\ud574\ud558\uac70\ub098 \ubd80\uc801\uc808\ud55c \uc751\ub2f5\uc744 \uc904\uc774\uace0, \ub3c4\uc6c0\uc774 \ub418\ub294 \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub3c4\ub85d \ud6c8\ub828\ud569\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_2","title":"SFT \ub370\uc774\ud130 \uc900\ube44","text":""},{"location":"tuning_techniques/supervised_finetuning/#-","title":"\uc9c0\uc2dc-\uc751\ub2f5 \uc30d \uad6c\uc131","text":"<ul> <li>SFT \ub370\uc774\ud130\ub294 \uae30\ubcf8\uc801\uc73c\ub85c (\uc9c0\uc2dc/\uc9c8\ubb38, \uae30\ub300 \uc751\ub2f5) \uc30d\uc73c\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. Instruction \ubaa8\ub308\uc5d0\uac8c \ud29c\ub2dd\uc744 \uc2dc\ud0a4\uae30 \ub584\ubb38\uc774\uc8e0.  </li> <li>\ub370\uc774\ud130 \uc218\uc9d1 \ubc29\ubc95:<ul> <li>\uc778\uac04 \uc791\uc131 \ub370\uc774\ud130: \uc804\ubb38\uac00\ub098 \ud3c9\uac00\uc790\uac00 \uc9c1\uc811 \uc791\uc131\ud55c \uace0\ud488\uc9c8 \uc751\ub2f5<ul> <li>ex. OpenAI InstructGPT\uc758 13K \ub370\uc774\ud130\uc14b, InstructGPT \ud398\uc774\ud37c\uc5d0 \uc5b4\ub5bb\uac8c \uc2e4\uc81c\ub85c \uc0ac\ub78c\ub4e4\uc744 \uace0\uc6a9\ud574\uc11c \ub370\uc774\ud130\ub97c \uc791\uc131\ud588\ub294\uc9c0 \uc798 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4.</li> </ul> </li> <li>\ud569\uc131 \ub370\uc774\ud130: \uae30\uc874 \uac15\ub825\ud55c LLM\uc744 \ud65c\uc6a9\ud574 \uc0dd\uc131\ud55c \uc9c0\uc2dc-\uc751\ub2f5 \uc30d (\uc608: Self-Instruct, Alpaca \ub370\uc774\ud130\uc14b, UltraChat \ub4f1\ub4f1.)</li> </ul> </li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_3","title":"\ub370\uc774\ud130 \ub2e4\uc591\uc131 \ud655\ubcf4","text":"<ul> <li>\ub2e4\uc591\ud55c \uc720\ud615\uc758 \uc9c0\uc2dc\ub97c \ud3ec\ud568\ud574\uc57c \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ub2a5\ub825\uc774 \ud5a5\uc0c1\ub429\ub2c8\ub2e4:</li> <li>\uc9c8\ubb38-\ub2f5\ubcc0, \ubd84\ub958, \uc694\uc57d, \ubc88\uc5ed, \ucc3d\uc758\uc801 \uae00\uc4f0\uae30 \ub4f1 \ub2e4\uc591\ud55c \uc791\uc5c5</li> <li>\uac04\ub2e8\ud55c \uc9c8\ubb38\ubd80\ud130 \ubcf5\uc7a1\ud55c \ucd94\ub860\uc774 \ud544\uc694\ud55c \uc9c8\ubb38\uae4c\uc9c0 \ub09c\uc774\ub3c4 \ubcc0\ud654</li> <li>\uc5ec\ub7ec \uc8fc\uc81c\uc640 \ub3c4\uba54\uc778\uc744 \ud3ec\uad04\ud558\ub294 \uc9c8\ubb38\ub4e4</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_4","title":"\ud488\uc9c8 \uc911\uc2ec \ub370\uc774\ud130 \ud050\ub808\uc774\uc158","text":"<ul> <li>\ub370\uc774\ud130 \ud488\uc9c8\uc774 SFT \uc131\uacf5\uc758 \ud575\uc2ec \uc694\uc18c\uc785\ub2c8\ub2e4.</li> <li>Meta\uc758 LIMA \uc5f0\uad6c \uc5d0 \ub530\ub974\uba74, 1,000\uac1c\uc758 \uace0\ud488\uc9c8 \uc608\uc81c\ub9cc\uc73c\ub85c\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>Alpaca \ub3c4 52,000 \uac1c\ub85c \uc88b\uc740 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc92c\uace0, \ucd5c\uadfc S1 \uc758 \uacbd\uc6b0\ub3c4 1,000\uac1c\uc758 \ub370\uc774\ud130\uc14b\uc73c\ub85c reasoning \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5c8\ub2e4\uace0 \ud569\ub2c8\ub2e4.  </li> <li>\ud488\uc9c8 \uae30\uc900:<ul> <li>\uc815\ud655\uc131: \uc0ac\uc2e4\uc5d0 \uae30\ubc18\ud55c \uc815\ubcf4 \uc81c\uacf5</li> <li>\uc720\uc6a9\uc131: \uc9c8\ubb38\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub41c \ub3c4\uc6c0\uc774 \ub418\ub294 \uc751\ub2f5</li> <li>\uba85\ud655\uc131: \uc774\ud574\ud558\uae30 \uc27d\uace0 \ub17c\ub9ac\uc801\uc778 \uad6c\uc870</li> <li>\uc548\uc804\uc131: \uc720\ud574\ud558\uac70\ub098 \ud3b8\ud5a5\ub41c \ub0b4\uc6a9 \ubc30\uc81c</li> </ul> </li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_3","title":"SFT \ucd5c\uc801\ud654 \uc804\ub7b5","text":""},{"location":"tuning_techniques/supervised_finetuning/#_5","title":"\ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc124\uc815","text":"<ul> <li>\ud559\uc2b5\ub960(Learning Rate): \uc77c\ubc18\uc801\uc73c\ub85c 1e-5 ~ 5e-5 \ubc94\uc704\uc758 \ub0ae\uc740 \uac12 \uc0ac\uc6a9</li> <li>\ubc30\uce58 \ud06c\uae30(Batch Size): \uba54\ubaa8\ub9ac \ud55c\uacc4 \ub0b4\uc5d0\uc11c \uac00\ub2a5\ud55c \ud06c\uac8c \uc124\uc815 (8-32)</li> <li>\uc5d0\ud3ec\ud06c(Epoch): \ub370\uc774\ud130 \uc591\uc5d0 \ub530\ub77c \uc870\uc808 (\ub300\ub7c9 \ub370\uc774\ud130: 1-2 \uc5d0\ud3ec\ud06c, \uc18c\ub7c9 \ub370\uc774\ud130: 3-5 \uc5d0\ud3ec\ud06c)</li> <li>\ud559\uc2b5 \uc2a4\ucf00\uc904\ub7ec: \uc120\ud615 \ub610\ub294 \ucf54\uc0ac\uc778 \uac10\uc1c4\uc640 \ucd08\uae30 \uc6cc\ubc0d\uc5c5(3-5%) \uc801\uc6a9</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_6","title":"\uacfc\uc801\ud569 \ubc29\uc9c0 \uae30\ubc95","text":"<ul> <li>\uc870\uae30 \uc885\ub8cc(Early Stopping): \uac80\uc99d \uc190\uc2e4\uc774 \ub354 \uc774\uc0c1 \uac1c\uc120\ub418\uc9c0 \uc54a\uc744 \ub54c \ud559\uc2b5 \uc911\ub2e8</li> <li>\uac00\uc911\uce58 \uac10\uc1c4(Weight Decay): \uc77c\ubc18\uc801\uc73c\ub85c 0.01 \ub0b4\uc678\ub85c \uc124\uc815</li> <li>\ub370\uc774\ud130 \uc99d\uac15: \uae30\uc874 \ub370\uc774\ud130\uc758 \ubcc0\ud615\uc744 \ud1b5\ud574 \ub2e4\uc591\uc131 \ud655\ubcf4</li> <li>\uacc4\uce35 \ub3d9\uacb0(Layer Freezing): \ud558\uc704 \uacc4\uce35\uc740 \uace0\uc815\ud558\uace0 \uc0c1\uc704 \uacc4\uce35\ub9cc \ubbf8\uc138\uc870\uc815</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_7","title":"\ud6a8\uc728\uc801 \ubbf8\uc138\uc870\uc815 \uae30\ubc95","text":"<ul> <li>LoRA(Low-Rank Adaptation): \uc6d0\ubcf8 \uac00\uc911\uce58\ub294 \uace0\uc815\ud558\uace0 \uc800\ub7ad\ud06c \ud589\ub82c\ub9cc \ud559\uc2b5\ud558\uc5ec \uba54\ubaa8\ub9ac \ud6a8\uc728\uc131 \uc99d\uac00</li> <li>QLoRA: \uc591\uc790\ud654\ub41c \ubaa8\ub378\uc5d0 LoRA \uc801\uc6a9\ud558\uc5ec \ub354 \ud070 \ubaa8\ub378\ub3c4 \uc801\uc740 \uba54\ubaa8\ub9ac\ub85c \ubbf8\uc138\uc870\uc815 \uac00\ub2a5</li> <li>Prefix/Prompt Tuning: \ubaa8\ub378 \ud30c\ub77c\ubbf8\ud130 \ub300\uc2e0 \uc785\ub825\uc5d0 \ud559\uc2b5 \uac00\ub2a5\ud55c \ud1a0\ud070 \ucd94\uac00</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_8","title":"\ud3c9\uac00 \ubc0f \ubaa8\ub2c8\ud130\ub9c1","text":"<ul> <li>\uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \uc815\uae30\uc801\uc73c\ub85c \uc131\ub2a5 \uce21\uc815 (\uc190\uc2e4, \uc815\ud655\ub3c4, \uc0dd\uc131 \ud488\uc9c8 \ub4f1)</li> <li>\uc778\uac04 \ud3c9\uac00\ub97c \ud1b5\ud55c \ucd9c\ub825 \ud488\uc9c8 \uac80\uc99d (\uc720\uc6a9\uc131, \uc815\ud655\uc131, \uc548\uc804\uc131 \ub4f1)</li> <li>\ub2e4\uc591\ud55c \ud504\ub86c\ud504\ud2b8\uc5d0 \ub300\ud55c \uc77c\ubc18\ud654 \ub2a5\ub825 \ud655\uc778</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_4","title":"SFT \uad6c\ud604 \uc608\uc81c","text":"<p>\uc544\ub798\ub294 Hugging Face Transformers\ub97c \uc0ac\uc6a9\ud55c \uac04\ub2e8\ud55c SFT \uad6c\ud604 \uc608\uc2dc\uc785\ub2c8\ub2e4:</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n# 1. \ud504\ub9ac\ud2b8\ub808\uc778 \ub41c GPT-2 \ubaa8\ub378\uacfc \ud1a0\ud06c\ub098\uc774\uc800 \ubd88\ub7ec\uc624\uae30\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# 2. \uc608\uc2dc\uc6a9 \uc9c0\ub3c4 \ud559\uc2b5 \ub370\uc774\ud130 \uc815\uc758 (Q&amp;A \ud615\ud0dc)\ntrain_pairs = [\n    {\"prompt\": \"Q: What is the capital of France?\\nA:\", \"response\": \" Paris.\"},\n    {\"prompt\": \"Q: Explain the theory of relativity in simple terms.\\nA:\",\n     \"response\": \" It is a physics theory by Einstein that says space and time are linked together...\"},\n    # ... (\ucd94\uac00 \ub370\uc774\ud130)\n]\n\n# 3. \ub370\uc774\ud130\uc14b \uc804\ucc98\ub9ac: \ud504\ub86c\ud504\ud2b8\uc640 \ub2f5\ubcc0\uc744 \uc774\uc5b4\ubd99\uc5ec \ud1a0\ud070\ud654\ud558\uace0 \ub808\uc774\ube14 \uc0dd\uc131\ntrain_encodings = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\nfor pair in train_pairs:\n    # \ud504\ub86c\ud504\ud2b8 + \uc751\ub2f5\uc744 \ud558\ub098\uc758 \uc2dc\ud000\uc2a4\ub85c \ub9cc\ub4e4\uace0 \ud1a0\ud070\ud654\n    text = pair[\"prompt\"] + pair[\"response\"]\n    enc = tokenizer(text, truncation=True, max_length=128)\n    input_ids = enc[\"input_ids\"]\n    attn_mask = enc[\"attention_mask\"]\n    # \ub808\uc774\ube14 \uc0dd\uc131: \ud504\ub86c\ud504\ud2b8 \ubd80\ubd84\uc740 -100\uc73c\ub85c \ub9c8\uc2a4\ud0b9\ud558\uace0, \ub2f5\ubcc0 \ubd80\ubd84\ub9cc \ud559\uc2b5 \ub300\uc0c1\uc73c\ub85c \uc124\uc815\n    # (GPT-2\ub294 causal LM\uc774\ubbc0\ub85c, \ud504\ub86c\ud504\ud2b8\uae4c\uc9c0 \ubaa8\ub450 \uc785\ub825\uc73c\ub85c \ubc1b\uace0 \ub2f5\ubcc0 \ubd80\ubd84\ub9cc \uc190\uc2e4 \uacc4\uc0b0)\n    prompt_len = len(tokenizer(pair[\"prompt\"])[\"input_ids\"])\n    labels = [-100]*prompt_len + input_ids[prompt_len:]  # \ud504\ub86c\ud504\ud2b8 \ud1a0\ud070\uc5d0 \ub300\uc751\ub418\ub294 \ubd80\ubd84 -100\n    # \uc778\ucf54\ub529 \uacb0\uacfc \uc800\uc7a5\n    train_encodings[\"input_ids\"].append(input_ids)\n    train_encodings[\"attention_mask\"].append(attn_mask)\n    train_encodings[\"labels\"].append(labels)\n\n# 4. Trainer\ub97c \ud65c\uc6a9\ud55c \ubaa8\ub378 \ubbf8\uc138\uc870\uc815 \uc124\uc815\ntraining_args = TrainingArguments(\n    output_dir=\"./sft-model\",\n    per_device_train_batch_size=2,\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"no\",   # (\uc608\uc2dc\uc5d0\uc11c\ub294 \uac80\uc99d \uc0dd\ub7b5)\n    save_strategy=\"epoch\"\n)\ntrainer = Trainer(model=model, args=training_args,\n                  train_dataset=list(range(len(train_encodings[\"input_ids\"]))),  # dummy indices\n                  # Trainer\ub97c \uc4f0\ub824\uba74 Dataset \ud615\uc2dd \ud544\uc694. \uc5ec\uae30\uc11c\ub294 \uac1c\ub150 \uc124\uba85\uc744 \uc704\ud574 \uc0dd\ub7b5\n                  tokenizer=tokenizer, data_collator=tokenizer)\n# \uc2e4\uc81c\ub85c\ub294 train_encodings\ub97c HuggingFace Dataset\uc73c\ub85c \ubcc0\ud658\ud558\uc5ec trainer.train() \ud638\ucd9c\n</code></pre>"},{"location":"tuning_techniques/supervised_finetuning/#sft_5","title":"SFT\uc640 \ub2e4\ub978 \ud559\uc2b5 \ubc29\ubc95 \ube44\uad50","text":""},{"location":"tuning_techniques/supervised_finetuning/#sft-vs-pre-training","title":"SFT vs \uc0ac\uc804\ud559\uc2b5(Pre-training)","text":"<ul> <li>\uc0ac\uc804\ud559\uc2b5: \ub300\uaddc\ubaa8 \ud14d\uc2a4\ud2b8 \ucf54\ud37c\uc2a4\uc5d0\uc11c \ub2e4\uc74c \ud1a0\ud070 \uc608\uce21\uc744 \ud1b5\ud574 \uc5b8\uc5b4 \uc774\ud574 \ub2a5\ub825 \uc2b5\ub4dd</li> <li>SFT: \uc9c0\uc2dc-\uc751\ub2f5 \uc30d\uc744 \ud1b5\ud574 \ud2b9\uc815 \uc791\uc5c5 \uc218\ud589 \ubc0f \uc0ac\uc6a9\uc790 \uc758\ub3c4 \uc774\ud574 \ub2a5\ub825 \ud5a5\uc0c1</li> <li>\ucc28\uc774\uc810: \uc0ac\uc804\ud559\uc2b5\uc740 \uc77c\ubc18\uc801 \uc5b8\uc5b4 \ub2a5\ub825 \uac1c\ubc1c, SFT\ub294 \ud2b9\uc815 \uc791\uc5c5 \ubc0f \ud615\uc2dd\uc5d0 \ub9de\ub294 \uc751\ub2f5 \uc0dd\uc131\uc5d0 \ucd08\uc810</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft-vs-cpt","title":"SFT vs \uc5f0\uc18d \uc0ac\uc804\ud559\uc2b5(CPT)","text":"<ul> <li>\uc5f0\uc18d \uc0ac\uc804\ud559\uc2b5: \ucd94\uac00 \ucf54\ud37c\uc2a4\ub85c \uae30\uc874 \uc0ac\uc804\ud559\uc2b5\uc744 \uacc4\uc18d\ud558\uc5ec \uc9c0\uc2dd \ud655\uc7a5/\uc5c5\ub370\uc774\ud2b8</li> <li>SFT: \uc9c0\ub3c4 \ud559\uc2b5\uc744 \ud1b5\ud574 \ubaa8\ub378 \ud589\ub3d9 \uad50\uc815 \ubc0f \ud2b9\uc815 \uc791\uc5c5 \uc218\ud589 \ub2a5\ub825 \ud5a5\uc0c1</li> <li>\ucc28\uc774\uc810: CPT\ub294 \uc9c0\uc2dd \ud655\uc7a5\uc5d0 \uc911\uc810, SFT\ub294 \ud589\ub3d9 \uc870\uc815\uc5d0 \uc911\uc810</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft-vs-rl","title":"SFT vs \uac15\ud654\ud559\uc2b5(RL)","text":"<ul> <li>SFT: \uc9c1\uc811\uc801\uc778 \uc9c0\ub3c4 \ud559\uc2b5\uc73c\ub85c \uc815\ub2f5 \uc608\uc2dc\ub97c \ubaa8\ubc29\ud558\ub3c4\ub85d \ud6c8\ub828</li> <li>RL: \ubcf4\uc0c1 \ud568\uc218\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 \ub354 \ub098\uc740 \uc751\ub2f5\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \uac04\uc811\uc801\uc73c\ub85c \uc720\ub3c4</li> <li>\uad00\uacc4: SFT\ub294 \uc885\uc885 RL\uc758 \ucd08\uae30 \uc815\ucc45 \ubaa8\ub378\ub85c \uc0ac\uc6a9\ub428 (\uc608: RLHF\uc5d0\uc11c SFT \ubaa8\ub378\uc774 \uae30\ubc18)</li> <li>\ud6a8\uacfc: \ucd5c\uadfc \uc5f0\uad6c\uc5d0 \ub530\ub974\uba74 \uace0\ud488\uc9c8 SFT\ub9cc\uc73c\ub85c\ub3c4 RL \uc218\uc900\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud560 \uc218 \uc788\uc74c</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_6","title":"SFT\uc758 \ud55c\uacc4\uc640 \uace0\ub824\uc0ac\ud56d","text":""},{"location":"tuning_techniques/supervised_finetuning/#_9","title":"\ub370\uc774\ud130 \uc758\uc874\uc131","text":"<ul> <li>SFT \uc131\ub2a5\uc740 \ud6c8\ub828 \ub370\uc774\ud130\uc758 \ud488\uc9c8\uacfc \ub2e4\uc591\uc131\uc5d0 \ud06c\uac8c \uc758\uc874\ud569\ub2c8\ub2e4.</li> <li>\ud3b8\ud5a5\ub418\uac70\ub098 \ubd80\uc815\ud655\ud55c \ub370\uc774\ud130\ub294 \ubaa8\ub378\uc758 \ucd9c\ub825\uc5d0\ub3c4 \ubc18\uc601\ub429\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_10","title":"\uacfc\uc801\ud569 \uc704\ud5d8","text":"<ul> <li>\ud2b9\ud788 \uc18c\ub7c9\uc758 \ub370\uc774\ud130\ub85c \ud559\uc2b5\ud560 \ub54c \uacfc\uc801\ud569\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ubaa8\ub378\uc774 \ud6c8\ub828 \ub370\uc774\ud130\uc758 \ud328\ud134\ub9cc \uc554\uae30\ud558\uace0 \uc0c8\ub85c\uc6b4 \uc785\ub825\uc5d0 \uc77c\ubc18\ud654\ud558\uc9c0 \ubabb\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_11","title":"\uc9c0\uc2dd \ub9dd\uac01","text":"<ul> <li>SFT \uacfc\uc815\uc5d0\uc11c \ubaa8\ub378\uc774 \uc0ac\uc804\ud559\uc2b5 \ub2e8\uacc4\uc5d0\uc11c \uc2b5\ub4dd\ud55c \uc77c\ubd80 \uc9c0\uc2dd\uc744 \uc78a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\ud2b9\ud788 \uacf5\uaca9\uc801\uc778 \ud559\uc2b5\ub960\uc774\ub098 \uae34 \ud559\uc2b5 \uae30\uac04\uc740 \uc774 \ubb38\uc81c\ub97c \uc545\ud654\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_12","title":"\ud3c9\uac00\uc758 \uc5b4\ub824\uc6c0","text":"<ul> <li>\uc0dd\uc131 \ubaa8\ub378\uc758 \ucd9c\ub825 \ud488\uc9c8\uc744 \uc790\ub3d9\uc73c\ub85c \ud3c9\uac00\ud558\uae30 \uc5b4\ub835\uc2b5\ub2c8\ub2e4.</li> <li>\uc778\uac04 \ud3c9\uac00\ub294 \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4e4\uace0 \uc8fc\uad00\uc801\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_7","title":"SFT \uc131\uacf5 \uc0ac\ub840","text":""},{"location":"tuning_techniques/supervised_finetuning/#stanford-alpaca-gpt-35","title":"Stanford Alpaca: \uc800\ube44\uc6a9\uc73c\ub85c GPT-3.5 \uc218\uc900 \ub2ec\uc131","text":"<ul> <li>\ubaa8\ub378 \uae30\ubc18: Meta\uc758 LLaMA 7B</li> <li>\ub370\uc774\ud130: GPT-3.5(text-davinci-003)\ub85c \uc0dd\uc131\ud55c 52,000\uac1c\uc758 \uc9c0\uc2dc-\uc751\ub2f5 \uc30d</li> <li>\ube44\uc6a9: \uc57d $500 (OpenAI API \uc0ac\uc6a9)</li> <li>\uc131\uacfc: \ube14\ub77c\uc778\ub4dc \ud3c9\uac00\uc5d0\uc11c GPT-3.5\uc640 \uc720\uc0ac\ud55c \uc131\ub2a5 \ub2ec\uc131 (90:89\ub85c \uadfc\uc18c\ud558\uac8c \uc55e\uc12c)</li> <li>\uc758\uc758: \uc18c\uaddc\ubaa8 \uacf5\uac1c \ubaa8\ub378\ub3c4 \uc801\uc808\ud55c SFT\ub85c \uc0c1\uc6a9 AI \uc218\uc900\uc5d0 \uadfc\uc811\ud560 \uc218 \uc788\uc74c\uc744 \uc99d\uba85</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#vicuna","title":"Vicuna: \uc2e4\uc81c \uc0ac\uc6a9\uc790 \ub300\ud654\ub85c \ud559\uc2b5\ud55c \uc624\ud508\uc18c\uc2a4 \ucc57\ubd07","text":"<ul> <li>\ubaa8\ub378 \uae30\ubc18: LLaMA 13B</li> <li>\ub370\uc774\ud130: ShareGPT\uc5d0\uc11c \uc218\uc9d1\ud55c 70,000\uac74\uc758 \uc0ac\uc6a9\uc790-ChatGPT \ub300\ud654</li> <li>\ube44\uc6a9: \uc57d $300</li> <li>\uc131\uacfc: GPT-4 \ud3c9\uac00\uc5d0\uc11c ChatGPT \ud488\uc9c8\uc758 90% \uc774\uc0c1 \ub2ec\uc131</li> <li>\uc758\uc758: \uc2e4\uc81c \uc0ac\uc6a9\uc790 \ub300\ud654 \ub370\uc774\ud130\ub85c SFT\ud558\uc5ec \ub2e4\uc911 \ud134 \ub300\ud654\uc5d0 \uac15\ud55c \uc624\ud508\uc18c\uc2a4 \ucc57\ubd07 \uac1c\ubc1c</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#microsoft-orca-gpt-4","title":"Microsoft Orca: GPT-4 \ucd94\ub860 \uacfc\uc815 \ubaa8\ubc29","text":"<ul> <li>\ubaa8\ub378 \uae30\ubc18: 13B \uaddc\ubaa8 \ubaa8\ub378</li> <li>\ub370\uc774\ud130: GPT-4\uac00 \uc0dd\uc131\ud55c \ub2e8\uacc4\ubcc4 \uc124\uba85(trace)\uacfc \uc0ac\uace0 \uacfc\uc815</li> <li>\uc131\uacfc:</li> <li>\ub3d9\uc77c \ud06c\uae30 \ubaa8\ub378(Vicuna-13B) \ub300\ube44 \ubcf5\uc7a1\ud55c \ucd94\ub860 \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 2\ubc30 \uc774\uc0c1 \uc131\ub2a5 \ud5a5\uc0c1</li> <li>Big-Bench Hard(BBH)\uc5d0\uc11c ChatGPT\uc640 \uac70\uc758 \ub300\ub4f1\ud55c \uc131\ub2a5</li> <li>AGIEval \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c \uc2dc\uc2a4\ud15c \ud504\ub86c\ud504\ud2b8 \ucd5c\uc801\ud654\ub41c ChatGPT\uc5d0 \uadfc\uc811</li> <li>\uc758\uc758: \ub300\ud615 \ubaa8\ub378\uc758 \uc0ac\uace0 \uacfc\uc815\uc744 \ubaa8\ubc29 \ud559\uc2b5\ud558\ub294 \ubc29\uc2dd\uc774 \uc791\uc740 \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \ud5a5\uc0c1\uc5d0 \ud6a8\uacfc\uc801\uc784\uc744 \uc785\uc99d</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#deepseek-r1","title":"DeepSeek R1 \uc9c0\uc2dd \uc99d\ub958: \ucd08\uac70\ub300 \ubaa8\ub378\uc758 \ucd94\ub860 \ub2a5\ub825 \uc804\uc218","text":"<ul> <li>\uc811\uadfc\ubc95: \ucd08\uac70\ub300 \uc5b8\uc5b4\ubaa8\ub378(DeepSeek R1)\uc744 \uad50\uc0ac\ub85c \ud65c\uc6a9\ud55c \uc9c0\uc2dd \uc99d\ub958</li> <li>\ud2b9\uc9d5: \ubcf5\uc7a1\ud55c \ubb38\uc81c\uc5d0 \ub300\ud55c \ub2e8\uacc4\ubcc4 \ucd94\ub860 \uacfc\uc815(Chain-of-Thought)\uc744 \ud3ec\ud568\ud55c \ud569\uc131 \ub370\uc774\ud130 \uc0dd\uc131</li> <li>\uc131\uacfc:</li> <li>LLaMA \uae30\ubc18 8B \ubaa8\ub378 \uc2e4\ud5d8\uc5d0\uc11c \ucd5c\uc885 \ub2f5\ub9cc \ud559\uc2b5 \uc2dc 29%, \uc778\uac04 \uc804\ubb38\uac00 \ud480\uc774\uacfc\uc815 \ud559\uc2b5 \uc2dc 68%, DeepSeek R1 \ud480\uc774\uacfc\uc815 \ud559\uc2b5 \uc2dc 87%\uc758 \uc815\ud655\ub3c4 \ub2ec\uc131</li> <li>\uc778\uac04 \uc804\ubb38\uac00\uac00 \uc791\uc131\ud55c \ud574\uc124\ubcf4\ub2e4 DeepSeek R1\uc774 \uc0dd\uc131\ud55c \ud574\uc124 \ub370\uc774\ud130\uac00 \ub354 \ud6a8\uacfc\uc801</li> <li>\uc758\uc758: \ub300\ud615 \ubaa8\ub378\uc758 \uc9c0\uc2dd\uc744 \uc791\uc740 \ubaa8\ub378\uc5d0 \ud6a8\uacfc\uc801\uc73c\ub85c \uc774\uc804\ud558\ub294 \uc0c8\ub85c\uc6b4 SFT \ubc29\ud5a5 \uc81c\uc2dc</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#sft_8","title":"\ub3c4\uba54\uc778 \ud2b9\ud654 SFT \uc131\uacf5 \uc0ac\ub840","text":""},{"location":"tuning_techniques/supervised_finetuning/#wizardcoder","title":"WizardCoder: \ucf54\ub4dc \uc0dd\uc131 \ud2b9\ud654 \ubaa8\ub378","text":"<ul> <li>\ubaa8\ub378 \uae30\ubc18: \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \ucf54\ub4dc \uc5b8\uc5b4 \ubaa8\ub378(15B, 34B \ub4f1)</li> <li>\ub370\uc774\ud130: \uace0\ud488\uc9c8 \ubb38\uc81c-\ud574\uacb0 \uc608\uc2dc\ub97c \uc99d\uac15(Evol-Instruct)\ud55c \ub370\uc774\ud130</li> <li>\uc131\uacfc:</li> <li>WizardCoder-15B: HumanEval \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c 57.3% \uc815\ub2f5\ub960(pass@1)</li> <li>WizardCoder-34B: 73% \uc774\uc0c1\uc758 \uc815\ub2f5\ub960\ub85c GPT-4(2023\ub144 3\uc6d4 \ubc84\uc804) \uc131\ub2a5 \ucd08\uacfc</li> <li>WizardCoder-33B v1.1: 79.9% \uc815\ub2f5\ub960\ub85c ChatGPT-3.5(72.6%) \ub2a5\uac00</li> <li>\uc758\uc758: \uc81c\ud55c\ub41c \ud30c\ub77c\ubbf8\ud130\ub85c\ub3c4 \ucd5c\uc801\uc758 \ub370\uc774\ud130\uc640 \uae30\ubc95\uc744 \uc801\uc6a9\ud558\uba74 \uc0c1\uc6a9 AI \uc218\uc900\uc758 \ucf54\ub529 \uc131\ub2a5 \ub2ec\uc131 \uac00\ub2a5</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#wizardmath","title":"WizardMath: \uc218\ud559 \ubb38\uc81c \ud574\uacb0 \ud2b9\ud654 \ubaa8\ub378","text":"<ul> <li>\ubaa8\ub378 \uae30\ubc18: Meta\uc758 LLaMA-2 \ubaa8\ub378\ub4e4</li> <li>\ub370\uc774\ud130: \uace0\ub09c\ub3c4 \uc218\ud559 \ubb38\uc81c\ub97c \ub2e8\uacc4\ubcc4\ub85c \ud480\uc774\ud558\ub294 \ub370\uc774\ud130</li> <li>\uae30\ubc95: \uac15\ud654 \ud559\uc2b5(RL)\uc744 \ubcd1\ud569\ud55c \ub3c5\uc790 \uae30\ubc95(RLEIF)</li> <li>\uc131\uacfc:</li> <li>WizardMath-13B: MATH \ubca4\uce58\ub9c8\ud06c\uc5d0\uc11c LLaMA2-70B\ubcf4\ub2e4 9.2%p \ub192\uc740 \uc810\uc218</li> <li>WizardMath-70B: ChatGPT-3.5, Claude Instant \ub4f1\uc744 \ub6f0\uc5b4\ub118\ub294 \uc218\ud559 \uc131\ub2a5</li> <li>WizardMath-7B: \uac19\uc740 7B\uae09\uc5d0\uc11c \ub3c5\ubcf4\uc801\uc778 \uc218\ud559 \uc2e4\ub825(GSM8K 83.2% \ub4f1)</li> <li>\uc758\uc758: \uc218\ud559\uc801 \uc0ac\uace0 \uacfc\uc815\uc744 \uc9d1\uc911\uc801\uc73c\ub85c \ud6c8\ub828\uc2dc\ud0a4\ub294 SFT\uac00 \ubaa8\ub378\uc758 \ubb38\uc81c \ud574\uacb0 \ub2a5\ub825\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \uc99d\uba85</li> </ul>"},{"location":"tuning_techniques/supervised_finetuning/#_13","title":"\ucc38\uace0\ubb38\ud5cc","text":"<ol> <li> <p>Stanford Alpaca: \"Alpaca: A Strong, Replicable Instruction-Following Model\", Stanford CRFM, 2023.</p> </li> <li> <p>Vicuna: \"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality\", LMSYS Org, 2023.</p> </li> <li> <p>Microsoft Orca: \"Orca: Progressive Learning from Complex Explanation Traces of GPT-4\", Microsoft Research, 2023.</p> </li> <li> <p>QLoRA &amp; Guanaco: \"QLoRA: Efficient Finetuning of Quantized LLMs\", Dettmers et al., 2023.</p> </li> <li> <p>DeepSeek R1: \"Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\", DeepSeek AI, 2024.</p> </li> <li> <p>DeepSeek R1 Distillation: \"Distillation with Reasoning: Can DeepSeek R1 Teach Better Than Humans?\", Fireworks AI, 2024.</p> </li> <li> <p>WizardCoder: \"WizardCoder: Empowering Code Large Language Models with Evol-Instruct\", Xu et al., 2023.</p> </li> <li> <p>WizardMath: \"WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\", Luo et al., 2023.</p> </li> <li> <p>\"What is supervised fine-tuning in LLMs? Unveiling the process\", Nebius, 2023.</p> </li> <li> <p>\"LLM continuous self-instruct fine-tuning framework powered by a compound AI system on Amazon SageMaker\", AWS Machine Learning Blog, 2023.</p> </li> </ol>"}]}