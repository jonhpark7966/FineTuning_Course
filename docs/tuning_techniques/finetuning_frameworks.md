# Supervised Fine-Tuning (SFT)

## SFT ê°œìš”
- SFTì˜ ëª©ì ê³¼ ì¤‘ìš”ì„±
- ì…ë ¥-ì¶œë ¥ ë§¤í•‘ì„ í†µí•œ í•™ìŠµ

## SFT ë°ì´í„° ì¤€ë¹„
- ì§€ì‹œ-ì‘ë‹µ ìŒ êµ¬ì„±
- ë‹¤ì–‘í•œ ì§€ì‹œ í˜•íƒœ í¬í•¨í•˜ê¸°
- í’ˆì§ˆ ì¤‘ì‹¬ ë°ì´í„° íë ˆì´ì…˜

## SFT ìµœì í™” ì „ëµ
- ê³¼ì í•© ë°©ì§€ ê¸°ë²•
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
- ì¡°ê¸° ì¢…ë£Œ ì „ëµ 




# ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸(LLM) ì§€ë„ íŒŒì¸íŠœë‹: Hugging Face vs. DeepSpeed vs. Unsloth

## ê°œìš” (Introduction)
ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸(LLM)ì˜ **ì§€ë„ í•™ìŠµ ê¸°ë°˜ íŒŒì¸íŠœë‹**(Supervised Fine-Tuning, SFT)ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì— ë§ì¶° ë¯¸ì„¸ì¡°ì •í•˜ì—¬ íŠ¹ì • ì‘ì—… ì„±ëŠ¥ì´ë‚˜ ì‘ë‹µ í’ˆì§ˆì„ ë†’ì´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. íŠ¹íˆ **Decoder-Only Transformer** êµ¬ì¡°(ì˜ˆ: GPT ê³„ì—´ ëª¨ë¸)ì˜ íŒŒì¸íŠœë‹ì€ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ì— ì´ì–´ì§€ëŠ” ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” í˜•íƒœë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ìµœê·¼ 2ë…„ê°„ LLM íŒŒì¸íŠœë‹ ë¶„ì•¼ì—ì„œëŠ” **ëª¨ë¸ í¬ê¸°ì— ë¹„í•´ í•œì •ëœ ìì›ìœ¼ë¡œë„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ**í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ê³¼ ë„êµ¬ë“¤ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œì—ì„œëŠ” Hugging Face ìƒíƒœê³„, Microsoft DeepSpeed, ê·¸ë¦¬ê³  ìµœì‹  ì»¤ë®¤ë‹ˆí‹° íˆ´ì¸ Unslothë¥¼ í™œìš©í•œ **ì§€ë„ íŒŒì¸íŠœë‹ ë°©ë²•**ì„ ë¹„êµí•©ë‹ˆë‹¤. ë˜í•œ ê° ì ‘ê·¼ë²•ì˜ **íŠ¹ì§•ê³¼ ì¥ì **, **ìµœì‹  ì—°êµ¬ ë™í–¥**, **ì‹¤ë¬´ ì ìš© ì˜ˆì œ ì½”ë“œ**, **ì„±ëŠ¥ ë° íš¨ìœ¨ í‰ê°€ ê¸°ì¤€**, **Decoder-Only ìµœì í™” ê¸°ë²•** ë“±ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

## Hugging Face ê¸°ë°˜ LLM íŒŒì¸íŠœë‹
**Hugging Face**ì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë°©ëŒ€í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì €ì¥ì†Œì™€ í¸ë¦¬í•œ APIë¥¼ ì œê³µí•˜ì—¬ LLM íŒŒì¸íŠœë‹ì„ ì†ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. PyTorch ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ëœ `Trainer` í´ë˜ìŠ¤ ë˜ëŠ” **ğŸ¤— Accelerate**ë¥¼ í†µí•´ ë‹¨ì¼ GPUë¶€í„° ë¶„ì‚° GPUê¹Œì§€ **ì†ì‰¬ìš´ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì„±**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. Hugging Faceì˜ ì£¼ìš” ê°•ì ì€ **ê´‘ë²”ìœ„í•œ ëª¨ë¸ ì§€ì›**ê³¼ **ì»¤ë®¤ë‹ˆí‹° ì¤‘ì‹¬ì˜ ì‹ ì†í•œ ê°œì„ **ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 2022ë…„ ë°œí‘œëœ **LoRA**(Low-Rank Adaptation) ê¸° ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper))ã€‘ì„ ë¹ ë¥´ê²Œ PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì— í†µí•©í•˜ê³ , 2023ë…„ ë“±ì¥í•œ **QLoRA** ë°©ë²•ë¡ ë„ ê³§ë°”ë¡œ ì§€ì›í•˜ì˜€ìŠµë‹ˆ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=LLMs%20are%20known%20to%20be,the%20QLoRA%20paper%20by%20Dettmers))ã€‘. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ìµœì†Œí•œì˜ ì½”ë“œ ë³€ê²½ë§Œìœ¼ë¡œ ìµœì‹  ì—°êµ¬ ì„±ê³¼ë¥¼ ì‹¤ìŠµì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Hugging FaceëŠ” **ë©”ëª¨ë¦¬ ìµœì í™”**ë¥¼ ìœ„í•´ 8-bit ë° 4-bit ì–‘ìí™”(qunatization)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `transformers`ì—ì„œ `from_pretrained` í˜¸ì¶œ ì‹œ `load_in_4bit=True`ë¡œ ì„¤ì •í•˜ë©´, ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=As%20a%20quickstart%2C%20load%20a,0)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=The%20basic%20way%20to%20load,that%20will%20be%20automatically%20inferred))ã€‘. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆì–´, ë¹„êµì  **ì ì€ GPU ë©”ëª¨ë¦¬ë¡œë„ ëŒ€í˜• ëª¨ë¸ì„ ë‹¤ë£° ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤* ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU))ã€‘. ì•„ë˜ ì˜ˆì‹œëŠ” OPT-350M ëª¨ë¸ì„ 4ë¹„íŠ¸ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-350m",
    load_in_4bit=True,  # 4ë¹„íŠ¸ ì–‘ìí™” ë¡œë“œ
    device_map="auto"   # ê°€ìš© GPU ìë™í• ë‹¹
)
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
```

ë˜í•œ Hugging Face PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ **LoRA ì–´ëŒ‘í„°**ë¥¼ ì†ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LoRAëŠ” ëª¨ë¸ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ë¯¸ì„¸ì¡°ì •í•˜ëŠ” ëŒ€ì‹ , **ì¼ë¶€ ë§¤íŠ¸ë¦­ìŠ¤ì— ì†Œê·œëª¨ì˜ í•™ìŠµê°€ëŠ¥í•œ ì €ë­í¬ í–‰ë ¬**(Adapters)ì„ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper))ã€‘. Hugging FaceëŠ” `peft.LoraConfig`ì™€ `get_peft_model` ë“±ì„ í†µí•´ ê¸°ì¡´ ëª¨ë¸ì— LoRA ëª¨ë“ˆì„ ì‚½ì…í•  ìˆ˜ ìˆëŠ” APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. LoRAë¥¼ ì‚¬ìš©í•˜ë©´ íŒŒì¸íŠœë‹ì‹œ **ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ëŸ‰ì„ í¬ê²Œ ì ˆê°**í•˜ë©´ì„œë„ ì›ë˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê±°ì˜ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2023ë…„ ì œì•ˆëœ **QLoRA**ëŠ” ì´ë¥¼ í•œ ë‹¨ê³„ ë°œì „ì‹œì¼œ **ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ 4-bitë¡œ ê³ ì •**í•˜ê³  LoRAë¡œë§Œ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, **65ì–µ~130ì–µê¸‰ ëª¨ë¸ë„ ë‹¨ì¼ GPUë¡œ ë¯¸ì„¸ì¡°ì • ê°€ëŠ¥**í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU))ã€‘. ì‹¤ì œë¡œ QLoRAë¥¼ í†µí•´ **65B íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ 48GB VRAMì˜ ë‹¨ì¼ GPUì—ì„œ í’€ 16ë¹„íŠ¸ íŒŒì¸íŠœë‹ê³¼ ë™ë“±í•œ ì„±ëŠ¥ìœ¼ë¡œ í•™ìŠµ**í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆ ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=enough%20to%20finetune%20a%2065B,we%20name%20Guanaco%2C%20outperforms%20all)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=QLoRA%20tuning%20is%20shown%20to,the%20power%20of%20QLoRA%20tuning))ã€‘. ì´ëŠ” **GPU 1ëŒ€ì—ì„œ 780GB ë©”ëª¨ë¦¬ê°€ í•„ìš”í–ˆë˜ ì‘ì—…ì„ 48GBë¡œ ì¤„ì¸ ì„±ê³¼**ë¡œ, ëŒ€ê·œëª¨ ëª¨ë¸ íŒŒì¸íŠœë‹ì˜ **ì ‘ê·¼ì„±ì„ í˜ì‹ ì ìœ¼ë¡œ í–¥ìƒ**ì‹œì¼°ìŠµë‹ˆ ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv](https://ar5iv.labs.arxiv.org/html/2305.14314#:~:text=ar5iv%20ar5iv,finetunable%20on%20a%20single%20GPU))ã€‘. QLoRAì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **NF4 (4-bit NormalFloat) ì–‘ìí™”**ì™€ **ì´ì¤‘ ì–‘ìí™”(Double Quantization)**, ê·¸ë¦¬ê³  **Paged Optimizer** ë“±ì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ ì—†ì´ ë©”ëª¨ë¦¬ë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ ì•„ë‚€ ê²ƒì…ë‹ˆ ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,new%20data%20type%20that%20is)) ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,of))ã€‘.

Hugging Face ë°©ë²•ì˜ ì¥ì ì€ **ê°„í¸í•¨ê³¼ ë²”ìš©ì„±**ì…ë‹ˆë‹¤. ë°©ëŒ€í•œ ì‚¬ì „í•™ìŠµ **ì²´í¬í¬ì¸íŠ¸ë¥¼ Hugging Face Hubì—ì„œ ì¦‰ì‹œ ë¶ˆëŸ¬ì™€** í™œìš©í•  ìˆ˜ ìˆê³ , **ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° í‰ê°€ê¹Œì§€ í†µí•©ëœ ìƒíƒœê³„**(ğŸ¤— Datasets ë“±)ë¥¼ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ **Transformer-Decorder ëª¨ë¸**(ì˜ˆ: GPT-2, GPT-3, LLaMA ë“±)ì˜ **í…ìŠ¤íŠ¸ ìƒì„± íƒœìŠ¤í¬**ë¥¼ ìœ„í•œ íŒŒì¸íŠœë‹ ì˜ˆì œê°€ í’ë¶€í•˜ë©°, í•™ìŠµ loop, í† í¬ë‚˜ì´ì €, ëª¨ë¸ ë³‘ë ¬í™” ë“±ì´ ì˜ ì¶”ìƒí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ `Trainer`ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ ì½”ë“œ (ì˜ˆ: GPT-2ë¥¼ í…ìŠ¤íŠ¸ ìƒì„± ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹) ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="output",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    fp16=True,                      # FP16 í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
    logging_steps=100,
    save_steps=500,
    deepspeed=None                  # (DeepSpeed ì‚¬ìš©ì‹œ ì„¤ì • íŒŒì¼ ê²½ë¡œ ì§€ì •)
)
trainer = Trainer(model=model, args=training_args, 
                  train_dataset=train_ds, eval_dataset=eval_ds, 
                  data_collator=data_collator)
trainer.train()
```

> **ì°¸ê³ :** ìƒê¸° ì½”ë“œì—ì„œ `deepspeed=None`ë¡œ ë‘ë©´ Hugging Faceì˜ ê¸°ë³¸ Trainerë¡œ í•™ìŠµí•©ë‹ˆë‹¤. DeepSpeedë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `deepspeed="ds_config.json"`ì²˜ëŸ¼ ì„¤ì • íŒŒì¼ì„ ì§€ì •í•˜ê±°ë‚˜ ğŸ¤— Accelerateë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì•„ë˜ DeepSpeed ì„¹ì…˜ ì°¸ê³ ).

Hugging Face ê¸°ë°˜ ì ‘ê·¼ì˜ ë‹¨ì ì´ë¼ë©´, **ì•„ì£¼ í° ëª¨ë¸ì„ ë‹¤ë£° ë•ŒëŠ” ê¸°ë³¸ í™˜ê²½ìœ¼ë¡œëŠ” í•œê³„**ê°€ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìˆ˜ì‹­ì–µ~ìˆ˜ë°±ì–µ íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ì „ì²´ ë¯¸ì„¸ì¡°ì •(full fine-tuning)í•˜ë ¤ë©´ ë©€í‹° GPUê°€ í•„ìˆ˜ì´ë©°, ì´ ê²½ìš° **DeepSpeedë‚˜ FSDP** ë“±ì˜ ë³´ì¡°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¤‘ì†Œê·œëª¨ ëª¨ë¸ì´ë‚˜ LoRAê°™ì€ íŒŒë¼ë¯¸í„° íš¨ìœ¨ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ë©´ Hugging Faceë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ì‹¤í—˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Hugging FaceëŠ” **ì—°êµ¬ ê°œë°œì˜ ì¶œë°œì **ìœ¼ë¡œì„œ ìµœì‹  ê¸°ë²•ë“¤ì„ ë¹ ë¥´ê²Œ ë°›ì•„ë“¤ì´ê³  ìˆì–´, **ì‹¤ë¬´ì—ì„œë„ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” LLM íŒŒì¸íŠœë‹ í”Œë«í¼**ì…ë‹ˆë‹¤.

## DeepSpeedë¥¼ í™œìš©í•œ ëŒ€ê·œëª¨ ëª¨ë¸ íŒŒì¸íŠœë‹
**DeepSpeed**ëŠ” ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ê°œë°œí•œ **ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¡œ, íŠ¹íˆ ê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì˜ í•™ìŠµì„ **ì†ë„ì™€ ìŠ¤ì¼€ì¼** ì¸¡ë©´ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤. DeepSpeedì˜ í•µì‹¬ì—ëŠ” **ZeRO (Zero Redundancy Optimizer)** ì•Œê³ ë¦¬ì¦˜ì´ ìˆ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))10ã€‘. ZeROëŠ” ë°ì´í„° ë³‘ë ¬ í•™ìŠµ ì‹œ ê° GPUì— ë™ì¼í•˜ê²Œ ë³µì œë˜ë˜ **ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ê·¸ë˜ë””ì–¸íŠ¸, ëª¨ë¸ íŒŒë¼ë¯¸í„°**ë¥¼ shard(ë¶„í• )í•˜ì—¬ **GPUë“¤ ê°„ì— ë¶„ì‚° ì €ì¥** ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))10ã€‘. ì´ë ‡ê²Œ í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì¡ì•„ë¨¹ëŠ” ìš”ì†Œê°€ ì‚¬ë¼ì ¸, **ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì ¸ë„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì‚°**ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ZeROëŠ” ë‹¨ê³„ë³„ë¡œ ë°œì „ë˜ì–´ **Stage 1**(ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¶„ì‚°), **Stage 2**(+ gradient ë¶„ì‚°), **Stage 3**(+ íŒŒë¼ë¯¸í„° ìì²´ ë¶„ì‚°)ìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, Stage ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ GPU ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ê°ì†Œ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))10ã€‘. íŠ¹íˆ **ZeRO-3**ëŠ” ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë“  GPUì— ë‚˜ëˆ„ì–´ ì˜¬ë ¤ë†“ê³  í•„ìš” ì‹œ ë™ì ìœ¼ë¡œ ë¶ˆëŸ¬ì“°ëŠ” ë°©ì‹ìœ¼ë¡œ, **ê°œë³„ GPUì—ëŠ” ì „ì²´ ëª¨ë¸ì˜ ì¼ë¶€ë§Œ ìƒì£¼**í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì˜ˆë¥¼ ë“¤ì–´ **70ì–µ~130ì–µ ê°œ íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ë‹¨ì¼ ë˜ëŠ” ì†Œìˆ˜ GPUì—ì„œ í•™ìŠµ**ì‹œí‚¤ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì¡Œ ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))09ã€‘. DeepSpeed íŒ€ì˜ íŠœí† ë¦¬ì–¼ì— ë”°ë¥´ë©´, ZeRO-Offload ê¸°ëŠ¥ê¹Œì§€ í™œìš©í•˜ë©´ **10ì–µ~13ì–µ íŒŒë¼ë¯¸í„° GPT-2 ëª¨ë¸ë„ ë‹¨ì¼ 32GB GPUì—ì„œ í•™ìŠµ**í•  ìˆ˜ ìˆ ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))09ã€‘. ì•„ë˜ëŠ” DeepSpeedì˜ ZeRO-Offloadë¥¼ í†µí•œ ë‹¨ì¼ GPU ëŒ€ìš©ëŸ‰ ëª¨ë¸ í•™ìŠµ ì‚¬ë¡€ì…ë‹ˆë‹¤:

- *â€œZeRO-OffloadëŠ” ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ì„ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬, ìµœëŒ€ 130ì–µ íŒŒë¼ë¯¸í„°ì— ë‹¬í•˜ëŠ” í° ëª¨ë¸ë„ ë‹¨ì¼ GPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•´ ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))09ã€‘.â€*

DeepSpeedì˜ ë˜ ë‹¤ë¥¸ ê°•ì ì€ **ë³‘ë ¬í™”ì™€ ìµœì í™” ì „ëµì˜ ë‹¤ì–‘ì„±**ì…ë‹ˆë‹¤. ëª¨ë¸ ë³‘ë ¬í™”, íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”, mixed precision ì—°ì‚°, Gradient Accumulation ë“±ì˜ ê¸°ë²•ì„ í†µí•©ì ìœ¼ë¡œ ì§€ì›í•˜ì—¬ **GPU ì—¬ëŸ¬ ëŒ€ë¥¼ ìµœëŒ€í•œ í™œìš©**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ **CPU Offloading**(ZeRO-Offload)ê³¼ **NVMe Offloading**(ZeRO-Infinity)ì„ í†µí•´, GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ê²½ìš° **ì¼ë¶€ ë°ì´í„°(ì˜ˆ: ëª¨ë¸ ê°€ì¤‘ì¹˜ë‚˜ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ)ë¥¼ CPU RAMì´ë‚˜ SSDë¡œ ë¶„ì‚°**ì‹œí‚´ìœ¼ë¡œì¨ **ì‚¬ì‹¤ìƒ ë¬´ì œí•œì— ê°€ê¹Œìš´ ëª¨ë¸ ì‚¬ì´ì¦ˆ**ê¹Œì§€ë„ í•™ìŠµì„ ì‹œë„í•  ìˆ˜ ìˆ ([ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/#:~:text=ZeRO,Infinity%20include)) ([ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/#:~:text=,efficiency%20and%20superlinear%20throughput%20scaling))94ã€‘. ì´ëŸ¬í•œ ê·¹ë‹¨ì  í™•ì¥ì„± ë•ë¶„ì— DeepSpeedëŠ” GPT-3(175B) ê¸‰ ëª¨ë¸ í•™ìŠµì´ë‚˜ ìˆ˜ì¡° ê°œ íŒŒë¼ë¯¸í„° ì‹¤í—˜ì²˜ëŸ¼ **ìµœì²¨ë‹¨ ìŠ¤ì¼€ì¼ì˜ ì—°êµ¬ì— í•„ìˆ˜ì ì¸ ë„êµ¬**ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.

DeepSpeedë¥¼ ì‹¤ë¬´ì— í™œìš©í•˜ë ¤ë©´ **ì„¤ì • íŒŒì¼ê³¼ ëŸ°ì²˜**ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. Hugging Face Trainerì—ë„ `deepspeed` ì¸ìë¥¼ í†µí•´ DeepSpeedë¥¼ í†µí•©í•  ìˆ˜ ìˆìœ¼ë©°, **ğŸ¤— Accelerate íˆ´ì„ ì“°ë©´ ëŒ€í™”í˜•ìœ¼ë¡œ ì„¤ì • íŒŒì¼ì„ ìƒì„±**í•  ìˆ˜  ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=Configuration)) ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=accelerate%20config%20))164ã€‘. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ DeepSpeed ì„¤ì •(`ds_config.json`)ì„ ì¤€ë¹„í•˜ì—¬ Trainerì— ì „ë‹¬í•˜ë©´ ZeRO ê¸°ë°˜ í›ˆë ¨ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤:

```json
{
  "zero_optimization": {
    "stage": 3,
    "offload_param": {
      "device": "cpu"
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

ìœ„ ì„¤ì •ì€ ZeRO-3 ë‹¨ê³„ì—ì„œ **ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œ**í•˜ë„ë¡ ì§€ì •í•œ ì˜ˆì‹œì…ë‹ˆë‹¤. `TrainingArguments(..., deepspeed="ds_config.json")` ì²˜ëŸ¼ ì„¤ì •í•˜ë©´ Hugging Face Trainerê°€ ë‚´ë¶€ì ìœ¼ë¡œ DeepSpeed ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ë˜ëŠ” **`deepspeed.init` APIë¥¼ ì§ì ‘ ì‚¬ìš©**í•´ ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €ë¥¼ ê°ì‹¼ ë’¤ `deepspeed.run`ìœ¼ë¡œ í›ˆë ¨ loopì„ êµ¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë°©ë²•ì´ë“ , **ê¸°ì¡´ PyTorch ì½”ë“œë¥¼ í¬ê²Œ ë³€ê²½í•˜ì§€ ì•Šìœ¼ë©´ì„œ** DeepSpeedì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì¥ì ì…ë‹ˆë‹¤.

DeepSpeedì™€ Hugging Face PEFTë¥¼ **ì¡°í•©í•˜ì—¬ ì‚¬ìš©**í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ **LoRA ì ìš© ëª¨ë¸ì„ DeepSpeed ZeRO-3ë¡œ ë¶„ì‚° í•™ìŠµ**í•˜ê±°ë‚˜, **QLoRA(4ë¹„íŠ¸ + LoRA)**ë¥¼ DeepSpeedì™€ í•¨ê»˜ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ GPUì—ì„œ ì´ˆê±°ëŒ€ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ì‹¤í—˜ë“¤ì´ ë³´ê³ ë˜ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=For%20DeepSpeed%20Stage%203%20%2B,models%20on%20multiple%20GPUs%20below)) ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=This%20section%20of%20guide%20will,by%20changing%20the%20accelerate%20config))142ã€‘. Hugging Face ê°€ì´ë“œì—ì„œëŠ” **8x H100 (80GB) GPUë¡œ LLaMA-70B ëª¨ë¸ì„ LoRA+ZeRO-3 ì„¤ì •ìœ¼ë¡œ SFT(ì§€ë„íŒŒì¸íŠœë‹)í•˜ëŠ” ì˜ˆì‹œ**ë¥¼ ì œê³µí•˜ê³   ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=This%20section%20of%20guide%20will,by%20changing%20the%20accelerate%20config)) ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=accelerate%20config%20))164ã€‘. ì´ì²˜ëŸ¼ DeepSpeedëŠ” Hugging Face ìƒíƒœê³„ì™€ë„ ì˜ ë§ë¬¼ë ¤ ë™ì‘í•˜ë©°, íŒŒì¸íŠœë‹ **ì†ë„ ë° í™•ì¥ì„±**ì„ ë†’ì´ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì •ë¦¬í•˜ë©´, DeepSpeedì˜ íŠ¹ì§•ê³¼ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ZeRO ì•Œê³ ë¦¬ì¦˜**ì„ í†µí•œ **ë©”ëª¨ë¦¬ ìµœì í™” ë° ëª¨ë¸ ë¶„ì‚°**: ë™ì¼ ìì›ìœ¼ë¡œ ë” í° ëª¨ë¸ í•™ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))110ã€‘ 
- **CPU/NVMe ì˜¤í”„ë¡œë“œ**ë¡œ ë‹¨ì¼ GPU ë©”ëª¨ë¦¬ í•œê³„ ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))109ã€‘ 
- **ê³ ë„í™”ëœ ë¶„ì‚° ë³‘ë ¬ í•™ìŠµ** ì§€ì›: ìˆ˜ì‹­~ìˆ˜ë°± GPUê¹Œì§€ íš¨ìœ¨ì  ìŠ¤ì¼€ì¼ ì•„ì›ƒ
- **ì„±ëŠ¥ ìµœì í™” ì»¤ë„** ì œê³µ: DeepSpeedì˜ CPU Adam ì˜µí‹°ë§ˆì´ì €ëŠ” ê¸°ë³¸ PyTorch ëŒ€ë¹„ 5~7 ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=For%20large%20model%20training%2C%20optimizers,please%20see%20our%20blog%20post))119ã€‘ ë“±

ë‹¨ì ìœ¼ë¡œëŠ” **í™˜ê²½ ì„¤ì •ì˜ ë³µì¡ì„±**ì´ ìˆìŠµë‹ˆë‹¤. ì„¤ì • íŒŒì¼ ì‘ì„±, ëŸ°ì²˜ ëª…ë ¹ ë“± ì²˜ìŒ ì‚¬ìš©ì‹œ ì§„ì…ì¥ë²½ì´ ìˆìœ¼ë©°, ì‘ì€ ê·œëª¨ ì‹¤í—˜ì—ëŠ” ê³¼í•œ ì¸¡ë©´ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ **ë™ì¼í•œ ì—°ì‚°ì´ë¼ë„ ì•½ê°„ì˜ ì˜¤ë²„í—¤ë“œ**(í†µì‹  ëŒ€ê¸° ë“±)ê°€ ì¡´ì¬í•˜ë¯€ë¡œ, ëª¨ë¸ì´ ì¶©ë¶„íˆ í¬ê±°ë‚˜ ë¶„ì‚°ì´ í•„ìš”í•œ ê²½ìš°ì— ê°€ì¥ í° íš¨ê³¼ë¥¼ ë´…ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„, **ì‹¤ë¬´ì—ì„œ ìˆ˜ì‹­ì–µ~ìˆ˜ì²œì–µ íŒŒë¼ë¯¸í„°** ëª¨ë¸ì„ ë‹¤ë¤„ì•¼ í•œë‹¤ë©´ DeepSpeedëŠ” ì‚¬ì‹¤ìƒ **í‘œì¤€ ë„êµ¬**ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.

## Unslothë¥¼ í™œìš©í•œ ê³ ì† LLM íŒŒì¸íŠœë‹
**Unsloth**ëŠ” 2023ë…„ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë“±ì¥í•œ **ê²½ëŸ‰í™” LLM íŒŒì¸íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¡œ, **â€œHugging Face í˜¸í™˜â€**ì„ í‘œë°©í•˜ë©´ì„œë„ **í•™ìŠµ ì†ë„ë¥¼ 2ë°° ì´ìƒ ë†’ì´ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ 40~70% ì¤„ì´ëŠ”** í˜ì‹ ì„ ë³´ì—¬ì£¼ê³   ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details))L92ã€‘. Unslothì˜ ì ‘ê·¼ë²•ì€ ê¸°ì¡´ Hugging Face `Transformers` ëª¨ë¸ì˜ ì¼ë¶€ ì—°ì‚°ì„ **Triton** ê¸°ë°˜ì˜ ë§ì¶¤ ì»¤ë„ë¡œ ëŒ€ì²´í•˜ì—¬ **PyTorch ìˆ˜ì¤€ì—ì„œì˜ ë¹„íš¨ìœ¨ì„ ì œê±°**í•˜ëŠ”  ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code))L72ã€‘. êµ¬ì²´ì ìœ¼ë¡œ, **Self-Attention, FFN ë“± Transformer í•µì‹¬ ëª¨ë“ˆì˜ backward ì—°ì‚°ì„ ìˆ˜ì‹ìœ¼ë¡œ ì§ì ‘ ìœ ë„**í•˜ì—¬ Tritonìœ¼ë¡œ êµ¬í˜„í•¨ìœ¼ë¡œì¨, ê°™ì€ ì‘ì—…ì„ í•˜ë©´ì„œë„ **ë©”ëª¨ë¦¬ ë³µì‚¬ë‚˜ ì¤‘ê°„ ì—°ì‚° overheadë¥¼ ìµœì†Œí™”** ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code))L72ã€‘. ì´ëŸ° **ìˆ˜ë™ ìµœì í™”(manual backprop)** ê¸°ë²• ë•ë¶„ì—, **ë™ì¼í•œ QLoRA íŒŒì¸íŠœë‹ì´ë¼ë„ Unsloth ì‚¬ìš© ì‹œ í•™ìŠµ ì†ë„ê°€ ì•½ 2ë°°ë¡œ í–¥ìƒë˜ê³  GPU VRAM ì‚¬ìš©ì€ ì ˆë°˜ ì´í•˜ë¡œ ê°ì†Œ**í•˜ëŠ” ê²°ê³¼ë¥¼ ì–» ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Llama,18.6))L88ã€‘. ë†€ëê²Œë„ **ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ ì €í•˜ê°€ 0%**ì„ì´ ê²€ì¦ë˜ì—ˆëŠ”ë°, ì´ëŠ” Unslothì˜ ì»¤ë„ ìµœì í™”ê°€ ê·¼ë³¸ì ìœ¼ë¡œ **ë™ì¼í•œ ê³„ì‚°ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„**í•œ ê²ƒì´ë¯€ë¡œ ì •í™•ë„ê°€ ë³´ì¡´ë˜ê¸° ë•Œ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code))L68ã€‘.

UnslothëŠ” **Hugging Faceì™€ì˜ í˜¸í™˜ì„±**ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì‚¬ìš©ë²•ë„ ë§¤ìš° ë¹„ìŠ·í•˜ì—¬, `FastLanguageModel.from_pretrained()` í•¨ìˆ˜ë¡œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ `transformers` ëª¨ë¸ì„ ë˜í•‘í•œ Unsloth ëª¨ë¸ ê°ì²´ì™€ í† í¬ë‚˜ì´ì €ë¥¼ ë°˜ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=,returns%20the%20model%20tokenizer%20for)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=model%2C%20tokenizer%20%3D%20FastLanguageModel,RoPE%20Scaling%20internally%2C%20so%20choose))L22ã€‘. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ LLaMA ê³„ì—´ ëª¨ë¸ì„ Unslothë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-bnb-4bit",  # HF í—ˆë¸Œ ëª¨ë¸ëª… (4-bit ì–‘ìí™”ëœ Mistral 7B)
    max_seq_length=2048                       # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (RoPE ìŠ¤ì¼€ì¼ë§ ìë™ì ìš©)
)
```

ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì€ Hugging Face `transformers`ì™€ ê±°ì˜ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë¯€ë¡œ, `transformers.Trainer`ë‚˜ ğŸ¤— TRLì˜ `SFTTrainer` ë“±ì— ê·¸ëŒ€ë¡œ ë„£ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=most%20NVIDIA%20GPUs%20%E2%80%93from%20GTX,and%20Mistral%20architectures))-L66ã€‘. UnslothëŠ” í˜„ì¬ **LLaMA ê³„ì—´(Llama-2, CodeLlama ë“±)ê³¼ Mistral, Qwen ë“± GPT ìœ ì‚¬ ì•„í‚¤í…ì²˜**ë¥¼ ì§€ì›í•˜ë©°, ë‹¤ì–‘í•œ NVIDIA GPU(ì˜ˆ: GTX 16GBê¸‰ë¶€í„° A100/H100ê¹Œì§€)ì—ì„œ  ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures))-L63ã€‘. íŠ¹íˆ **FP16, BF16 í˜¼í•©ì •ë°€ë„**ë„ ì˜µì…˜ìœ¼ë¡œ ì¼¤ ìˆ˜ ìˆê³ , **ì–‘ìí™”ëœ ëª¨ë¸(`bnb-4bit`)ë„ ì§ì ‘ ë¡œë“œ**í•  ìˆ˜ ìˆì–´ (bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”), Hugging Faceì—ì„œ í•˜ë˜ 4-bit QLoRA íŒŒì¸íŠœë‹ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ë©´ì„œ ì„±ëŠ¥ í–¥ìƒì„ ëˆ„ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Unslothì˜ íŠ¹ê¸°í•  ë§Œí•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” **RoPE Scaling**ì„ ìë™ ì²˜ë¦¬í•˜ëŠ” ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=,returns%20the%20model%20tokenizer%20for))-L15ã€‘. RoPE(Rotary Positional Embedding)ëŠ” GPT ê³„ì—´ì—ì„œ ì“°ì´ëŠ” ìœ„ì¹˜ì¸ì½”ë”© ê¸°ë²•ì¸ë°, Unsloth ëª¨ë¸ ë¡œë“œì‹œ `max_seq_length`ë¥¼ í¬ê²Œ ì§€ì •í•˜ë©´ **í•™ìŠµ ì‹œ ë” ê¸´ ë¬¸ë§¥ê¸¸ì´**ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë‚´ë¶€ì ìœ¼ë¡œ ì£¼íŒŒìˆ˜ë¥¼ ìŠ¤ì¼€ì¼ë§í•´ ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³¸ë˜ 2048 í† í°ê¹Œì§€ì˜€ë˜ LLaMA-2 ëª¨ë¸ë„ **ìµœëŒ€ 4ë°° ì´ìƒ ê¸´ ì»¨í…ìŠ¤íŠ¸ê¹Œì§€** íŒŒì¸íŠœë‹í•  ([Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=,tuning))L163ã€‘, ì¼ë¶€ ìµœì‹  ëª¨ë¸(Llama-3.3 70B ë“±)ì€ Unslothë¡œ **8ë§Œ~3ì‹­ë§Œ í† í° ì´ìƒì˜ ë¬¸ë§¥ í•™ìŠµ**ë„ ì‹œë„ë˜ê³  ([GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1 & Reasoning LLMs 2x faster with 70% less memory! ](https://github.com/unslothai/unsloth#:~:text=with%20Llama%20%26%20Qwen%20distillations,13x%20longer))L308ã€‘. ê¸´ ë¬¸ë§¥ ëŒ€ì‘ì€ **Decoder-Only ëª¨ë¸ì˜ ì‹¤ì œ í™œìš©ë„**ë¥¼ ë†’ì´ëŠ” ì¤‘ìš”í•œ ìµœì í™”ì¸ë°, Unslothê°€ ì´ë¥¼ í¸ë¦¬í•˜ê²Œ ì§€ì›í•˜ëŠ” ì ì€ ì‹¤ìš©ì  ì¥ì ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìš”ì•½í•˜ë©´ Unslothì˜ íŠ¹ì§•ê³¼ ì¥ì :

- **Triton ì»¤ë„ ê¸°ë°˜ ìµœì í™”**ë¡œ **í•™ìŠµì†ë„ ~2ë°° í–¥ìƒ**, **ë©”ëª¨ë¦¬ ~50% ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Llama,18.6))-L88ã€‘ (ë™ì¼ í•˜ë“œì›¨ì–´/ëª¨ë¸ ëŒ€ë¹„)
- Hugging Face **Transformers/PEFTì™€ ì™„ì „ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures))-L63ã€‘ â€“ ì¹œìˆ™í•œ APIë¡œ ì‚¬ìš© ê°€ëŠ¥
- **QLoRA(4-bit + LoRA) ì§€ì›** â€“ ì €ìë“¤ì´ ì œê³µí•œ ë‹¤ì´ë‚˜ë¯¹ 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ QLoRAì˜ ë¯¸ì„¸ ì„±ëŠ¥ ì €í•˜ ([Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=improves%20accuracy%20%281%E2%80%932)) ([Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=We%20recommend%20starting%20with%20QLoRA%2C,LoRA%20is%20now%20largely%20recovered))L174ã€‘
- **RoPE ë“± Decoderìš© ì¶”ê°€ ê¸°ëŠ¥** â€“ ë¬¸ë§¥ê¸¸ì´ í™•ì¥ ë“± ë””ì½”ë” Transformerì— ìœ ìš©í•œ ìµœì í™” ì œê³µ
- **ì˜¤í”ˆì†ŒìŠ¤ ê°œë°œ í™œì„±í™”** â€“ ì½œë© ë…¸íŠ¸ë¶, ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ê³µê°œ ë“±ìœ¼ë¡œ ì¬í˜„ì„±ê³¼ ì ‘ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details))-L92ã€‘

Unslothì˜ í˜„ì¬ í•œê³„ë¡œëŠ” **ì§€ì› ì•„í‚¤í…ì²˜ê°€ ì œí•œì **ì´ë¼ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤. ì£¼ë¡œ Metaì˜ Llama ê³„ì—´ê³¼ ê·¸ íŒŒìƒëª¨ë¸ì— ì§‘ì¤‘ë˜ì–´ ìˆê³ , Transformer êµ¬ì¡°ê°€ ë‹¤ë¥¸ T5(Encoder-Decoder)ë‚˜ GLM ì–‘ë°©í–¥ ëª¨ë¸ ë“±ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë˜í•œ ë¶„ì‚° í•™ìŠµ(ë©€í‹° GPU)ì— ëŒ€í•œ ì–¸ê¸‰ì´ ì ì€ë°, ì£¼ë¡œ ë‹¨ì¼ GPUì—ì„œì˜ ê·¹í•œ ìµœì í™”ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ì£¼ í° ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ë‚˜ëˆ„ì–´ í•™ìŠµí•˜ëŠ” ìš©ë„ëŠ” DeepSpeedë§Œí¼ ì£¼ì•ˆì ì€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ **ë‹¨ì¼/ì†Œìˆ˜ GPUë¡œ LLMì„ ìµœëŒ€í•œ ë¹ ë¥´ê²Œ íŠœë‹**í•´ì•¼ í•˜ëŠ” ì‹¤ë¬´ ìƒí™©ì—ì„œ UnslothëŠ” ëŒ€ë‹¨íˆ ë§¤ë ¥ì ì¸ ì„ íƒì§€ì…ë‹ˆë‹¤. ì˜ˆì»¨ëŒ€, 1ì¥ì˜ A100ìœ¼ë¡œ í•˜ë£¨ ê±¸ë¦¬ë˜ íŒŒì¸íŠœë‹ ì‘ì—…ì„ Unslothë¡œ ë°˜ë‚˜ì ˆì— ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6))-L77ã€‘, ê°™ì€ GPUì—ì„œ ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆë‚˜ ë” ê¸´ ë¬¸ë§¥ì„ ì‹¤í—˜í•  ì—¬ìœ ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ê³§ **ê°œë°œ ìƒì‚°ì„±ê³¼ ì‹¤í—˜ ë²”ìœ„ì˜ í™•ëŒ€**ë¡œ ì´ì–´ì§€ë¯€ë¡œ, ì•ìœ¼ë¡œ Unslothì™€ ê°™ì€ ìµœì í™” íˆ´ì˜ í™œìš©ë„ëŠ” ë”ìš± ë†’ì•„ì§ˆ ì „ë§ì…ë‹ˆë‹¤.

## ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€ ë°©ë²•
LLM íŒŒì¸íŠœë‹ ê¸°ë²•ë“¤ì„ í‰ê°€í•  ë•Œì—ëŠ” **ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥** ë¿ ì•„ë‹ˆë¼ **í•™ìŠµ íš¨ìœ¨ ì§€í‘œ**ë“¤ë„ ì¤‘ìš”í•©ë‹ˆë‹¤. ì£¼ìš” ë¹„êµ ê¸°ì¤€ì€ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰(VRAM)**, **í•™ìŠµ ì†ë„(throughput)**, **í•™ìŠµ ì•ˆì •ì„± ë° íš¨ìœ¨ì„±** ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ í‘œëŠ” Hugging Face ê¸°ë³¸ ë°©ë²•, DeepSpeed, Unslothì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ì„±ëŠ¥ ìƒì˜ ì¥ë‹¨ì ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤:

| ì ‘ê·¼ë²•                     | ì£¼ìš” íŠ¹ì§• ë° ìµœì í™”               | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰              | í•™ìŠµ ì†ë„                 | ë¹„ê³  (ì¥ë‹¨ì  ìš”ì•½)                  |
|----------------------------|----------------------------------|---------------------------|---------------------------|--------------------------------------|
| **Hugging Face ê¸°ë³¸**      | - Pretrained ëª¨ë¸/ë°ì´í„° ì—ì½”ì‹œìŠ¤í…œ<br>- Trainer/Accelerate í†µí•œ ì†ì‰¬ìš´ êµ¬í˜„<br>- PEFT: LoRA, P-Tuning ë“± ì§€ì›<br>- 8/4-bit ì–‘ìí™” ë¡œë“œ ì§€ì› | ê¸°ì¤€ (100%)                | ê¸°ì¤€ (1Ã—)                | ì‰¬ìš´ êµ¬í˜„ê³¼ ì»¤ë®¤ë‹ˆí‹° ì§€ì›ì´ ê°•ì . ëŒ€í˜• ëª¨ë¸ì€ ì¶”ê°€ ìµœì í™” í•„ìš” (ì˜ˆ: DeepSpeed í†µí•© ê°€ëŠ¥). |
| **DeepSpeed (ZeRO)**       | - ZeRO-1/2/3 ì˜µí‹°ë§ˆ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))L110ã€‘<br>- CPU/NVMe Offlo ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))L109ã€‘<br>- ë³‘ë ¬í™” ìµœì  íŠœë‹ (ì¼ê´„ í†µì‹ , One-bit Adam ë“±)<br>- ë¶„ì‚° í›ˆë ¨ì— íŠ¹í™” | **ë§¤ìš° ì ìŒ** (íŒŒë¼ë¯¸í„°/ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ì‚°ìœ¼ë¡œ GPUë‹¹ ë¶€ ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization))L110ã€‘ | **ë†’ìŒ** (ë©€í‹° GPUë¡œ ì„ í˜• ìŠ¤ì¼€ì¼ë§, ë‹¨ì¼ GPUì—ì„  ë‹¤ì†Œ ì˜¤ë²„í—¤ë“œ) | ì´ˆëŒ€í˜• ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥ (ìˆ˜ì‹­ì–µ~ìˆ˜ì²œì–µâ†‘  ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))L109ã€‘. ì´ˆê¸° ì„¤ì • ë³µì¡í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ ì‹¤í—˜ì—” í•„ìˆ˜ ë„êµ¬. |
| **Unsloth (QLoRA ê¸°ë°˜)**   | - Triton ì»¤ë„ë¡œ ëª¨ë¸ ì—° ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code))-L72ã€‘<br>- ìˆ˜ë™ backpropìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½<br>- RoPE ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë¬¸ë§¥ í™•ì¥<br>- HF Transformersì™€ í˜¸í™˜ API | **ì ìŒ** (ë™ì¼ QLoRA ëŒ€ë¹„ VRAM ìµœëŒ€ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details))-L92ã€‘) | **ë§¤ìš° ë†’ìŒ** (ë™ì¼ QLoRA ëŒ€ë¹„ ~ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6))-L77ã€‘) | ë‹¨ì¼/ì†Œìˆ˜ GPU í™˜ê²½ì— ìµœì í™”. ì •í™•ë„ ì†ì‹¤ ì—†ì´  ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code))-L68ã€‘. ì§€ì› ëª¨ë¸ í•œì •ì ì´ë‚˜ ë¹ ë¥´ê²Œ í™•ëŒ€ ì¤‘. |

*í‘œ: Hugging Face vs. DeepSpeed vs. Unslothì˜ íŠ¹ì§• ë° íš¨ìœ¨ ë¹„êµ*

ìœ„ ë¹„êµì—ì„œ ë³´ë“¯ì´, **Hugging Face + ê¸°ë³¸ PyTorch**ëŠ” êµ¬í˜„ í¸ì˜ì„± ì¸¡ë©´ì—ì„œ ë›°ì–´ë‚˜ë‚˜ **ëŒ€í˜• ëª¨ë¸ í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ ë³‘ëª©**ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DeepSpeedëŠ” ì´ë¥¼ í•´ì†Œí•˜ì—¬ **ëª¨ë¸ ì‚¬ì´ì¦ˆ í•œê³„ë¥¼ í¬ê²Œ ë†’ì—¬ì£¼ì§€ë§Œ**, êµ¬ì„± ë³µì¡ì„±ê³¼ **í†µì‹  ì˜¤ë²„í—¤ë“œ**ê°€ ì•½ê°„ ì¡´ì¬í•©ë‹ˆë‹¤. UnslothëŠ” **ë‚®ì€ ìˆ˜ì¤€ì˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ í†µí•´** ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ì‹œë‚˜ë¦¬ì˜¤(ì˜ˆ: LLaMA ê³„ì—´ì˜ SFT)ì—ì„œ **ìµœëŒ€ì˜ ì†ë„/ë©”ëª¨ë¦¬ íš¨ìœ¨**ì„ ëŒì–´ì˜¬ë¦° ì‚¬ë¡€ì…ë‹ˆë‹¤. íŠ¹íˆ QLoRAì²˜ëŸ¼ **4-bit ì–‘ìí™”ë¡œ ì¸í•œ 16-bit ëŒ€ë¹„ ì•½ê°„ì˜ ì†ë„ ì €í•˜**ê°€ ì›ë˜ ([Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of ...](https://lightning.ai/pages/community/lora-insights/#:~:text=Code%20Framework,which%20is%20to%20be)) ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv](https://ar5iv.labs.arxiv.org/html/2305.14314#:~:text=a%20significant%20shift%20in%20accessibility,finetunable%20on%20a%20single%20GPU))-L57ã€‘, Unsloth ìµœì í™”ë¡œ ì´ëŸ¬í•œ **ì–‘ìí™” ì˜¤ë²„í—¤ë“œê¹Œì§€ ìƒì‡„**í•œ ê²ƒì´ í° ì¥ì ì…ë‹ˆë‹¤.

**í‰ê°€ ë°©ë²•**ìœ¼ë¡œ, **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**ì€ ì¼ë°˜ì ìœ¼ë¡œ **í›ˆë ¨ ì¤‘ ìµœëŒ€ GPU VRAM ì ìœ **ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤ (ì˜ˆ: `nvidia-smi` ëª¨ë‹ˆí„°ë§). DeepSpeedì˜ ê²½ìš° ZeRO-3ë¥¼ ì“°ë©´ ê° GPUê°€ ëª¨ë¸ ì¼ë¶€ë§Œ ê°–ê³  ìˆìœ¼ë¯€ë¡œ ê°œë³„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ê²Œ ì¤„ê³ , ë‚˜ë¨¸ì§€ëŠ” CPU/NVMe ì‚¬ìš©ëŸ‰ìœ¼ë¡œ  ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=GPU%20Peak%20Memory%20consumed%20during,begin%29%3A%200)) ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=GPU%20Peak%20Memory%20consumed%20during,begin%29%3A%200))-L20ã€‘. **ì²˜ë¦¬ ì†ë„**ëŠ” ë³´í†µ **ì´ˆë‹¹ ì²˜ë¦¬ í† í° ìˆ˜ (tokens per second)** ë˜ëŠ” **ìŠ¤í…ë‹¹ ì‹œê°„**ìœ¼ë¡œ ì‚°ì¶œí•©ë‹ˆë‹¤. ê°™ì€ í•˜ë“œì›¨ì–´ì—ì„œ ë°°ì¹˜ë‹¹ í† í° throughputì„ ë¹„êµí•˜ë©´ ìµœì í™” íš¨ê³¼ë¥¼ ì •ëŸ‰í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì»¨ëŒ€, Unsloth íŒ€ì€ ë‹¤ì–‘í•œ ëª¨ë¸/ë°ì´í„°ì…‹ì— ëŒ€í•´ **ì´ˆë‹¹ í† í° ì²˜ë¦¬ëŸ‰**ì„ ì¸¡ì •í•˜ì—¬ Hugging Face ëŒ€ë¹„ **1.5Ã—~2.7Ã— ì†ë„ í–¥ìƒ**ì„ ë³´ê³  ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Free%20Colab%20T4%20Dataset%20Hugging,18.6))-L87ã€‘.

í•™ìŠµ íš¨ìœ¨ ì´ì™¸ì—, **ëª¨ë¸ ì„±ëŠ¥ í‰ê°€** ë˜í•œ í•„ìˆ˜ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì§€ë„íŒŒì¸íŠœë‹ì„ í†µí•´ ëª©í‘œ ì‘ì—…ì— ì–¼ë§ˆë‚˜ í–¥ìƒë˜ì—ˆëŠ”ì§€, ë˜ëŠ” í˜¹ì‹œ **ê¸°ì¡´ ì§€ì‹ì„ í›¼ì†**í•˜ì§€ ì•Šì•˜ëŠ”ì§€ ë“±ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. **Decoder-Only LLM**ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ **í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆ**ì´ë‚˜ **ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ ì„±ëŠ¥**ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, **ì§€ë„í•™ìŠµìœ¼ë¡œ ëŒ€í™”í˜• ëª¨ë¸**ì„ íŠœë‹í–ˆë‹¤ë©´ **ChatGPTì™€ ìœ ì‚¬í•œ ë²¤ì¹˜ë§ˆí¬(Vicuna Benchmark ë“±)**ì—ì„œ ëŒ€í™” í’ˆì§ˆì„ ì¸¡ì •í•˜ê±°ë‚˜, Human í‰ê°€ í˜¹ì€ GPT-4ë¥¼ í™œìš©í•œ ë¹„êµ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=full%2016,innovations%20to%20save%20memory%20without)) ([](https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=on%20the%20Vicuna%20,Table%204))L107ã€‘. QLoRA ë…¼ë¬¸ì—ì„œëŠ” **GPT-4 ê¸°ë°˜ ìë™ í‰ê°€**ë¥¼ í†µí•´, 65B ëª¨ë¸ì„ QLoRAë¡œ ë¯¸ì„¸ì¡°ì •í•œ Guanacoê°€ ChatGPT ëŒ€ë¹„ 99.3% ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŒì„  ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=full%2016,innovations%20to%20save%20memory%20without))-L18ã€‘. ì´ì²˜ëŸ¼ **ëª¨ë¸ ì¶œë ¥ì˜ ì •ëŸ‰Â·ì •ì„± í‰ê°€**ë¥¼ í†µí•´ íŒŒì¸íŠœë‹ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ **perplexity**(ì–¸ì–´ëª¨ë¸ì˜ ë¡œê·¸í™•ë¥  ì§€í‘œ)ë„ ì‚¬ìš©ë˜ëŠ”ë°, ì›ë˜ ëª¨ë¸ ëŒ€ë¹„ í¼í”Œë ‰ì„œí‹° ë³€í™”ë¡œ **ê³¼ì í•© ì—¬ë¶€ë‚˜ ì¼ë°˜í™” ì„±ëŠ¥**ì„ ê°€ëŠ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ì—°êµ¬ì— ë”°ë¥´ë©´ **íŒŒì¸íŠœë‹ ë°ì´í„°ì˜ í’ˆì§ˆì´ ë°ì´í„°ëŸ‰ë³´ë‹¤ ì¤‘ìš”**í•˜ë©°, ê³ í’ˆì§ˆ ì†ŒëŸ‰ ë°ì´í„°ë¡œë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ([](https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=Guanaco%2C%20,strong%20Vicuna%20chatbot%20benchmark%20performance)) ([](https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=analyze%20trends%20in%20the%20trained,strong%20Vicuna%20chatbot%20benchmark%20performance))L142ã€‘. Metaì˜ **LIMA ì—°êµ¬(2023)**ì—ì„œëŠ” LLaMA 65B ëª¨ë¸ì„ **ì—„ì„ ëœ 1000ê°œì˜ ì˜ˆì‹œ**ë§Œìœ¼ë¡œ ì§€ë„í•™ìŠµ íŒŒì¸íŠœë‹ í•˜ì˜€ì„ ë•Œ GPT-4 ë“± ê±°ëŒ€ ëª¨ë¸ì— í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸°ë„ ([Paper page - LIMA: Less Is More for Alignment - Hugging Face](https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling))-L18ã€‘. ì´ëŠ” **ì‚¬ì „í•™ìŠµëœ ê±°ëŒ€ LMì˜ ì ì¬ë ¥ì„ ëŒì–´ë‚´ëŠ” ë° ìˆì–´, ë°©ëŒ€í•œ ì–‘ì˜ ë¯¸ì„¸ì¡°ì • ë°ì´í„°ë³´ë‹¤ ì¸ê°„ ì „ë¬¸ê°€ê°€ ê³ ë¥¸ í•µì‹¬ ë°ì´í„°ê°€ íš¨ê³¼ì **ì¼ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, **Decoder-Only Transformer ìµœì í™” ê¸°ë²•**ë“¤ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **ì–‘ìí™”(Quantization)**: 16-bit ëŒ€ì‹  8-bit, 4-bitë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ í‘œí˜„í•´ ë©”ëª¨ë¦¬ ê°ì†Œ (ì˜ˆ: QLoRAì˜ 4-bit NF ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU))L205ã€‘. ì ì ˆí•œ ì–‘ìí™”ëŠ” **ì„±ëŠ¥ ìœ ì§€í•˜ë©´ì„œ ë©”ëª¨ë¦¬ 4ë°° ì ˆì•½** ê°€ëŠ¥.
- **íŒŒë¼ë¯¸í„° íš¨ìœ¨ ê¸°ë²•(PEFT)**: ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper))L210ã€‘, Adaptor, Prefix-Tuning ë“±ìœ¼ë¡œ **ì†Œìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ**í•˜ì—¬ ì—°ì‚°/ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ .
- **Flash Attention ë“± ë©”ëª¨ë¦¬ íš¨ìœ¨ Attention**: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆ ë•Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì´ëŠ” **ìµœì í™” Attention ì•Œê³ ë¦¬ì¦˜**. PyTorch 2.xì—ì„œëŠ” ì´ëŸ¬í•œ **SDPA(Scaled Dot-Product Attention)**ê°€ ê¸°ë³¸ í†µí•©ë˜ì–´ ìˆì–´ ì„±ëŠ¥ í–¥ ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details))-L92ã€‘.
- **Gradient Checkpointing**: ì¤‘ê°„ í™œì„±ê°’ì„ ì €ì¥í•˜ì§€ ì•Šê³  ì¬ê³„ì‚°í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, **GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ í° í­ìœ¼ë¡œ ì ˆê°** (ëŒ€ì‹  ê³„ì‚°ëŸ‰ ì¦ê°€). ëŒ€í˜• ëª¨ë¸ íŒŒì¸íŠœë‹ì— ê±°ì˜ í•„ìˆ˜ì ìœ¼ë¡œ ì“°ì…ë‹ˆë‹¤.
- **Mixed Precision Training**: FP32 ëŒ€ì‹  **FP16/BF16** ë“±ì„ ì‚¬ìš©í•˜ì—¬ **ì—°ì‚° ì†ë„ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”**. ìµœê·¼ GPUëŠ” BF16/FP16 ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë¯€ë¡œ, ì •í™•ë„ì— í° ë¬¸ì œì—†ì´ í™œìš©.
- **ë¶„ì‚° ë³‘ë ¬í™”**: ëª¨ë¸ ë³‘ë ¬í™”(ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„í• ), ë°ì´í„° ë³‘ë ¬í™”, íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” ë“± ì¡°í•©ìœ¼ë¡œ **í•˜ë“œì›¨ì–´ ìì› í™œìš© ê·¹ëŒ€í™”**. DeepSpeed, FSDP, Megatron-LM ë“±ì´ ì§€ì›.
- **ë™ì  ì¥ë¹„ ë©”ëª¨ë¦¬ í™œìš©**: GPUì™€ CPU, ë””ìŠ¤í¬ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ **ê³„ì‚° ìì› ëŒ€ë¹„ ìµœëŒ€ ë©”ëª¨ë¦¬ í™œìš©** (ZeRO-Offload/Infinit ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))L109ã€‘.
- **ìµœì‹  ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©**: AdamW ì™¸ì— LAMB, Lion ë“±ì˜ ëŒ€ì•ˆ ì˜µí‹°ë§ˆì´ì €ë‚˜, DeepSpeedì˜ One-bit Adamì²˜ëŸ¼ **í†µì‹ ëŸ‰ì„ ì¤„ì¸ ë¶„ì‚° ì˜µí‹°ë§ˆì´ì €**ë¡œ íš¨ìœ¨ ê°œì„ .
- **ì •ê·œí™” ë° ì•ˆì •í™” ê¸°ë²•**: ëŒ€ê·œëª¨ LM íŒŒì¸íŠœë‹ ì‹œ **ëŸ¬ë‹ë ˆì´íŠ¸ ì›Œë°ì—…**, **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„**, **Gradient Clipping** ë“±ìœ¼ë¡œ ì•ˆì •ì  ìˆ˜ë ´ì„ ë„ëª¨. ì´ëŠ” ê°„ì ‘ì ìœ¼ë¡œ íš¨ìœ¨(ì¬ì‹œë„ ê°ì†Œ ë“±)ì— ê¸°ì—¬.
- **Continuous Pretrainingê³¼ SFT ê²°í•©**: ê²½ìš°ì— ë”°ë¼ **ì‚¬ì „í•™ìŠµ ì—°ì¥(Continued Pretraining)** í›„ SFTë¥¼ í•˜ë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ê±°ë‚˜, SFT ë„ì¤‘ **ê¸°ì¡´ ì§€ì‹ ìœ ì§€**ë¥¼ ìœ„í•œ **æ··åˆ ì‚¬ì „í•™ìŠµ ë°ì´í„° ì‚¬ìš©** ë“±ì˜ ê¸°ë²•ë„ ì—°êµ¬ë˜ê³  ([Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=the%20accuracy%20loss%20for%20QLoRA,LoRA%20is%20now%20largely%20recovered))L174ã€‘.

## ìµœì‹  ì—°êµ¬ ë™í–¥ ë° ê²°ë¡ 
ìµœê·¼ 2ë…„ê°„ LLM íŒŒì¸íŠœë‹ ë¶„ì•¼ëŠ” **â€œë” ì ì€ ìì›ìœ¼ë¡œ ë” í° ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ë²•â€**ì— ì§‘ì¤‘ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. **QL ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,innovations%20to%20save%20memory%20without))-L18ã€‘ì˜ ë“±ì¥ìœ¼ë¡œ ì´‰ë°œëœ **ì €ë¹„íŠ¸ ì–‘ìí™” + ì–´ëŒ‘í„° í•™ìŠµ** íŒ¨ëŸ¬ë‹¤ì„ì€ í˜„ì¬ ì—…ê³„ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ì•˜ê³ , ì´ë¥¼ ë„˜ì–´ **ì•„ì§ ì‹¤í—˜ ë‹¨ê³„ì¸ 3ë¹„íŠ¸, 2ë¹„íŠ¸** ë¯¸ì„¸íŠœë‹ ì—°êµ¬ë„ ì§„í–‰ì¤‘ì…ë‹ˆë‹¤. ë˜í•œ **LORAì˜ ë³€í˜•**ìœ¼ë¡œì„œ ì¤‘ìš”ë„ê°€ ë†’ì€ ë ˆì´ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ë” í• ë‹¹í•˜ëŠ” **AdaLoRA** ë“±ì˜ ê¸°ë²•ë„ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. í•œí¸, **íŒŒì¸íŠœë‹ ë°ì´í„° í™•ë³´** ì¸¡ë©´ì—ì„œëŠ” Stanfordì˜ **Alpaca** í”„ë¡œì íŠ¸ì²˜ëŸ¼ **ê¸°ì¡´ ëª¨ë¸(ì˜ˆ: GPT-3)ë¥¼ ì´ìš©í•œ Self-Instruct ë°ì´í„° ìƒì„±**ì´ ìœ í–‰í•˜ì—¬, ë¹„êµì  ì €ë ´í•˜ê²Œ ì§€ë„í•™ìŠµ ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” íë¦„ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íƒ„ìƒí•œ **Vicuna**, **WizardLM**, **OpenAssistant** ë“±ì˜ **ì˜¤í”ˆì†ŒìŠ¤ ëŒ€í™”í˜• ëª¨ë¸**ë“¤ì€ ëª¨ë‘ ê³µê°œ ë°ì´í„°ë‚˜ ìƒì„± ë°ì´í„°ë¡œ SFTëœ ì‚¬ë¡€ë“¤ì…ë‹ˆë‹¤. ì„±ëŠ¥ ë©´ì—ì„œ, ì•ì„œ ì–¸ê¸‰í•œ **LIMA (Less is More for Alignme ([Paper page - LIMA: Less Is More for Alignment - Hugging Face](https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling))-L18ã€‘ ì—°êµ¬ëŠ” **ê³ í’ˆì§ˆ ì†Œê·œëª¨ ë°ì´í„°ì˜ ìœ„ë ¥**ì„ ë³´ì—¬ì£¼ì—ˆê³ , OpenAIë„ **InstructGPT ë…¼ë¬¸(2022)**ì—ì„œ ì¸ê°„ í”¼ë“œë°± ì™¸ì— **ì´ˆê¸° ë‹¨ê³„ì˜ ìŠˆí¼ë°”ì´ì¦ˆë“œ íŒŒì¸íŠœë‹(SFT)**ì´ í•µì‹¬ì ìœ¼ë¡œ ì¤‘ìš”í•¨ì„ ë°íŒ ë°” ìˆìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” **RLHF**(ê°•í™”í•™ìŠµ íœ´ë¨¼ í”¼ë“œë°±) ëŒ€ì‹  **DPO**(Direct Preference Optimization)ë‚˜ **RLAIF**(AI í”¼ë“œë°±) ë“± **ìˆœìˆ˜ ì§€ë„ ì‹ í˜¸ë§Œìœ¼ë¡œ ì„ í˜¸ë„ë¥¼ í•™ìŠµ**í•˜ë ¤ëŠ” ì‹œë„ë„ ë‚˜ì˜¤ê³  ìˆì–´, **ì§€ë„ íŒŒì¸íŠœë‹ì˜ ë²”ìœ„ê°€ í™•ì¥**ë˜ê³  ìˆìŠµë‹ˆë‹¤.

ì •ë¦¬í•˜ë©´, **Decoder-Only LLMì˜ ì§€ë„ íŒŒì¸íŠœë‹**ì€ ì—¬ì „íˆ **ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ê³¼ íš¨ìœ¨ì  í•™ìŠµ**ì„ ì–‘ë¦½í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì—°êµ¬ë¡œ í™œë°œíˆ ì§„í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. Hugging Face, DeepSpeed, Unslothì™€ ê°™ì€ ë„êµ¬ë“¤ì€ ì´ëŸ¬í•œ ì—°êµ¬ ì„±ê³¼ë¥¼ í˜„ì—…ì— ì ìš©í•˜ëŠ” ë‹¤ë¦¬ ì—­í• ì„ í•˜ë©°, ê°ê¸° **ì‚¬ìš©ì ìš”êµ¬ì™€ í™˜ê²½ì— ë§ëŠ” ì†”ë£¨ì…˜**ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì„¸ ê°€ì§€ ì ‘ê·¼ë²•ì„ **ìƒí™©ì— ë”°ë¼ ì¡°í•©**í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, **ì¤‘ê°„ ê·œëª¨ ëª¨ë¸ì€ Unslothë¡œ ì‹±ê¸€ GPU ë¹ ë¥´ê²Œ íŠœë‹**í•˜ê³ , **ì´ˆê±°ëŒ€ ëª¨ë¸ì€ DeepSpeedë¡œ ë©€í‹° GPU ë¶„ì‚° í•™ìŠµ**í•˜ë©°, ì „ë°˜ì ì¸ ì›Œí¬í”Œë¡œìš°ëŠ” Hugging Face ì—ì½”ì‹œìŠ¤í…œìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ì‹ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ **ëª¨ë¸ì˜ ëª©í‘œì™€ ì œì•½ì— ë§ì¶° ìµœì ì˜ ê¸°ë²•ì„ ì„ íƒ**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•ìœ¼ë¡œë„ í•˜ë“œì›¨ì–´ì™€ ì•Œê³ ë¦¬ì¦˜ ì¸¡ë©´ì˜ ë°œì „ìœ¼ë¡œ LLM íŒŒì¸íŠœë‹ì€ ë”ìš± ìµœì í™”ë˜ê³  ëŒ€ì¤‘í™”ë  ê²ƒì´ë©°, **â€œë” ë‚®ì€ ë¹„ìš©ìœ¼ë¡œ ë” ë˜‘ë˜‘í•œ ëª¨ë¸â€**ì„ ë§Œë“œëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ê²ƒì…ë‹ˆë‹¤.

**ì°¸ê³  ë¬¸í—Œ ë° ë§í¬:** ìµœì‹  íŒŒì¸íŠœë‹ ê¸°ë²•ê³¼ ì‚¬ë¡€ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ Hugging Face ë¸”ë¡œê·¸ ë° ê° ë…¼ë¬¸ì˜ ì›ë¬¸ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ì•„ë˜ëŠ” ë³¸ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ìë£Œë“¤ì˜ ì¶œì²˜ì…ë‹ˆë‹¤.

- Hugging Face ë¸”ë¡œê·¸: *Making LLMs even more accessible with 4-bit quantization and Q ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU)) ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=QLoRA%20tuning%20is%20shown%20to,the%20power%20of%20QLoRA%20tuning))L222ã€‘
- Hugging Face ë¸”ë¡œê·¸: *Make LLM fine-tuning 2x faster with Unsloth and ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6))-L80ã€‘
- Hugging Face Docs: *DeepSpeed & Accelerate Integration G ([DeepSpeed](https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization)) ([ZeRO-Offload - DeepSpeed](https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed))L109ã€‘
- Unsloth ê³µì‹ ë¬¸ ([Fine-tuning Guide | Unsloth Documentation](https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=,tuning)) ([Make LLM Fine-tuning 2x faster with Unsloth and  TRL](https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6))-L77ã€‘
- QLoRA ë…¼ë¬¸ (Dettmers et al.,  ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,innovations%20to%20save%20memory%20without)) ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://ar5iv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,of))-L27ã€‘
- LIMA ë…¼ë¬¸ (Zhou et al.,  ([Paper page - LIMA: Less Is More for Alignment - Hugging Face](https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling))-L18ã€‘

