
<!doctype html>
<html lang="ko" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LLM Fine-Tuning 기법을 배우는 워크샵">
      
      
        <meta name="author" content="Jong Hyun Park">
      
      
      
        <link rel="prev" href="../peft_methods/">
      
      
        <link rel="next" href="../korean_tuning/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.28">
    
    
      
        <title>양자화 기법 - LLM Fine-Tuning Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6543a935.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#quantization" class="md-skip">
          콘텐츠로 이동
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="상단/헤더">
    <a href="../.." title="LLM Fine-Tuning Course" class="md-header__button md-logo" aria-label="LLM Fine-Tuning Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM Fine-Tuning Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              양자화 기법
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber"  aria-label="라이트 모드로 전환"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="라이트 모드로 전환" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="amber" data-md-color-accent="amber"  aria-label="다크 모드로 전환"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="다크 모드로 전환" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="검색" placeholder="검색" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="검색">
        
        <button type="reset" class="md-search__icon md-icon" title="지우기" aria-label="지우기" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            검색 초기화
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="네비게이션" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM Fine-Tuning Course" class="md-nav__button md-logo" aria-label="LLM Fine-Tuning Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    LLM Fine-Tuning Course
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    홈
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    기본 개념
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            기본 개념
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/llm_background/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Background 지식
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/fine_tuning_basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-Tuning 기본
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/model_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    모델 유형 및 특징
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/llm_paradigm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM 패러다임
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    데이터 준비
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            데이터 준비
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_preparation/dataset_creation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    데이터셋 준비
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_preparation/templates_formats/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    템플릿과 포맷
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    튜닝 기법
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            튜닝 기법
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../continual_pretraining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CPT (Continued Pre-Training)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supervised_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SFT (Supervised Fine-Tuning)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preference_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO (Direct Preference Optimization)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft_methods/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT 방법론
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    양자화 기법
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    양자화 기법
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="목차">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      목차
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      1. 주요 Quantization 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 주요 Quantization 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      각 양자화 방식 간 비교
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quantization의 장점 (속도, 메모리)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-quatization" class="md-nav__link">
    <span class="md-ellipsis">
      3. Quatization 단점
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-2023" class="md-nav__link">
    <span class="md-ellipsis">
      3. 최근 연구 동향 (2023년을 중심으로)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-20232025" class="md-nav__link">
    <span class="md-ellipsis">
      4. 최근 양자화 적용 사례 (2023~2025)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      5. Quantization 관련 주요 도구 및 라이브러리
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Quantization 관련 주요 도구 및 라이브러리">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bitsandbytes" class="md-nav__link">
    <span class="md-ellipsis">
      bitsandbytes 라이브러리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      참고 문헌
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../korean_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    한국어 확장 튜닝
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    모델 평가
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            모델 평가
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/benchmarks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    벤치마크 및 평가
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/llm_as_judge/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM as Judge
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/serving_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    서빙 및 최적화
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    활용 사례
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            활용 사례
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/alpaca/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alpaca
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/deepseek/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DeepSeek
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/zephyr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zephyr
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  <span class="md-ellipsis">
    실습 가이드
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            실습 가이드
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/gpt_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI GPT 파인튜닝
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_ipynb/1_openai_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT (Chat Model) 파인튜닝 하기!
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/open_weight_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    오픈 웨이트 파인튜닝
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/dpo_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO 튜닝
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/reasoning_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    추론 모델 개발
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="">
            
  
  <span class="md-ellipsis">
    연구 동향 및 프로젝트
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            연구 동향 및 프로젝트
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/latest_research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    최신 연구 동향
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/license_data_issues/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    데이터 및 라이선스
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/competition_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    경쟁 모델 개발
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/domain_specific/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    도메인 특화 모델
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
            
  
  <span class="md-ellipsis">
    About
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            About
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/jonhpark7966/courses_archive" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Other Courses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://sudormrf.run/jong-hyun-park/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Author
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="목차">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      목차
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      1. 주요 Quantization 기법
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. 주요 Quantization 기법">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      각 양자화 방식 간 비교
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quantization의 장점 (속도, 메모리)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-quatization" class="md-nav__link">
    <span class="md-ellipsis">
      3. Quatization 단점
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-2023" class="md-nav__link">
    <span class="md-ellipsis">
      3. 최근 연구 동향 (2023년을 중심으로)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-20232025" class="md-nav__link">
    <span class="md-ellipsis">
      4. 최근 양자화 적용 사례 (2023~2025)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      5. Quantization 관련 주요 도구 및 라이브러리
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Quantization 관련 주요 도구 및 라이브러리">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bitsandbytes" class="md-nav__link">
    <span class="md-ellipsis">
      bitsandbytes 라이브러리
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      참고 문헌
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="quantization">양자화(Quantization)<a class="headerlink" href="#quantization" title="Permanent link">&para;</a></h1>
<p>LLM에 적용되는 <strong>양자화(quantization)</strong> 기법들은 모델의 가중치와/또는 활성값을 저정밀도로 표현하여 메모리와 연산량을 줄입니다.
컴퓨터에서 기본적으로 사용하는 정밀도는 Floating Point 32bit 또는 64bit 인데, LLM 에서는 이만큼 정밀한 것이 꼭 필요하지는 않습니다.<br />
이는 뉴럴넷 알고리즘에 모두 해당하는 이야기로 LLM 뿐만 아니라 CNN (Convolutional Neural Network) 기반 모델들 에서도 애용되었던 방법입니다.  </p>
<h2 id="1-quantization">1. 주요 Quantization 기법<a class="headerlink" href="#1-quantization" title="Permanent link">&para;</a></h2>
<p>가장 기본이 되는 FP16 과 BF16 에 대해서 보겠습니다. </p>
<div style="text-align: center;">
  <img src="../../rscs/quantization.png" alt="quatization">
  <p><em>Quantization</em></p>
</div>

<ul>
<li>
<p><strong>FP16 (반정밀도 부동소수점)</strong>: 16비트 부동소수점으로 표준 32비트 대신 연산하는 방식입니다. 현재 대규모 모델 훈련과 추론에서 기본으로 활용되는 기법으로, <strong>메모리 사용을 절반으로 줄이고 연산 속도를 높이면서도</strong> 모델 성능 저하가 미미합니다 FP16 사용 시 큰 속도 향상을 얻을 수 있습니다. 대부분의 최신 LLM들은 FP32보다 FP16/BF16(Brain Floating Point)으로 학습/추론되는 것이 일반적입니다.</p>
</li>
<li>
<p><strong>BF16 (Brain Floating Point 16)</strong>: 요즘 GPU 에서 기본으로 취급 받는 포맷이죠. BF16은 Google Brain 팀이 개발한 16비트 부동소수점 형식으로, FP32의 <strong>지수부(8비트)를 그대로 유지</strong>하고 가수부만 줄인 형태입니다. 이 특징 덕분에 <strong>FP32와 동일한 넓은 수 범위</strong>를 표현하면서도 메모리는 절반만 사용합니다. FP16과 달리 <strong>오버플로우/언더플로우 위험이 적어</strong> 별도의 손실 스케일링 없이도 안정적인 학습이 가능합니다. NVIDIA A100/H100, Google TPU v3+ 등 최신 AI 하드웨어에서 기본 지원되며, <strong>FP32 대비 2배 이상의 처리량</strong>을 제공합니다. 실제로 GPT-4, LLaMA, DeepSeek 등 대부분의 최신 LLM들이 BF16으로 학습되었으며, 현재 딥러닝 학습의 사실상 표준 포맷으로 자리잡았습니다. 최근에는 FP8과 함께 혼합 정밀도 학습에 활용되어 DeepSeek-V3 같은 초거대 모델(671B)의 효율적 학습을 가능케 했습니다.</p>
</li>
</ul>
<blockquote>
<p>참고 - NVIDIA GPU 기준 RTX 30XX 부터 BF16을 지원하기 떄문에, 옛날 GPU 들은 이 부분에서 호환이 안되어 문제되는 경우가 많습니다. 주의하세요!!!</p>
</blockquote>
<ul>
<li><strong>INT8 (8비트 정수 양자화)</strong>: 가중치나 활성값을 8비트 정수로 표현하는 방식입니다. Float 아니고 Int 로요. Float 과 Int 의 차이에 대해서는 여기서는 이 글을 보시는 분들은 다 안다고 가정하겠습니다. 하드웨어적으로도 int 는 연산에 이점이 꽤 있습니다만, 문제는 표현 범위가 작죠. 그래서 특정 순간의 값들이 잘려버릴 수가 있습니다. 성능에 꽤 치명적일 수 있겠죠. 이를 보완하기 위해 <strong>벡터별 스케일 조정</strong> 등 여러 기법들이 개발되었습니다.</li>
</ul>
<hr />
<p>위 방법들은 Training 단계에서도 쓰이는 범용적인 방법이라면, GPTQ/AWQ 는 학습이 모두 완료된 후 (Post Training Quatization) Inference 의 효율을 높이기 위한 방법들 입니다. </p>
<ul>
<li>
<p><strong><a href="https://arxiv.org/abs/2210.17323">GPTQ</a> (Generative Pre-trained Transformer Quantization)</strong>: 2022년 말 제안된 Post Training Quantization (PTQ) 알고리즘입니다. 모델의 각 층을 한 번에 최적화하여 4비트 또는 3비트로 <strong>1회(pass)만에 양자화</strong>하는 효율적인 방법입니다. GPTQ는 <strong>2차 최적화 정보(Hessian 근사)</strong>를 활용해 양자화 오차를 최소화하며, 175B같은 초거대 모델도 <strong>몇 시간 내 3~4비트로 변환하면서도 정확도 손실이 거의 없음을</strong> 보여주었습니다. 실제로 GPTQ로 3~4비트로 압축한 모델은 원본 FP16 모델 대비 <strong>성능 저하가 무시할 만큼 작고</strong>, 모델 크기는 16분의 1 수준으로 줄어듭니다. 이를 통해 <strong>175B 규모 모델도 단일 GPU에서 실행 가능</strong>함을 시연하였습니다.    </p>
</li>
<li>
<p><strong><a href="https://arxiv.org/abs/2306.00978">AWQ</a> (Activation-aware Weight Quantization)</strong>: 2023년 발표되었고, GPTQ와 유사하게 가중치만 저비트로 양자화하는 PTQ 방법이지만, <strong>활성값 분포를 참고하여 "중요한" 가중치 채널 1%만 보호</strong>한다는 아이디어가 핵심입니다. <strong>일부 채널의 가중치는 스케일을 키워 양자화 오차를 줄이고</strong> (추론 시에는 다시 스케일 보정), 나머지는 균일하게 양자화합니다. 이렇게 하면 <strong>혼합 정밀도 없이도</strong> 중요한 가중치의 정보를 유지할 수 있어 하드웨어 효율성을 해치지 않습니다. 특히 <strong>Instructio 모델이나 멀티모달 LLM까지</strong> 4비트로 양자화해도 정확도를 높게 유지하는 성과를 달성했습니다.</p>
</li>
<li>
<p><strong>기타 저비트 양자화</strong>: 상기 외에도 4비트 이하 정밀도로의 도전이 이어지고 있습니다. 예를 들어 <strong>INT4 (4비트 정수)</strong> 양자화는 메모리 극소화 장점 때문에 많이 연구되며, GPTQ/AWQ 같은 최신 방법도 본질적으로는 INT4에 해당합니다. 일부 연구는 <strong>INT3</strong>이나 <strong>INT2</strong>까지 실험하고 있으나, 2비트 이하에서는 성능 저하가 두드러져 실용성이 낮습니다.</p>
</li>
</ul>
<h3 id="_1">각 양자화 방식 간 비교<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>전반적으로, <strong>FP16</strong>은 비교적 <strong>안정적인 성능</strong>을 유지하면서 <strong>메모리 절약과 속도 향상</strong>을 얻는 기본 기법이고, <strong>INT8</strong>은 추가 절반의 메모리 절감을 제공하지만 <strong>정확도 관리가 중요</strong>합니다. <strong>GPTQ/AWQ 같은 4비트 PTQ 알고리즘</strong>들은 <strong>최소한의 정확도 손실로 최대 메모리 압축</strong>을 노리는 최신 방법으로, 기존 8비트보다 훨씬 높은 압축률을 달성합니다. GPTQ는 <strong>가중치 재구성 최적화</strong>로 정확도를 높인 반면, AWQ는 <strong>활성값 기반 채널 스케일링</strong>으로 <strong>하드웨어 효율</strong>을 강조한 차이가 있습니다. 또한 <strong>양자화 대상</strong> 측면에서, 위 기법들은 <strong>주로 가중치(weight)</strong>를 양자화하지만, <strong>SmoothQuant</strong> 등 일부 연구는 <strong>활성값(activation)</strong>까지 8비트로 함께 양자화하여 <strong>엔드투엔드 INT8 추론</strong>을 가능케 했습니다 활성값 양자화는 난이도가 더 높지만, 성공하면 추가적인 메모리/대역폭 이점을 줍니다.</p>
<h2 id="2-quantization">2. Quantization의 장점 (속도, 메모리)<a class="headerlink" href="#2-quantization" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>속도 개선 효과</p>
<ul>
<li>양자화는 연산당 비트수가 줄어들어 <strong>처리해야 할 데이터량이 감소</strong>하고, 하드웨어의 저정밀도 연산 기능을 활용하여 <strong>추론 속도를 향상</strong>시킬 수 있습니다. 예를 들어 GPTQ로 4비트로 압축된 모델은 동일 GPU에서 FP16 모델 대비 <strong>약 3.25배 빨라졌다는</strong> 보고가 있습니다 (A100 GPU 기준).</li>
<li>최신 AI 하드웨어에서는 이점이 더 커지는데, FP8의 경우 NVIDIA Hopper 계열 GPU에서 FP16의 두 배에 달하는 연산 처리량(FLOPS)을 제공해 학습 속도를 대폭 향상 시켰습니다.</li>
<li>속도 개선 정도는 하드웨어 지원에 따라 매우(!!!!) 상이합니다. 일부 GPU에서는 4비트 연산을 직접 지원하지 않아 4비트 -&gt; 16비트 변환 후 연산하기도 하므로, 그런 경우 이득이 메모리 절감에 국한될 수 있습니다. </li>
<li>메모리 대역폭 bottleneck이 있는 대규모 모델에서는 전송해야 할 데이터량이 줄어드는 것만으로도 throughput 향상 효과가 발생합니다.</li>
<li>당연히 전기값도 적게 나가겠죠.</li>
</ul>
</li>
<li>
<p>메모리/저장 공간 절감</p>
<ul>
<li>양자화의 가장 직접적인 이점은 <strong>모델 메모리 풋프린트 감소</strong>입니다. 정밀도를 1/2, 1/4로 줄이면 그만큼 필요한 저장 공간과 메모리도 감소합니다</li>
<li><strong>QLoRA</strong> 기법은 7~13B급 모델을 4비트로 압축하여 Single GPU 메모리로도 Fine Tuning할 수 있음을 보여주었습니다 (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>).</li>
<li>실제로 70B 파라미터 LLM을 4비트로 로드하면 약 40~48GB 정도로도 처리 가능합니다. BF16 원본 모델을 로드하려면 130GB 이상 필요했던 것과 차이가 크죠....</li>
<li>이처럼 메모리 감소는 연구자나 개발자가 <strong>저비용 장비</strong>에서 실험하거나, <strong>모바일/엣지 디바이스</strong>에서 대규모 모델을 구동할 수 있도록 해주는 핵심 방안입니다</li>
</ul>
</li>
</ul>
<h2 id="3-quatization">3. Quatization 단점<a class="headerlink" href="#3-quatization" title="Permanent link">&para;</a></h2>
<p>그렇다면, 정밀도를 희생한만큼 모델이 멍청해질 텐데요... 얼마나 안 좋을까요?? 
<a href="https://qwen.readthedocs.io/en/latest/benchmark/quantization_benchmark.html">Qwen</a> 에서 제공하는 벤치마크 결과를 보면 양자화 방식에 따른 성능 저하를 확인할 수 있습니다. </p>
<table>
<thead>
<tr>
<th>모델</th>
<th>양자화 방식</th>
<th>Average</th>
<th>MMLU</th>
<th>C-Eval</th>
<th>IFEval</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Qwen2-72B-Instruct</strong></td>
<td>BF16</td>
<td>81.3</td>
<td>82.3</td>
<td>83.8</td>
<td>77.6</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int8</td>
<td>80.7</td>
<td>81.3</td>
<td>83.4</td>
<td>77.5</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int4</td>
<td>81.2</td>
<td>80.8</td>
<td>83.9</td>
<td>78.9</td>
</tr>
<tr>
<td></td>
<td>AWQ</td>
<td>80.4</td>
<td>80.5</td>
<td>83.9</td>
<td>76.9</td>
</tr>
<tr>
<td><strong>Qwen2-7B-Instruct</strong></td>
<td>BF16</td>
<td>66.9</td>
<td>70.5</td>
<td>77.2</td>
<td>53.1</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int8</td>
<td>66.2</td>
<td>69.1</td>
<td>76.7</td>
<td>52.9</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int4</td>
<td>64.1</td>
<td>67.8</td>
<td>75.2</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>AWQ</td>
<td>64.1</td>
<td>67.4</td>
<td>73.6</td>
<td>51.4</td>
</tr>
<tr>
<td><strong>Qwen2-1.5B-Instruct</strong></td>
<td>BF16</td>
<td>48.4</td>
<td>52.4</td>
<td>63.8</td>
<td>29.0</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int8</td>
<td>48.1</td>
<td>53.0</td>
<td>62.5</td>
<td>28.8</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int4</td>
<td>45.0</td>
<td>50.7</td>
<td>57.4</td>
<td>27.0</td>
</tr>
<tr>
<td></td>
<td>AWQ</td>
<td>46.5</td>
<td>51.6</td>
<td>58.1</td>
<td>29.9</td>
</tr>
<tr>
<td><strong>Qwen2-0.5B-Instruct</strong></td>
<td>BF16</td>
<td>34.4</td>
<td>37.9</td>
<td>45.2</td>
<td>20.0</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int8</td>
<td>32.6</td>
<td>35.6</td>
<td>43.9</td>
<td>18.1</td>
</tr>
<tr>
<td></td>
<td>GPTQ-Int4</td>
<td>29.7</td>
<td>33.0</td>
<td>39.2</td>
<td>16.8</td>
</tr>
<tr>
<td></td>
<td>AWQ</td>
<td>31.1</td>
<td>34.4</td>
<td>42.1</td>
<td>16.7</td>
</tr>
</tbody>
</table>
<p>위 표에서 볼 수 있듯이, 양자화로 인한 성능 저하는 모델 크기에 따라 다르게 나타납니다:</p>
<ul>
<li><strong>대형 모델(72B)</strong>: 양자화를 적용해도 성능 저하가 거의 없으며, 일부 벤치마크에서는 오히려 GPTQ-Int4가 BF16보다 좋은 성능을 보이기도 합니다.</li>
<li><strong>중형 모델(7B)</strong>: 8비트 양자화는 미미한 성능 저하만 보이지만, 4비트 양자화에서는 2-3% 정도의 성능 감소가 관찰됩니다.</li>
<li><strong>소형 모델(1.5B, 0.5B)</strong>: 모델 크기가 작을수록 양자화의 영향이 더 커지며, 특히 4비트 양자화에서는 5% 이상의 성능 저하가 발생할 수 있습니다.</li>
</ul>
<p>이러한 결과는 대형 모델일수록 파라미터 중복성이 높아 양자화에 더 강건하다는 것을 시사합니다. 실제 응용에서는 모델 크기, 요구되는 성능, 하드웨어 제약 등을 종합적으로 고려하여 적절한 양자화 방식을 선택해야 합니다.</p>
<hr />
<p>어떻게 quatization 을 했는데도 성능이 많이 떨어지지 않을까요?</p>
<ul>
<li>표현 비트수가 줄어들면 weight 들이 <strong>양자화 오차</strong>로 인해 손실되고, 이는 누적되어 출력 품질 하락으로 이어질 수 있습니다. 특히 LLM처럼 파라미터 수가 매우 큰 모델에서는 일부 계층의 <strong>이상치(outlier)</strong> 값이 결과에 큰 영향을 주는데, 이러한 값을 저정밀도로 표현하면 오차가 커집니다.</li>
<li>최근 연구들은 다양한 보완책을 통해 양자화로 인한 성능 저하를 극복하고 있습니다. <strong>벡터별 스케일 조정</strong>이나 <strong>채널별 중요도 스케일링</strong> 기법(LLM.int8(), AWQ 등)은 양자화 <strong>오차를 줄여 정확도 손실을 최소화</strong>합니다.</li>
</ul>
<hr />
<p>위 Qwen의 실험 결과는 post training quantization 기반의 결과 인데요, 학습 단계에서 양자화를 잘하면 성능저하를 더 줄일 수도 있습니다. </p>
<ul>
<li>QLoRA 와 같은 Quantized 된 상태에서의 학습은 성능 손실을 보완합니다. QLoRA 연구에 따르면, 4비트로 양자화하여 발생하는 성능 저하는 <strong>LoRA 미세튜닝 단계를 거치면 완전히 회복</strong>할 수 있다고 주장합니다 <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></li>
<li>Quantization-Aware Training 을 통해 학습 단계에서부터 양자화 오차에 적응시키는 방법도 있습니다. 대표적으로 NVIDIA의 FP8 학습에서는 학습 중간중간 FP8로 변환해 연산하면서도, 일부 민감한 부분은 BF16/FP32로 유지하는 <strong>혼합 정밀 학습</strong>을 적용해 학습 안정성을 확보했습니다<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup><sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, 이는 LLM 시대 이전에 CNN 기반의 딥러닝 모델들에서도 자주 사용되었던 방법이죠.</li>
</ul>
<table>
<thead>
<tr>
<th>💡 필자의 의견</th>
</tr>
</thead>
<tbody>
<tr>
<td>Quantization 은 아주 좋은 선택지 입니다. 특히나 GPU 메모리 제약으로 인해 진입 장벽이 높은 이 시점에서요. Inferece 기준, 80GB 가 있다면 32B BF16 모델 보다 70B Quatized 모델을 사용하는 것이 제 경혐상 항상 좋았습니다. SFT 학습 기준으로도 동일 GPU 기준 모델을 최대한 키우고 QLoRA 를 사용하는 것이 대부분 제일 좋은 선택이었습니다.</td>
</tr>
</tbody>
</table>
<h2 id="3-2023">3. 최근 연구 동향 (2023년을 중심으로)<a class="headerlink" href="#3-2023" title="Permanent link">&para;</a></h2>
<p>지난 2년간 LLM 양자화 분야는 급속히 발전했습니다. 주요 동향은 다음과 같습니다:</p>
<ul>
<li>
<p><strong>2022년: 8비트 양자화 시대</strong> – LLM.int8()이 <strong>벡터 단위 양자화와 이상치 16비트 처리</strong>로 성능 저하 없는 8비트 연산을 구현했습니다. SmoothQuant는 <strong>활성값 outlier 완화</strong>를 통해 <strong>가중치와 활성값 모두 INT8로 양자화</strong>하여 <strong>2배 메모리 절감, 1.5배 속도 향상</strong>을 달성했습니다.</p>
</li>
<li>
<p><strong>2023년: 4비트 양자화 확산</strong> – GPTQ는 <strong>헤시안 근사 기반 one-shot 양자화</strong>로 175B 모델을 4비트로 압축하면서도 성능 저하를 최소화했습니다. QLoRA는 <strong>4비트 고정 모델에 LoRA 어댑터 훈련</strong>을 적용해 양자화 손실을 완전히 회복했으며, 65B 모델을 단일 GPU에서 효율적으로 튜닝했습니다. AWQ는 <strong>중요 가중치 1%만 스케일 보정</strong>하는 방식으로 <strong>도메인 무관하게</strong> 4비트 양자화를 적용했고, 지시형 LLM과 멀티모달 모델에서도 좋은 성능을 보였습니다.</p>
</li>
<li>
<p><strong>2024년: 학습 단계 양자화</strong> – DeepSeek-V3는 <strong>FP8 정밀도를 학습에 도입한 최초의 오픈소스 LLM</strong>으로, <strong>128×128 블록 단위 fine-grained 양자화</strong>와 <strong>E4M3 포맷</strong>을 활용해 <strong>BF16 대비 0.25% 이내 손실</strong>로 6710억 파라미터 MoE 모델을 학습했습니다. 이로써 양자화는 모델 학습 단계와 초거대 모델로까지 확장되었습니다.</p>
</li>
</ul>
<h2 id="4-20232025">4. 최근 양자화 적용 사례 (2023~2025)<a class="headerlink" href="#4-20232025" title="Permanent link">&para;</a></h2>
<p>최근 공개된 LLM에서 양자화 기술이 실제로 아주 활발히! 활용되고 있습니다:</p>
<ul>
<li>
<p><strong>DeepSeek-V3 및 R1 (2024~2025)</strong></p>
<ul>
<li>DeepSeek-V3는 <strong>671B 매개변수의 MoE 모델</strong>로, <strong>학습과 추론에 FP8 정밀도를 전면 활용</strong>했습니다.</li>
<li>이 모델은 <strong>128×128 블록별 스케일링</strong>과 <strong>E4M3 포맷</strong>을 사용해 <strong>outlier 문제를 해결</strong>했고, 결과적으로 <strong>BF16 대비 0.25% 이내 손실</strong>로 안정적인 학습을 완료했습니다.</li>
<li>DeepSeek-R1도 동일한 FP8 + MoE 구조를 채택했으며, 두 모델 모두 TensorRT-LLM이나 vLLM 등 서빙 프레임워크를 통해 FP8로 추론이 가능합니다. </li>
<li>DeepSeek은 그냥 모두가 인정하는 압도적을 성능을 보였기 때문에, 2025년 부터는 다들 Pre-Train 단계에서 부터 FP8 혼합 학습을 하게 될 것 같네요. </li>
</ul>
</li>
<li>
<p><strong>Meta LLaMA-2 및 오픈 LLM들 (2023)</strong>: LLaMA-2 모델은 공개 직후부터 다양한 양자화 버전이 등장했습니다. <strong>HuggingFace Transformers</strong>는 <code>load_in_8bit</code>/<code>load_in_4bit</code> 옵션으로 <strong>bitsandbytes 기반 양자화</strong>를 지원하며, <strong>AWQ, GPTQ 기반 4비트 LLaMA-2 70B</strong> 모델이 공개되어 있습니다. 커뮤니티에서는 <strong>GGML/GGUF</strong> 포맷으로 8bit부터 3bit까지 다양한 양자화 모델을 배포하고 있으며, 이는 LLM 양자화가 <strong>실험실 단계를 넘어 실제 배포와 활용 단계</strong>로 진입했음을 보여줍니다.</p>
</li>
<li>
<p><strong>경량 디바이스/온디바이스 사례</strong>: 이제 LLM 은 온디바이스 에서의 상용화 단계를 시작하는 것 같습니다. CNN 이 그러했든 Quatization 은 필수적으로 할 것입니다. 최근 1-2년간은 <strong>"가능한 한 양자화해서 사용하라"</strong>가 대규모 모델 배포의 암묵적인 모토가 될 정도로, 양자화는 필수적인 요소로 자리잡은 것 같네요.</p>
</li>
</ul>
<h2 id="5-quantization">5. Quantization 관련 주요 도구 및 라이브러리<a class="headerlink" href="#5-quantization" title="Permanent link">&para;</a></h2>
<h3 id="bitsandbytes"><strong>bitsandbytes 라이브러리</strong><a class="headerlink" href="#bitsandbytes" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>bitsandbytes</strong>는 Tim Dettmers가 개발한 <strong>PyTorch용 경량화 연산 라이브러리</strong>로, LLM의 양자화에 사실상 표준처럼 가장 많이 활용되고 있습니다.</li>
<li>8비트 Adam 등의 <strong>8비트 최적화 알고리즘</strong>으로 유명해졌고, 이후 <strong>8-bit 및 4-bit 행렬 곱셈 지원</strong>을 추가하여 모델 Weight의 효율적 양자화를 가능케 했습니다. HuggingFace Transformers에 통합되어, 예를 들어 <code>AutoModel.from_pretrained(..., load_in_8bit=True)</code>처럼 호출하면 내부적으로 bitsandbytes를 이용해 모델을 <strong>INT8 양자화 모드로 로드</strong>합니다.</li>
<li>bitsandbytes의 8bit 구현은 <strong>LLM.int8() 기법</strong>(벡터 단위 스케일링 + Outlier 16bit 연산)을 포함하고 있어, <strong>대부분의 연산을 INT8로 처리하면서도 정확도를 거의 손상시키지 않습니다</strong>. </li>
<li>4bit 로드 (<code>load_in_4bit=True</code>)도 지원하는데, 이는 QLoRA 연구에서 도입된 <strong>NF4 (NormalFloat4) 데이터 타입</strong>을 사용합니다. NF4는 <strong>가중치 분포의 분위수에 기반한 4비트 양자화</strong>로, 균일 quantization보다 정보를 더 잘 보존합니다. bitsandbytes를 통해 모델을 4bit로 로드하면 내부적으로 <strong>블록별 (기본 64개 요소)로 가중치를 quantize</strong>하고, <strong>양자화 스케일 값들은 별도 테이블</strong>에 저장합니다. 메모리 절감 효과는 매우 커서, 예를 들어 130억 파라미터 모델도 4bit로는 약 6.5GB 정도면 적재가 가능합니다.</li>
<li><strong>성능 측면</strong>에서 bitsandbytes 양자화는 주로 메모리와 <strong>GPU 대역폭 이득</strong>을 통해 간접적인 속도 향상을 주며, H100처럼 8bit 연산이 특화된 하드웨어에서는 <strong>추론 속도가 FP16 대비 향상</strong>되기도 합니다. 다만 4bit의 경우 아직 low-level GPU 지원이 없으므로, 8bit 연산으로 시뮬레이션하는 형태라 <strong>속도 이점은 크지 않고 메모리 이점이 주효</strong>합니다.</li>
<li>bitsandbytes는 오늘날 <strong>수많은 LLM 압축 프로젝트의 기반</strong>으로 쓰이고 있으며, 오픈소스 LLM을 취급하는 HuggingFace, LangChain 등의 생태계에서 표준 구성요소로 자리잡았습니다.</li>
<li>
<p>한 가지 유의할 점은, bitsandbytes로 양자화한 모델을 다룰 때 <strong>연산 자체는 여전히 FP16으로 수행</strong>되는 경우가 많다는 것입니다. 예컨대 4bit로 로드된 가중치는 계산 시 BF16으로 디퀀타이즈되어 곱셈이 이루어지므로, <strong>메모리에는 이득이 있지만 계산적 정밀도가 4bit인 것은 아닙니다</strong>. 이는 정확도를 보존하기 위한 트레이드오프로 이해할 수 있습니다. 전체적으로, bitsandbytes는 <strong>사용의 편의성, 안정성, 성능</strong> 측면에서 LLM 양자화를 실용화하는데 큰 기여를 한 라이브러리입니다.</p>
</li>
<li>
<p><strong>적용 방법</strong>: HuggingFace Transformers에서 <code>from_pretrained</code> 호출 시 <code>load_in_8bit</code> 또는 <code>load_in_4bit</code>를 지정하거나, <code>BitsAndBytesConfig</code>를 사용하여 세부 설정(양자화 임계값 등)을 조절합니다. 또는 GPTQ처럼 사후 양자화된 모델 가중치를 불러올 수도 있습니다. 예시:
  <div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;... (ANY MODEL)&quot;</span><span class="p">,</span>
    <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
  이렇게 하면 weights가 8bit로 압축되어 로드되고, 추론 시 내부적으로 bitsandbytes의 최적화된 8bit 연산을 사용합니다. 4bit도 동일합니다.</p>
</li>
<li>
<p><strong>기능 요약</strong>: </p>
<ul>
<li><strong>8-bit 옵티마이저</strong> – 학습 시 옵티마이저 상태 메모리를 8bit로 줄여 대용량 모델 학습 메모리 절감</li>
<li><strong>INT8 모델 로드</strong> – 추론 시 가중치 8bit 양자화 및 outlier 처리</li>
<li><strong>INT4 모델 로드</strong> – 4bit 양자화 (NF4) 지원</li>
<li><strong>기타</strong> – GPU 메모리 할당 최적화, 장치간 메모리 이동 최적화 등</li>
</ul>
</li>
</ul>
<h2 id="_2">참고 문헌<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>QLoRA: Efficient Finetuning of Quantized LLMs, https://ar5iv.labs.arxiv.org/html/2305.14314&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>DeepSeek v3 and R1 Model Architecture: Why it's powerful and economical, https://fireworks.ai/blog/deepseek-model-architecture&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>deepseek-ai/DeepSeek-V3, https://huggingface.co/deepseek-ai/DeepSeek-V3&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Understanding LLM.int8() Quantization, https://picovoice.ai/blog/understanding-llm-int8/&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, https://arxiv.org/abs/2211.10438&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, https://arxiv.org/abs/2210.17323&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA, https://huggingface.co/blog/4bit-transformers-bitsandbytes&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, https://arxiv.org/abs/2306.00978&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Quantization, https://huggingface.co/docs/transformers/main/en/main_classes/quantization&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>DeepSeek-V3/README_WEIGHTS.md, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/README_WEIGHTS.md&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="하단/푸터" >
        
          
          <a href="../peft_methods/" class="md-footer__link md-footer__link--prev" aria-label="이전: PEFT 방법론">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                이전
              </span>
              <div class="md-ellipsis">
                PEFT 방법론
              </div>
            </div>
          </a>
        
        
          
          <a href="../korean_tuning/" class="md-footer__link md-footer__link--next" aria-label="다음: 한국어 확장 튜닝">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                다음
              </span>
              <div class="md-ellipsis">
                한국어 확장 튜닝
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.path", "navigation.instant", "navigation.tracking", "navigation.sections", "navigation.footer", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\ud074\ub9bd\ubcf4\ub4dc\uc5d0 \ubcf5\uc0ac\ub428", "clipboard.copy": "\ud074\ub9bd\ubcf4\ub4dc\ub85c \ubcf5\uc0ac", "search.result.more.one": "\uc774 \ubb38\uc11c\uc5d0\uc11c 1\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.more.other": "\uc774 \ubb38\uc11c\uc5d0\uc11c #\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.none": "\uac80\uc0c9\uc5b4\uc640 \uc77c\uce58\ud558\ub294 \ubb38\uc11c\uac00 \uc5c6\uc2b5\ub2c8\ub2e4", "search.result.one": "1\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.other": "#\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.placeholder": "\uac80\uc0c9\uc5b4\ub97c \uc785\ub825\ud558\uc138\uc694", "search.result.term.missing": "\ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uac80\uc0c9\uc5b4", "select.version": "\ubc84\uc804 \uc120\ud0dd"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>