# 파인튜닝 데이터셋 준비

## 파인튜닝을 위한 유형별 공개 데이터셋

파인튜닝 데이터셋은 크게 두 가지 유형으로 나눌 수 있습니다:

### 1. 지도 학습(Supervised Fine-Tuning) 데이터셋

지도 학습 데이터셋은 베이스 모델이 사용자 지시에 따라 응답하는 능력을 향상시키는 데 사용됩니다. 주로 질문-답변 쌍이나 대화 형태로 구성됩니다.

#### OpenAssistant Conversations (OASST1)

- **데이터 구성 및 생성**: OpenAssistant 프로젝트에서 크라우드소싱으로 수집한 대화형 지시 응답 데이터셋입니다. 전세계 자원봉사자들이 OpenAssistant 플랫폼에서 사용자 질문과 이에 대한 어시스턴트 응답을 직접 작성하고 평가하여 구축했습니다. 
- **규모와 특징**: 약 161,443개의 메시지(사용자/시스템/어시스턴트 발화)와 35개 언어로 이루어진 대화 데이터가 포함되어 있으며, 각 발화에는 461,292건의 품질 평가가 달려 총 10,000개 이상의 대화 트리를 형성합니다.
- **활용 사례**: OASST1은 ChatGPT 대안이 될 수 있는 공개 챗봇을 훈련하는 데 폭넓게 쓰였습니다. EleutherAI의 Pythia-12B 모델을 OASST1로 미세조정한 oasst-sft-4-pythia-12b가 공개되었고, OpenAssistant 자체도 이 데이터를 활용해 30억~120억 규모 모델을 튜닝했습니다. 품질이 높고 다양한 언어를 포함하고 있어, 이후 여러 오픈소스 ChatGPT류 모델(예: XLM, Falcon 미세튜닝 등)의 지도튜닝 단계에 OASST 데이터가 사용되고 있습니다.

#### Stanford Alpaca (52K Instruction Corpus)

- **데이터 구성 및 생성**: Stanford Alpaca는 LLaMA 7B를 지시 따라하기 모델로 만들며 소개된 52,000개 규모의 영어 지시-응답 데이터셋입니다. Stanford 팀은 먼저 Self-Instruct 방법론을 따라 175개의 인간 작성 시드 지시문을 준비한 뒤, OpenAI의 text-davinci-003 (GPT-3.5) 모델을 활용해 새로운 지시문과 답변을 자동 생성했습니다.
- **생성 방식**: GPT-3.5에 시드 예시를 보여주고 새로운 지시를 만들게 한 다음, 그 지시에 대한 모범 답변을 다시 GPT-3.5로 생성하는 식으로 파이프라인을 단순화하여 52K쌍의 고품질 시연 데이터를 얻었습니다. 데이터 생성 비용은 약 $500 상당의 OpenAI API 사용으로 추산됩니다.
- **활용 사례**: 이 데이터로 LLaMA-7B를 3시간 만에 미세조정한 결과, OpenAI의 text-davinci-003와 유사한 거짓말 및 추론 능력을 보이는 Alpaca 7B 모델이 탄생했습니다. Alpaca의 성공은 저비용 학습으로도 강력한 지시 따르기 성능을 달성할 수 있음을 보여주어 큰 반향을 일으켰습니다. 이후 Alpaca 데이터는 Alpaca-LoRA 등 경량화 모델이나 다른 LLM (예: GPT-J 등)의 지시튜닝에 사용되었고, 많은 후속 연구들이 Alpaca 방식을 참고하여 자체 데이터를 생성하는 계기가 되었습니다. 다만 Alpaca 데이터는 OpenAI 응답을 포함하므로 상업적 사용에는 제약이 있습니다.

#### Databricks Dolly 15K (Human-Generated Instructions)

- **데이터 구성 및 생성**: Dolly 2.0 모델 학습을 위해 Databricks사가 공개한 15,000개 규모의 인간 생성 지시-응답 데이터셋입니다. 5천 명 이상의 Databricks 직원들이 2023년 3~4월에 걸쳐 다양한 형식의 프롬프트와 이에 대한 응답을 직접 작성하여 모았습니다.
- **내용과 특징**: 브레인스토밍, 콘텐츠 생성, 정보 검색, 요약 등 광범위한 과제 유형을 포함합니다. 이 데이터셋(databricks-dolly-15k)은 CC BY-SA 라이선스로 배포되어 연구 및 상업용으로 자유롭게 사용 가능하며, 오픈소스 LLM의 ChatGPT류 인터랙티브 특성을 이끌어내기 위한 첫 대규모 인간 행동 데이터세트로 알려져 있습니다.
- **활용 사례**: Dolly 15K는 상업적 제약이 없는 점 때문에 주목받았으며, 이 데이터를 활용해 EleutherAI의 Pythia-12B를 미세조정한 Dolly 2.0 (12B) 모델이 공개되었습니다. Dolly 2.0은 완전 공개 데이터만으로 학습된 첫 상업가능 ChatGPT 유사 모델로, 이후 기업들이 자체 데이터로 맞춤 LLM을 구축하는 예시가 되었습니다. Dolly 15k 자체도 여러 오픈소스 튜닝 프로젝트에서 참고 데이터로 사용되고 있으며, 규모는 작지만 품질 검증된 인간 생성 응답으로서 가치가 있습니다.

#### ShareGPT (Vicuna) 대화 데이터

- **데이터 구성 및 생성**: Vicuna 모델로 유명해진 ShareGPT 대화 데이터셋은, 실제 ChatGPT 사용자들이 웹사이트 ShareGPT에 공유한 대화 기록 약 70,000건을 모은 것입니다. 각 데이터는 ChatGPT와 사용자 간의 멀티턴 대화(예: 사용자 질문과 연속적인 챗봇 답변들)로 이루어져 있습니다.
- **데이터 처리**: 수집팀(LMSYS)은 공유된 HTML 대화를 모아 마크다운 형식으로 정제하고, 음란하거나 부적절한 대화를 필터링하여 품질을 확보했습니다. 또한 너무 긴 대화는 모델 맥락길이에 맞게 분할하여 사용했습니다. 이 데이터는 ChatGPT의 실제 활용 사례를 담고 있어 다양한 도메인과 현실적 질문이 반영된 것이 특징입니다.
- **활용 사례**: ShareGPT 기반 데이터는 Vicuna-13B 모델의 지도학습에 활용되어 큰 주목을 받았습니다. LLaMA-13B에 이 70k 대화를 1일간 미세조정한 Vicuna-13B는, GPT-4 평가에서 ChatGPT 대비 90% 수준의 성능을 보였다고 보고되었습니다. Vicuna의 등장 이후 많은 오픈챗봇 프로젝트들이 ShareGPT 데이터를 활용하거나, 비슷한 사용자 대화 데이터를 모아 학습을 진행했습니다. 다만 ShareGPT 데이터는 사용자 개인정보나 OpenAI 응답을 포함할 가능성이 있어 라이선스 이슈가 존재합니다.

#### UltraChat (다중턴 AI 생성 대화 데이터)

- **데이터 구성 및 생성**: UltraChat은 2023년 4월 공개된 대규모 멀티턴 대화 데이터셋으로, 모든 데이터가 AI 모델에 의해 자동 생성된 것이 특징입니다. 중국 Tsinghua 대학 NLP 그룹에서 공개한 것으로, OpenAI의 GPT-3.5 Turbo API를 이용해 사용자 역할과 어시스턴트 역할을 시뮬레이션하여 대화를 만들어냈습니다.
- **데이터 구조**: 지식질문(Q&A), 글쓰기/창작, 주어진 자료에 대한 도움의 3가지 시나리오로 나누어, 각 영역에서 수십만 건의 대화를 생성했습니다. 예를 들어 "세계에 대한 질문" 섹터에서는 일반상식부터 전문지식까지 광범위한 질문을 만들고 이에 대한 답변을 생성했고, "글쓰기/창작" 섹터에서는 이메일 작성, 스토리 창작 등 창의적 요청과 그 대응을 생성하는 식입니다. 이렇게 모인 대화는 총 70만~80만 건 규모로, 평균 몇 차례의 질의응답 턴을 포함합니다.
- **활용 사례**: UltraChat 데이터는 Supervised Fine-tuning(SFT) 단계에서 대규모 대화 데이터를 제공함으로써, 강력한 오픈 챗봇 개발에 쓰였습니다. UltraChat을 학습한 UltraLM 시리즈(LLaMA-13B 기반)는 AlpacaEval 벤치마크에서 오픈모델 1위를 기록하는 등 성능을 입증했습니다. 또한 UltraChat은 라이선스 제약이 없는 방대한 대화 자료로서, 다른 연구자들도 자연스러운 대화능력을 모델에 부여하기 위해 활용하고 있습니다. 예컨대 Allen AI의 Tülu-2 모델 개발 시에도 OpenAssistant나 UltraChat같은 커뮤니티 생성 대화 코퍼스를 조합해 고품질 SFT 데이터를 구축하였습니다.

---

#### SimpleScaling S1 (고난도 문제 1K 데이터셋)

| 💡 필자의 의견 |
|---------|
| 25년 3월 현재, 제가 관심을 가지고 있는 실험을 하고 있는 데이터셋 입니다. DeepSeek-R1 을 통해 Reasoning 모델에 대한 힌트가 세상에 알려졌는데요. R1 에서 사용했던 distillation과 비슷한 방식으로 보면 되겠습니다. |


- **데이터 구성 및 생성**: SimpleScaling S1은 Stanford Hashimoto 그룹 등이 2024년 초 제안한 소규모 고품질 데이터셋으로, 단 1,000개의 질문과 각 질문에 대한 정교한 추론 과정(COT) 및 최종 답변으로 이루어져 있습니다. 이 데이터는 주로 수학 경시대회 문제(AIME, MATH) 등 난이도 높은 과제를 포함하며, 각 질문에 대해 GPT-4 기반의 심화 단계 추론(Gemini Thinking)을 실험하고 사람이 검증한 사고흔적(traces)과 정답을 쌍으로 제공합니다.
- **데이터 특성**: 데이터 구축시 문항의 다양성, 난이도, 품질 세 가지 기준을 충족하도록 선별했으며, 일종의 체인-오브-소트(Chain-of-Thought) 포함 데이터셋이라 볼 수 있습니다.
- **활용 사례**: SimpleScaling 팀은 이 S1K 데이터로 Tencent의 Qwen-7B 모델을 미세조정하고, 추론 단계 조절(budget forcing) 기법을 함께 적용하여 S1-32B 모델을 공개했습니다. 작은 데이터셋이지만 고품질 추론 학습 덕분에, S1-32B는 OpenAI의 o1-preview 모델을 수학 문제에서 최대 27%p 상회하는 성능을 보였고, 추론 테스트타임 증강이라는 새로운 방향을 시연했습니다. S1 데이터셋은 규모는 작으나 논리적 사고력 강화에 특화되어 있어, 다른 LLM의 고난도 문제 해결 능력을 향상시키기 위한 보충 학습자료로 활용 가능성을 보여줍니다.


### 2. 선호도 학습(RLHF/DPO) 데이터셋

RLHF(Reinforcement Learning from Human Feedback) 또는 DPO(Direct Preference Optimization) 단계에서는, 모델의 응답 품질을 높이기 위해 선호도 정보가 포함된 데이터셋이 사용됩니다. 이러한 데이터는 대개 한 프롬프트에 대한 두 개 이상의 모델 응답과 인간의 선호 판단(좋음/나쁨)으로 구성됩니다. 선호도 데이터셋은 보상모델 훈련이나, 최근 제안된 DPO 알고리즘을 통한 직접 미세조정에 활용됩니다.

#### Anthropic HH-RLHF (Helpful/Harmless Preferences)

- **데이터 구성 및 생성**: Anthropic이 자사 헌법형 AI 연구에서 공개한 Helpful-Harmless (HH) 대화 선호 데이터셋입니다. 한 프롬프트에 대해 AI 어시스턴트의 두 가지 답변(예: 하나는 도움이 되지만 다른 하나는 무례하거나 유해한 답변)을 제시하고, 인간 평가자가 어느 쪽을 선호하는지를 라벨링한 페어 데이터로 구성됩니다. 전체 약 160k 쌍의 응답 비교 사례가 포함되며, 여러 턴의 대화 맥락도 담겨 있습니다.
- **목적과 특징**: 이 데이터는 Anthropic의 '헬프풀-하름리스' 정책에 따라 어시스턴트가 유용하면서도 안전하게 대화하도록 학습시키기 위해 수집되었습니다.
- **활용 사례**: HH-RLHF 데이터는 Anthropic의 Claude 모델 학습뿐 아니라, 공개 연구에서도 표준 선호도 데이터셋으로 활용됩니다. 예를 들어 Stanford의 DPO 레퍼런스 구현에서는 Anthropic HH 데이터를 선호도 학습에 사용하고 있습니다. 또한 여러 오픈 RLHF 실험(예: Redwood Research 등)에서 모델의 유해발언 억제나 도움되는 답변 선호를 가르치는 용도로 이 데이터를 활용합니다. 이 데이터셋의 등장으로, 비교적 소규모 모델에 대해서도 인간 선호도 적용이 가능해졌으며, 대화 안전성 관련 연구의 기반이 되고 있습니다.

#### Stanford SHP (Human Preferences from Reddit)

- **데이터 구성 및 생성**: SHP (Stanford Human Preferences)는 Stanford팀이 Reddit에서 자연발생적으로 축적된 인간 선호를 추출하여 만든 대규모 텍스트 선호 데이터셋입니다. Reddit의 여러 섹션(요리, 법률 조언 등 18개 주제)에서 질문/요청 게시물과 그에 대한 여러 답글을 수집한 뒤, 커뮤니티 투표(upvote)를 비교함으로써 어느 답변이 더 선호되는지 상대적 판단을 얻었습니다.
- **데이터 특성**: 이렇게 모은 약 38.5만 건의 선호 비교는 모두 사람이 쓴 응답들 간의 우열이므로, 자연스러운 인간 선호 경향을 반영합니다. (예: 같은 질문에 대한 두 개의 댓글 중 하나에 더 많은 추천이 있다면, 그 답변을 선호하는 것으로 간주함.)
- **활용 사례**: SHP는 RLHF 보상모델 학습 초기단계에 널리 쓰이고 있습니다. 예컨대 OpenAI 요약 논문에서도 TL;DR 데이터에 대한 인간 피드백 외에 이런 웹 상의 선호 데이터를 참고하기도 했습니다. Stanford DPO 연구에서도 SFT 후 SHP 데이터로 DPO 미세튜닝을 수행하여 효과를 검증했고, HuggingFace 등 커뮤니티에서도 선호 학습 벤치마크로 활용하고 있습니다. SHP의 장점은 방대한 현실 응답을 기반으로 하여 특정 모델에 편향되지 않은 인간 선호 신호를 준다는 점이며, 이를 통해 LLM이 더 도움이 되는 답변을 생성하도록 유도할 수 있습니다.

#### UltraFeedback (AI 생성 대규모 선호 데이터)

- **데이터 구성 및 생성**: UltraFeedback은 OpenBMB 팀이 확장 가능한 AI 피드백을 통해 만든 대규모 정밀 선호도 데이터셋입니다. 먼저 UltraChat, ShareGPT, Evol-Instruct, TruthfulQA 등 다양한 출처의 64k 프롬프트를 수집한 후, 각 프롬프트마다 여러 LLM들(여러 오픈소스 및 상용 모델)을 활용해 4개의 상이한 응답을 생성했습니다.
- **평가 방식**: 이렇게 얻은 256k개의 모델 응답에 대해, OpenAI GPT-4 모델에게 미리 정의된 기준(지시 준수, 정확성, 정직성, 유용성의 4측면)을 따라 세밀한 평가를 수행하게 하였습니다. GPT-4는 각 응답에 대해 4가지 측면별 점수와 전체 점수를 매겼으며, 그 결과 응답별 세부 피드백과 등급이 부여된 피드백 데이터셋이 구축되었습니다. 이 데이터를 통해 응답 간 약 34만 건의 선호 비교 쌍을 생성할 수 있으며, 모든 샘플에 세밀 평가 코멘트까지 포함된 것이 특징입니다.
- **활용 사례**: UltraFeedback 데이터셋은 보상모델(RM)이나 비평모델(Critic) 학습에 바로 활용되고 있습니다. 이 데이터로 학습한 UltraRM 보상모델을 통해, UltraLM 같은 생성모델에 best-of-n 생성을 시도한 결과 text-davinci-003 대비 92% 정답률로 우수함을 보였습니다. 또한 최근에는 이 선호도 데이터를 활용한 DPO 미세조정도 시도되고 있습니다. 예를 들어 open LLM인 Zephyr-7B는 Mistral-7B를 기반으로 UltraFeedback 선호쌍으로 DPO 학습을 진행하여, AlpacaEval 평가 점수를 향상시켰다는 보고가 있습니다. Allen AI의 Tülu-2 70B 모델 역시 UltraFeedback을 포함한 대량의 선호 데이터로 RLHF/DPO를 수행해 성능을 높였습니다. UltraFeedback은 인간 대신 AI로부터 얻은 피드백이므로 저비용으로 대규모 구축이 가능하며, 이를 통해 오픈소스 LLM의 RLHF 품질을 크게 개선할 수 있음을 입증하고 있습니다.

## 데이터셋 제작 방법론

파인튜닝 데이터셋은 다양한 방식으로 제작되며, 각 방법론은 고유한 장단점을 가지고 있습니다:

### 1. 인간 작성 데이터

- **전문가 제작**: Dolly 15K처럼 전문가나 기업 직원들이 직접 작성한 고품질 데이터셋입니다. 품질이 높고 의도에 맞는 데이터를 얻을 수 있지만, 비용과 시간이 많이 소요됩니다.
- **크라우드소싱**: OASST1처럼 다수의 참여자가 협력하여 구축한 데이터셋입니다. 다양한 관점과 스타일을 포함할 수 있으나, 품질 관리가 어려울 수 있습니다.
- **사용자 공유 데이터**: ShareGPT처럼 실제 사용자들의 대화를 수집한 데이터셋입니다. 실제 사용 패턴을 반영하지만, 개인정보나 저작권 문제가 발생할 수 있습니다.

### 2. AI 생성 데이터

- **Self-Instruct**: Alpaca처럼 소량의 인간 작성 시드 데이터로 AI가 대량의 데이터를 생성하는 방식입니다. 적은 비용으로 대량 데이터를 얻을 수 있지만, 원본 모델의 한계와 편향이 반영될 수 있습니다.
- **AI 시뮬레이션**: UltraChat처럼 AI가 사용자와 어시스턴트 역할을 모두 수행하여 대화를 생성하는 방식입니다. 다양한 시나리오를 빠르게 생성할 수 있으나, 실제 인간 대화의 자연스러움이 부족할 수 있습니다.
- **AI 평가**: UltraFeedback처럼 AI가 다른 모델의 응답을 평가하여 선호도 데이터를 구축하는 방식입니다. 대규모 평가가 가능하지만, 평가 모델의 편향이 결과에 영향을 미칠 수 있습니다.

### 3. 혼합 접근법

- **인간-AI 협업**: SimpleScaling S1처럼 AI가 초안을 생성하고 인간이 검증하거나, 인간이 질문을 만들고 AI가 답변을 생성하는 방식입니다. 품질과 규모의 균형을 맞출 수 있습니다.
- **자연발생 데이터 활용**: SHP처럼 온라인 플랫폼의 자연스러운 상호작용에서 선호도를 추출하는 방식입니다. 실제 인간 선호도를 반영하면서도 대규모 데이터 수집이 가능합니다.
- **데이터 증강**: 기존 데이터셋을 AI로 변형하거나 확장하여 더 많은 학습 데이터를 확보하는 방식입니다. 원본 데이터의 품질을 유지하면서 다양성을 높일 수 있습니다.


## 데이터셋 제작 & 파인튜닝 사례 탐구

### 1. Llama3 의 Hallucination 방지를 위한 데이터셋 

Llama3 Herd of Models 페이퍼에서 발췌해 왔습니다. 

```
Align the model to "Know what it knows"

1. Extract a data snippet from the pre-training data.
2. Generate a factual question about these snippets (context) by prompting Llama 3.
3. Sample responses from Llama 3 to the question.
4. Score the correctness of the generations using the original context as a reference and Llama 3 as a judge.
5. Score the informativeness of the generations using Llama 3 as a judge.
6. Generate a refusal for responses which are consistently informative and incorrect across the generations, using Llama 3.
```

(작성 중)










