
<!doctype html>
<html lang="ko" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LLM Fine-Tuning κΈ°λ²•μ„ λ°°μ°λ” μ›ν¬μƒµ">
      
      
        <meta name="author" content="Jong Hyun Park">
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.28">
    
    
      
        <title>Supervised Fine-Tuning (SFT) - LLM Fine-Tuning Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6543a935.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#supervised-fine-tuning-sft" class="md-skip">
          μ½ν…μΈ λ΅ μ΄λ™
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="μƒλ‹¨/ν—¤λ”">
    <a href="../.." title="LLM Fine-Tuning Course" class="md-header__button md-logo" aria-label="LLM Fine-Tuning Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM Fine-Tuning Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Supervised Fine-Tuning (SFT)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="amber"  aria-label="λΌμ΄νΈ λ¨λ“λ΅ μ „ν™"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="λΌμ΄νΈ λ¨λ“λ΅ μ „ν™" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="amber" data-md-color-accent="amber"  aria-label="λ‹¤ν¬ λ¨λ“λ΅ μ „ν™"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="λ‹¤ν¬ λ¨λ“λ΅ μ „ν™" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="κ²€μƒ‰" placeholder="κ²€μƒ‰" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="κ²€μƒ‰">
        
        <button type="reset" class="md-search__icon md-icon" title="μ§€μ°κΈ°" aria-label="μ§€μ°κΈ°" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            κ²€μƒ‰ μ΄κΈ°ν™”
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="λ„¤λΉ„κ²μ΄μ…" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM Fine-Tuning Course" class="md-nav__button md-logo" aria-label="LLM Fine-Tuning Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    LLM Fine-Tuning Course
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ν™
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    κΈ°λ³Έ κ°λ…
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            κΈ°λ³Έ κ°λ…
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/llm_background/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Background μ§€μ‹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/fine_tuning_basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine-Tuning κΈ°λ³Έ
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/model_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    λ¨λΈ μ ν• λ° νΉμ§•
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/llm_paradigm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM ν¨λ¬λ‹¤μ„
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    λ°μ΄ν„° μ¤€λΉ„
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            λ°μ΄ν„° μ¤€λΉ„
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_preparation/dataset_creation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    λ°μ΄ν„°μ…‹ μ¤€λΉ„
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../data_preparation/templates_formats/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ν…ν”λ¦Ώκ³Ό ν¬λ§·
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    νλ‹ κΈ°λ²•
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            νλ‹ κΈ°λ²•
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../continual_pretraining/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CPT (Continued Pre-Training)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supervised_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SFT (Supervised Fine-Tuning)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preference_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO (Direct Preference Optimization)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../peft_methods/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PEFT λ°©λ²•λ΅ 
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    μ–‘μν™” κΈ°λ²•
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../korean_tuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ν•κµ­μ–΄ ν™•μ¥ νλ‹
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="">
            
  
  <span class="md-ellipsis">
    λ¨λΈ ν‰κ°€
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            λ¨λΈ ν‰κ°€
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/benchmarks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    λ²¤μΉλ§ν¬ λ° ν‰κ°€
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/llm_as_judge/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM as Judge
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../evaluation/serving_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    μ„λΉ™ λ° μµμ ν™”
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    ν™μ© μ‚¬λ΅€
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            ν™μ© μ‚¬λ΅€
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/alpaca/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Alpaca
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/deepseek/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DeepSeek
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../case_studies/zephyr/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Zephyr
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  <span class="md-ellipsis">
    μ‹¤μµ κ°€μ΄λ“
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            μ‹¤μµ κ°€μ΄λ“
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/gpt_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI GPT νμΈνλ‹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_ipynb/1_openai_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT (Chat Model) νμΈνλ‹ ν•κΈ°!
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/open_weight_finetuning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    μ¤ν” μ›¨μ΄νΈ νμΈνλ‹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/dpo_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DPO νλ‹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../practice_guides/reasoning_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    μ¶”λ΅  λ¨λΈ κ°λ°
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="">
            
  
  <span class="md-ellipsis">
    μ—°κµ¬ λ™ν–¥ λ° ν”„λ΅μ νΈ
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            μ—°κµ¬ λ™ν–¥ λ° ν”„λ΅μ νΈ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/latest_research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    μµμ‹  μ—°κµ¬ λ™ν–¥
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/license_data_issues/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    λ°μ΄ν„° λ° λΌμ΄μ„ μ¤
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/competition_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    κ²½μ λ¨λΈ κ°λ°
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../trends_projects/domain_specific/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    λ„λ©”μΈ νΉν™” λ¨λΈ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="">
            
  
  <span class="md-ellipsis">
    About
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            About
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/jonhpark7966/courses_archive" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Other Courses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://sudormrf.run/jong-hyun-park/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Author
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="λ©μ°¨">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      λ©μ°¨
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sft" class="md-nav__link">
    <span class="md-ellipsis">
      SFT κ°μ”
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sft_1" class="md-nav__link">
    <span class="md-ellipsis">
      SFT λ°μ΄ν„° μ¤€λΉ„
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sft_2" class="md-nav__link">
    <span class="md-ellipsis">
      SFT μµμ ν™” μ „λµ
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)<a class="headerlink" href="#supervised-fine-tuning-sft" title="Permanent link">&para;</a></h1>
<h2 id="sft">SFT κ°μ”<a class="headerlink" href="#sft" title="Permanent link">&para;</a></h2>
<ul>
<li>SFTμ λ©μ κ³Ό μ¤‘μ”μ„±</li>
<li>μ…λ ¥-μ¶λ ¥ λ§¤ν•‘μ„ ν†µν• ν•™μµ</li>
</ul>
<h2 id="sft_1">SFT λ°μ΄ν„° μ¤€λΉ„<a class="headerlink" href="#sft_1" title="Permanent link">&para;</a></h2>
<ul>
<li>μ§€μ‹-μ‘λ‹µ μ κµ¬μ„±</li>
<li>λ‹¤μ–‘ν• μ§€μ‹ ν•νƒ ν¬ν•¨ν•κΈ°</li>
<li>ν’μ§ μ¤‘μ‹¬ λ°μ΄ν„° νλ μ΄μ…</li>
</ul>
<h2 id="sft_2">SFT μµμ ν™” μ „λµ<a class="headerlink" href="#sft_2" title="Permanent link">&para;</a></h2>
<ul>
<li>κ³Όμ ν•© λ°©μ§€ κΈ°λ²•</li>
<li>ν•™μµλ¥  μ¤μΌ€μ¤„λ§</li>
<li>μ΅°κΈ° μΆ…λ£ μ „λµ </li>
</ul>
<h1 id="llm-hugging-face-vs-deepspeed-vs-unsloth">λ€κ·λ¨ μ–Έμ–΄λ¨λΈ(LLM) μ§€λ„ νμΈνλ‹: Hugging Face vs. DeepSpeed vs. Unsloth<a class="headerlink" href="#llm-hugging-face-vs-deepspeed-vs-unsloth" title="Permanent link">&para;</a></h1>
<h2 id="introduction">κ°μ” (Introduction)<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>λ€κ·λ¨ μ–Έμ–΄λ¨λΈ(LLM)μ <strong>μ§€λ„ ν•™μµ κΈ°λ° νμΈνλ‹</strong>(Supervised Fine-Tuning, SFT)μ€ μ‚¬μ „ ν•™μµλ λ¨λΈμ„ μƒλ΅μ΄ λ°μ΄ν„°μ…‹μ— λ§μ¶° λ―Έμ„Έμ΅°μ •ν•μ—¬ νΉμ • μ‘μ—… μ„±λ¥μ΄λ‚ μ‘λ‹µ ν’μ§μ„ λ†’μ΄λ” κ³Όμ •μ…λ‹λ‹¤. νΉν <strong>Decoder-Only Transformer</strong> κµ¬μ΅°(μ: GPT κ³„μ—΄ λ¨λΈ)μ νμΈνλ‹μ€ μ£Όμ–΄μ§„ ν”„λ΅¬ν”„νΈμ— μ΄μ–΄μ§€λ” λ‹¤μ ν† ν°μ„ μμΈ΅ν•λ„λ΅ λ¨λΈμ„ ν•™μµμ‹ν‚¤λ” ν•νƒλ΅ μ΄λ£¨μ–΄μ§‘λ‹λ‹¤. μµκ·Ό 2λ…„κ°„ LLM νμΈνλ‹ λ¶„μ•Όμ—μ„λ” <strong>λ¨λΈ ν¬κΈ°μ— λΉ„ν•΄ ν•μ •λ μμ›μΌλ΅λ„ ν¨μ¨μ μΌλ΅ ν•™μµ</strong>ν•  μ μλ” λ‹¤μ–‘ν• κΈ°λ²•κ³Ό λ„κµ¬λ“¤μ΄ λ“±μ¥ν–μµλ‹λ‹¤. λ³Έ λ³΄κ³ μ„μ—μ„λ” Hugging Face μƒνƒκ³„, Microsoft DeepSpeed, κ·Έλ¦¬κ³  μµμ‹  μ»¤λ®¤λ‹ν‹° ν΄μΈ Unslothλ¥Ό ν™μ©ν• <strong>μ§€λ„ νμΈνλ‹ λ°©λ²•</strong>μ„ λΉ„κµν•©λ‹λ‹¤. λν• κ° μ ‘κ·Όλ²•μ <strong>νΉμ§•κ³Ό μ¥μ </strong>, <strong>μµμ‹  μ—°κµ¬ λ™ν–¥</strong>, <strong>μ‹¤λ¬΄ μ μ© μμ  μ½”λ“</strong>, <strong>μ„±λ¥ λ° ν¨μ¨ ν‰κ°€ κΈ°μ¤€</strong>, <strong>Decoder-Only μµμ ν™” κΈ°λ²•</strong> λ“±μ„ μ •λ¦¬ν•©λ‹λ‹¤.</p>
<h2 id="hugging-face-llm">Hugging Face κΈ°λ° LLM νμΈνλ‹<a class="headerlink" href="#hugging-face-llm" title="Permanent link">&para;</a></h2>
<p><strong>Hugging Face</strong>μ Transformers λΌμ΄λΈλ¬λ¦¬λ” λ°©λ€ν• μ‚¬μ „ν•™μµ λ¨λΈ μ €μ¥μ†μ™€ νΈλ¦¬ν• APIλ¥Ό μ κ³µν•μ—¬ LLM νμΈνλ‹μ„ μ†μ‰½κ² μ‹μ‘ν•  μ μκ² ν•΄μ¤λ‹λ‹¤. PyTorch κΈ°λ°μΌλ΅ κµ¬ν„λ <code>Trainer</code> ν΄λμ¤ λλ” <strong>π¤— Accelerate</strong>λ¥Ό ν†µν•΄ λ‹¨μΌ GPUλ¶€ν„° λ¶„μ‚° GPUκΉμ§€ <strong>μ†μ‰¬μ΄ ν•™μµ μ¤ν¬λ¦½νΈ κµ¬μ„±</strong>μ΄ κ°€λ¥ν•©λ‹λ‹¤. Hugging Faceμ μ£Όμ” κ°•μ μ€ <strong>κ΄‘λ²”μ„ν• λ¨λΈ μ§€μ›</strong>κ³Ό <strong>μ»¤λ®¤λ‹ν‹° μ¤‘μ‹¬μ μ‹ μ†ν• κ°μ„ </strong>μ…λ‹λ‹¤. μλ¥Ό λ“¤μ–΄, 2022λ…„ λ°ν‘λ <strong>LoRA</strong>(Low-Rank Adaptation) κΈ° (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘μ„ λΉ λ¥΄κ² PEFT λΌμ΄λΈλ¬λ¦¬μ— ν†µν•©ν•κ³ , 2023λ…„ λ“±μ¥ν• <strong>QLoRA</strong> λ°©λ²•λ΅ λ„ κ³§λ°”λ΅ μ§€μ›ν•μ€μµλ‹ (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=LLMs%20are%20known%20to%20be,the%20QLoRA%20paper%20by%20Dettmers">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. μ΄λ¥Ό ν†µν•΄ μ‚¬μ©μλ” μµμ†ν•μ μ½”λ“ λ³€κ²½λ§μΌλ΅ μµμ‹  μ—°κµ¬ μ„±κ³Όλ¥Ό μ‹¤μµμ— μ μ©ν•  μ μμµλ‹λ‹¤.</p>
<p>Hugging Faceλ” <strong>λ©”λ¨λ¦¬ μµμ ν™”</strong>λ¥Ό μ„ν•΄ 8-bit λ° 4-bit μ–‘μν™”(qunatization)λ¥Ό μ§€μ›ν•©λ‹λ‹¤. μλ¥Ό λ“¤μ–΄ <code>transformers</code>μ—μ„ <code>from_pretrained</code> νΈμ¶ μ‹ <code>load_in_4bit=True</code>λ΅ μ„¤μ •ν•λ©΄, μ‚¬μ „ν•™μµλ λ¨λΈ κ°€μ¤‘μΉλ¥Ό 4λΉ„νΈ μ •λ°€λ„λ΅ λ¶λ¬μ¬ μ μμµλ‹ (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=As%20a%20quickstart%2C%20load%20a,0">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>) (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=The%20basic%20way%20to%20load,that%20will%20be%20automatically%20inferred">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. μ΄λ ‡κ² ν•λ©΄ λ¨λΈ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ ν¬κ² μ¤„μΌ μ μμ–΄, λΉ„κµμ  *<em>μ μ€ GPU λ©”λ¨λ¦¬λ΅λ„ λ€ν• λ¨λΈμ„ λ‹¤λ£° μ μκ² λ©λ‹λ‹¤</em> (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. μ•„λ μμ‹λ” OPT-350M λ¨λΈμ„ 4λΉ„νΈλ΅ λ¶λ¬μ¤λ” μ½”λ“μ…λ‹λ‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">,</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 4λΉ„νΈ μ–‘μν™” λ΅λ“</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>   <span class="c1"># κ°€μ© GPU μλ™ν• λ‹Ή</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">)</span>
</code></pre></div>
<p>λν• Hugging Face PEFT λΌμ΄λΈλ¬λ¦¬λ¥Ό μ‚¬μ©ν•λ©΄ <strong>LoRA μ–΄λ‘ν„°</strong>λ¥Ό μ†μ‰½κ² μ μ©ν•  μ μμµλ‹λ‹¤. LoRAλ” λ¨λΈμ λ¨λ“  κ°€μ¤‘μΉλ¥Ό λ―Έμ„Έμ΅°μ •ν•λ” λ€μ‹ , <strong>μΌλ¶€ λ§¤νΈλ¦­μ¤μ— μ†κ·λ¨μ ν•™μµκ°€λ¥ν• μ €λ­ν¬ ν–‰λ ¬</strong>(Adapters)μ„ μ¶”κ°€ν•μ—¬ ν•™μµν•λ” λ°©λ²•μ…λ‹ (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. Hugging Faceλ” <code>peft.LoraConfig</code>μ™€ <code>get_peft_model</code> λ“±μ„ ν†µν•΄ κΈ°μ΅΄ λ¨λΈμ— LoRA λ¨λ“μ„ μ‚½μ…ν•  μ μλ” APIλ¥Ό μ κ³µν•©λ‹λ‹¤. LoRAλ¥Ό μ‚¬μ©ν•λ©΄ νμΈνλ‹μ‹ <strong>λ©”λ¨λ¦¬μ™€ μ—°μ‚°λ‰μ„ ν¬κ² μ κ°</strong>ν•λ©΄μ„λ„ μ›λ λ¨λΈμ μ„±λ¥μ„ κ±°μ μ μ§€ν•  μ μμµλ‹λ‹¤. 2023λ…„ μ μ•λ <strong>QLoRA</strong>λ” μ΄λ¥Ό ν• λ‹¨κ³„ λ°μ „μ‹μΌ <strong>μ‚¬μ „ν•™μµ λ¨λΈμ„ 4-bitλ΅ κ³ μ •</strong>ν•κ³  LoRAλ΅λ§ μ—…λ°μ΄νΈλ¥Ό μν–‰ν•¨μΌλ΅μ¨, <strong>65μ–µ~130μ–µκΈ‰ λ¨λΈλ„ λ‹¨μΌ GPUλ΅ λ―Έμ„Έμ΅°μ • κ°€λ¥</strong>ν•κ² λ§λ“¤μ—μµλ‹ (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. μ‹¤μ λ΅ QLoRAλ¥Ό ν†µν•΄ <strong>65B νλΌλ―Έν„° λ¨λΈμ„ 48GB VRAMμ λ‹¨μΌ GPUμ—μ„ ν’€ 16λΉ„νΈ νμΈνλ‹κ³Ό λ™λ“±ν• μ„±λ¥μΌλ΅ ν•™μµ</strong>ν•λ” λ° μ„±κ³µν–μµλ‹ (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=enough%20to%20finetune%20a%2065B,we%20name%20Guanaco%2C%20outperforms%20all">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>) (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=QLoRA%20tuning%20is%20shown%20to,the%20power%20of%20QLoRA%20tuning">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)γ€‘. μ΄λ” <strong>GPU 1λ€μ—μ„ 780GB λ©”λ¨λ¦¬κ°€ ν•„μ”ν–λ μ‘μ—…μ„ 48GBλ΅ μ¤„μΈ μ„±κ³Ό</strong>λ΅, λ€κ·λ¨ λ¨λΈ νμΈνλ‹μ <strong>μ ‘κ·Όμ„±μ„ νμ‹ μ μΌλ΅ ν–¥μƒ</strong>μ‹μΌ°μµλ‹ (<a href="https://ar5iv.labs.arxiv.org/html/2305.14314#:~:text=ar5iv%20ar5iv,finetunable%20on%20a%20single%20GPU">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv</a>)γ€‘. QLoRAμ ν•µμ‹¬ μ•„μ΄λ””μ–΄λ” <strong>NF4 (4-bit NormalFloat) μ–‘μν™”</strong>μ™€ <strong>μ΄μ¤‘ μ–‘μν™”(Double Quantization)</strong>, κ·Έλ¦¬κ³  <strong>Paged Optimizer</strong> λ“±μ„ λ„μ…ν•μ—¬ μ„±λ¥ μ €ν• μ—†μ΄ λ©”λ¨λ¦¬λ¥Ό κ·Ήλ‹¨μ μΌλ΅ μ•„λ‚€ κ²ƒμ…λ‹ (<a href="https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,new%20data%20type%20that%20is">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>) (<a href="https://ar5iv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,of">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>)γ€‘.</p>
<p>Hugging Face λ°©λ²•μ μ¥μ μ€ <strong>κ°„νΈν•¨κ³Ό λ²”μ©μ„±</strong>μ…λ‹λ‹¤. λ°©λ€ν• μ‚¬μ „ν•™μµ <strong>μ²΄ν¬ν¬μΈνΈλ¥Ό Hugging Face Hubμ—μ„ μ¦‰μ‹ λ¶λ¬μ™€</strong> ν™μ©ν•  μ μκ³ , <strong>λ°μ΄ν„° μ „μ²λ¦¬λ¶€ν„° ν‰κ°€κΉμ§€ ν†µν•©λ μƒνƒκ³„</strong>(π¤— Datasets λ“±)λ¥Ό μ κ³µν•©λ‹λ‹¤. νΉν <strong>Transformer-Decorder λ¨λΈ</strong>(μ: GPT-2, GPT-3, LLaMA λ“±)μ <strong>ν…μ¤νΈ μƒμ„± νƒμ¤ν¬</strong>λ¥Ό μ„ν• νμΈνλ‹ μμ κ°€ ν’λ¶€ν•λ©°, ν•™μµ loop, ν† ν¬λ‚μ΄μ €, λ¨λΈ λ³‘λ ¬ν™” λ“±μ΄ μ μ¶”μƒν™”λμ–΄ μμµλ‹λ‹¤. κΈ°λ³Έ <code>Trainer</code>λ¥Ό μ‚¬μ©ν•λ” μμ‹ μ½”λ“ (μ: GPT-2λ¥Ό ν…μ¤νΈ μƒμ„± λ°μ΄ν„°λ΅ νμΈνλ‹) λ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>                      <span class="c1"># FP16 νΌν•© μ •λ°€λ„ μ‚¬μ©</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="o">=</span><span class="kc">None</span>                  <span class="c1"># (DeepSpeed μ‚¬μ©μ‹ μ„¤μ • νμΌ κ²½λ΅ μ§€μ •)</span>
<span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span> 
                  <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_ds</span><span class="p">,</span> 
                  <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<blockquote>
<p><strong>μ°Έκ³ :</strong> μƒκΈ° μ½”λ“μ—μ„ <code>deepspeed=None</code>λ΅ λ‘λ©΄ Hugging Faceμ κΈ°λ³Έ Trainerλ΅ ν•™μµν•©λ‹λ‹¤. DeepSpeedλ¥Ό μ‚¬μ©ν•λ ¤λ©΄ <code>deepspeed="ds_config.json"</code>μ²λΌ μ„¤μ • νμΌμ„ μ§€μ •ν•κ±°λ‚ π¤— Accelerateλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤ (μ•„λ DeepSpeed μ„Ήμ… μ°Έκ³ ).</p>
</blockquote>
<p>Hugging Face κΈ°λ° μ ‘κ·Όμ λ‹¨μ μ΄λΌλ©΄, <strong>μ•„μ£Ό ν° λ¨λΈμ„ λ‹¤λ£° λ•λ” κΈ°λ³Έ ν™κ²½μΌλ΅λ” ν•κ³„</strong>κ°€ μλ‹¤λ” κ²ƒμ…λ‹λ‹¤. μλ¥Ό λ“¤μ–΄ μμ‹­μ–µ~μλ°±μ–µ νλΌλ―Έν„° λ¨λΈμ„ μ „μ²΄ λ―Έμ„Έμ΅°μ •(full fine-tuning)ν•λ ¤λ©΄ λ©€ν‹° GPUκ°€ ν•„μμ΄λ©°, μ΄ κ²½μ° <strong>DeepSpeedλ‚ FSDP</strong> λ“±μ λ³΄μ΅°κ°€ ν•„μ”ν•  μ μμµλ‹λ‹¤. ν•μ§€λ§ μ¤‘μ†κ·λ¨ λ¨λΈμ΄λ‚ LoRAκ°™μ€ νλΌλ―Έν„° ν¨μ¨ κΈ°λ²•μ„ μ‚¬μ©ν•λ‹¤λ©΄ Hugging Faceλ§μΌλ΅λ„ μ¶©λ¶„ν μ‹¤ν—μ΄ κ°€λ¥ν•©λ‹λ‹¤. κ²°κ³Όμ μΌλ΅ Hugging Faceλ” <strong>μ—°κµ¬ κ°λ°μ μ¶λ°μ </strong>μΌλ΅μ„ μµμ‹  κΈ°λ²•λ“¤μ„ λΉ λ¥΄κ² λ°›μ•„λ“¤μ΄κ³  μμ–΄, <strong>μ‹¤λ¬΄μ—μ„λ„ κ°€μ¥ λ„λ¦¬ μ“°μ΄λ” LLM νμΈνλ‹ ν”λ«νΌ</strong>μ…λ‹λ‹¤.</p>
<h2 id="deepspeed">DeepSpeedλ¥Ό ν™μ©ν• λ€κ·λ¨ λ¨λΈ νμΈνλ‹<a class="headerlink" href="#deepspeed" title="Permanent link">&para;</a></h2>
<p><strong>DeepSpeed</strong>λ” λ§μ΄ν¬λ΅μ†ν”„νΈκ°€ κ°λ°ν• <strong>λ€κ·λ¨ λ¶„μ‚° ν•™μµ μµμ ν™” λΌμ΄λΈλ¬λ¦¬</strong>λ΅, νΉν κ±°λ€ μ–Έμ–΄λ¨λΈμ ν•™μµμ„ <strong>μ†λ„μ™€ μ¤μΌ€μΌ</strong> μΈ΅λ©΄μ—μ„ μ§€μ›ν•©λ‹λ‹¤. DeepSpeedμ ν•µμ‹¬μ—λ” <strong>ZeRO (Zero Redundancy Optimizer)</strong> μ•κ³ λ¦¬μ¦μ΄ μ (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)10γ€‘. ZeROλ” λ°μ΄ν„° λ³‘λ ¬ ν•™μµ μ‹ κ° GPUμ— λ™μΌν•κ² λ³µμ λλ <strong>μµν‹°λ§μ΄μ € μƒνƒ, κ·Έλλ””μ–ΈνΈ, λ¨λΈ νλΌλ―Έν„°</strong>λ¥Ό shard(λ¶„ν• )ν•μ—¬ <strong>GPUλ“¤ κ°„μ— λ¶„μ‚° μ €μ¥</strong> (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)10γ€‘. μ΄λ ‡κ² ν•λ©΄ μ¤‘λ³µμΌλ΅ λ©”λ¨λ¦¬λ¥Ό μ΅μ•„λ¨Ήλ” μ”μ†κ°€ μ‚¬λΌμ Έ, <strong>λ¨λΈ ν¬κΈ°κ°€ μ»¤μ Έλ„ λ©”λ¨λ¦¬ ν¨μ¨μ μΌλ΅ λ¶„μ‚°</strong>μ‹ν‚¬ μ μμµλ‹λ‹¤. ZeROλ” λ‹¨κ³„λ³„λ΅ λ°μ „λμ–΄ <strong>Stage 1</strong>(μµν‹°λ§μ΄μ € μƒνƒ λ¶„μ‚°), <strong>Stage 2</strong>(+ gradient λ¶„μ‚°), <strong>Stage 3</strong>(+ νλΌλ―Έν„° μμ²΄ λ¶„μ‚°)μΌλ΅ κµ¬λ¶„λλ©°, Stage μ«μκ°€ λ†’μ„μλ΅ GPU λ©”λ¨λ¦¬ λ¶€λ‹΄μ΄ κ°μ† (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)10γ€‘. νΉν <strong>ZeRO-3</strong>λ” λ¨λ“  λ¨λΈ νλΌλ―Έν„°λ¥Ό λ¨λ“  GPUμ— λ‚λ„μ–΄ μ¬λ ¤λ†“κ³  ν•„μ” μ‹ λ™μ μΌλ΅ λ¶λ¬μ“°λ” λ°©μ‹μΌλ΅, <strong>κ°λ³„ GPUμ—λ” μ „μ²΄ λ¨λΈμ μΌλ¶€λ§ μƒμ£Ό</strong>ν•κ² λ©λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μλ¥Ό λ“¤μ–΄ <strong>70μ–µ~130μ–µ κ° νλΌλ―Έν„° λ¨λΈμ„ λ‹¨μΌ λλ” μ†μ GPUμ—μ„ ν•™μµ</strong>μ‹ν‚¤λ” κ²ƒμ΄ κ°€λ¥ν•΄μ΅ (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)09γ€‘. DeepSpeed ν€μ νν† λ¦¬μ–Όμ— λ”°λ¥΄λ©΄, ZeRO-Offload κΈ°λ¥κΉμ§€ ν™μ©ν•λ©΄ <strong>10μ–µ~13μ–µ νλΌλ―Έν„° GPT-2 λ¨λΈλ„ λ‹¨μΌ 32GB GPUμ—μ„ ν•™μµ</strong>ν•  μ μ (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)09γ€‘. μ•„λλ” DeepSpeedμ ZeRO-Offloadλ¥Ό ν†µν• λ‹¨μΌ GPU λ€μ©λ‰ λ¨λΈ ν•™μµ μ‚¬λ΅€μ…λ‹λ‹¤:</p>
<ul>
<li><em>β€ZeRO-Offloadλ” μµν‹°λ§μ΄μ € λ©”λ¨λ¦¬μ™€ μ—°μ‚°μ„ CPUλ΅ μ¤ν”„λ΅λ“ν•μ—¬, μµλ€ 130μ–µ νλΌλ―Έν„°μ— λ‹¬ν•λ” ν° λ¨λΈλ„ λ‹¨μΌ GPUμ—μ„ ν¨μ¨μ μΌλ΅ ν•™μµν•  μ μκ² ν•΄ (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)09γ€‘.β€</em></li>
</ul>
<p>DeepSpeedμ λ λ‹¤λ¥Έ κ°•μ μ€ <strong>λ³‘λ ¬ν™”μ™€ μµμ ν™” μ „λµμ λ‹¤μ–‘μ„±</strong>μ…λ‹λ‹¤. λ¨λΈ λ³‘λ ¬ν™”, νμ΄ν”„λΌμΈ λ³‘λ ¬ν™”, mixed precision μ—°μ‚°, Gradient Accumulation λ“±μ κΈ°λ²•μ„ ν†µν•©μ μΌλ΅ μ§€μ›ν•μ—¬ <strong>GPU μ—¬λ¬ λ€λ¥Ό μµλ€ν• ν™μ©</strong>ν•  μ μμµλ‹λ‹¤. λν• <strong>CPU Offloading</strong>(ZeRO-Offload)κ³Ό <strong>NVMe Offloading</strong>(ZeRO-Infinity)μ„ ν†µν•΄, GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•  κ²½μ° <strong>μΌλ¶€ λ°μ΄ν„°(μ: λ¨λΈ κ°€μ¤‘μΉλ‚ μµν‹°λ§μ΄μ € μƒνƒ)λ¥Ό CPU RAMμ΄λ‚ SSDλ΅ λ¶„μ‚°</strong>μ‹ν‚΄μΌλ΅μ¨ <strong>μ‚¬μ‹¤μƒ λ¬΄μ ν•μ— κ°€κΉμ΄ λ¨λΈ μ‚¬μ΄μ¦</strong>κΉμ§€λ„ ν•™μµμ„ μ‹λ„ν•  μ μ (<a href="https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/#:~:text=ZeRO,Infinity%20include">ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research</a>) (<a href="https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/#:~:text=,efficiency%20and%20superlinear%20throughput%20scaling">ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model scale for deep learning training - Microsoft Research</a>)94γ€‘. μ΄λ¬ν• κ·Ήλ‹¨μ  ν™•μ¥μ„± λ•λ¶„μ— DeepSpeedλ” GPT-3(175B) κΈ‰ λ¨λΈ ν•™μµμ΄λ‚ μμ΅° κ° νλΌλ―Έν„° μ‹¤ν—μ²λΌ <strong>μµμ²¨λ‹¨ μ¤μΌ€μΌμ μ—°κµ¬μ— ν•„μμ μΈ λ„κµ¬</strong>λ΅ μλ¦¬μ΅μ•μµλ‹λ‹¤.</p>
<p>DeepSpeedλ¥Ό μ‹¤λ¬΄μ— ν™μ©ν•λ ¤λ©΄ <strong>μ„¤μ • νμΌκ³Ό λ°μ²</strong>λ¥Ό μ‚¬μ©ν•λ” λ°©μ‹μ΄ μΌλ°μ μ…λ‹λ‹¤. Hugging Face Trainerμ—λ„ <code>deepspeed</code> μΈμλ¥Ό ν†µν•΄ DeepSpeedλ¥Ό ν†µν•©ν•  μ μμΌλ©°, <strong>π¤— Accelerate ν΄μ„ μ“°λ©΄ λ€ν™”ν•μΌλ΅ μ„¤μ • νμΌμ„ μƒμ„±</strong>ν•  μ  (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=Configuration">DeepSpeed</a>) (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=accelerate%20config%20">DeepSpeed</a>)164γ€‘. μλ¥Ό λ“¤μ–΄, μ•„λμ™€ κ°™μ€ DeepSpeed μ„¤μ •(<code>ds_config.json</code>)μ„ μ¤€λΉ„ν•μ—¬ Trainerμ— μ „λ‹¬ν•λ©΄ ZeRO κΈ°λ° ν›λ ¨μ΄ μ΄λ£¨μ–΄μ§‘λ‹λ‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;offload_param&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;fp16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>μ„ μ„¤μ •μ€ ZeRO-3 λ‹¨κ³„μ—μ„ <strong>λ¨λΈ νλΌλ―Έν„°λ¥Ό CPUλ΅ μ¤ν”„λ΅λ“</strong>ν•λ„λ΅ μ§€μ •ν• μμ‹μ…λ‹λ‹¤. <code>TrainingArguments(..., deepspeed="ds_config.json")</code> μ²λΌ μ„¤μ •ν•λ©΄ Hugging Face Trainerκ°€ λ‚΄λ¶€μ μΌλ΅ DeepSpeed μ—”μ§„μ„ μ΄κΈ°ν™”ν•μ—¬ ν•™μµμ„ μ§„ν–‰ν•©λ‹λ‹¤. λλ” <strong><code>deepspeed.init</code> APIλ¥Ό μ§μ ‘ μ‚¬μ©</strong>ν•΄ λ¨λΈ, μµν‹°λ§μ΄μ €λ¥Ό κ°μ‹Ό λ’¤ <code>deepspeed.run</code>μΌλ΅ ν›λ ¨ loopμ„ κµ¬ν„ν•  μλ„ μμµλ‹λ‹¤. μ–΄λ–¤ λ°©λ²•μ΄λ“ , <strong>κΈ°μ΅΄ PyTorch μ½”λ“λ¥Ό ν¬κ² λ³€κ²½ν•μ§€ μ•μΌλ©΄μ„</strong> DeepSpeedμ μ΄μ μ„ μ–»μ„ μ μλ‹¤λ” κ²ƒμ΄ μ¥μ μ…λ‹λ‹¤.</p>
<p>DeepSpeedμ™€ Hugging Face PEFTλ¥Ό <strong>μ΅°ν•©ν•μ—¬ μ‚¬μ©</strong>ν•λ” κ²ƒλ„ κ°€λ¥ν•©λ‹λ‹¤. μλ¥Ό λ“¤μ–΄ <strong>LoRA μ μ© λ¨λΈμ„ DeepSpeed ZeRO-3λ΅ λ¶„μ‚° ν•™μµ</strong>ν•κ±°λ‚, <strong>QLoRA(4λΉ„νΈ + LoRA)</strong>λ¥Ό DeepSpeedμ™€ ν•¨κ» ν™μ©ν•μ—¬ λ‹¤μ¤‘ GPUμ—μ„ μ΄κ±°λ€ λ¨λΈμ„ ν•™μµν•λ” μ‹¤ν—λ“¤μ΄ λ³΄κ³ λ (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=For%20DeepSpeed%20Stage%203%20%2B,models%20on%20multiple%20GPUs%20below">DeepSpeed</a>) (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=This%20section%20of%20guide%20will,by%20changing%20the%20accelerate%20config">DeepSpeed</a>)142γ€‘. Hugging Face κ°€μ΄λ“μ—μ„λ” <strong>8x H100 (80GB) GPUλ΅ LLaMA-70B λ¨λΈμ„ LoRA+ZeRO-3 μ„¤μ •μΌλ΅ SFT(μ§€λ„νμΈνλ‹)ν•λ” μμ‹</strong>λ¥Ό μ κ³µν•κ³   (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=This%20section%20of%20guide%20will,by%20changing%20the%20accelerate%20config">DeepSpeed</a>) (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=accelerate%20config%20">DeepSpeed</a>)164γ€‘. μ΄μ²λΌ DeepSpeedλ” Hugging Face μƒνƒκ³„μ™€λ„ μ λ§λ¬Όλ ¤ λ™μ‘ν•λ©°, νμΈνλ‹ <strong>μ†λ„ λ° ν™•μ¥μ„±</strong>μ„ λ†’μ΄λ” μ—­ν• μ„ ν•©λ‹λ‹¤.</p>
<p>μ •λ¦¬ν•λ©΄, DeepSpeedμ νΉμ§•κ³Ό μ¥μ μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:</p>
<ul>
<li><strong>ZeRO μ•κ³ λ¦¬μ¦</strong>μ„ ν†µν• <strong>λ©”λ¨λ¦¬ μµμ ν™” λ° λ¨λΈ λ¶„μ‚°</strong>: λ™μΌ μμ›μΌλ΅ λ” ν° λ¨λΈ ν•™ (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)110γ€‘ </li>
<li><strong>CPU/NVMe μ¤ν”„λ΅λ“</strong>λ΅ λ‹¨μΌ GPU λ©”λ¨λ¦¬ ν•κ³„ (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)109γ€‘ </li>
<li><strong>κ³ λ„ν™”λ λ¶„μ‚° λ³‘λ ¬ ν•™μµ</strong> μ§€μ›: μμ‹­~μλ°± GPUκΉμ§€ ν¨μ¨μ  μ¤μΌ€μΌ μ•„μ›ƒ</li>
<li><strong>μ„±λ¥ μµμ ν™” μ»¤λ„</strong> μ κ³µ: DeepSpeedμ CPU Adam μµν‹°λ§μ΄μ €λ” κΈ°λ³Έ PyTorch λ€λΉ„ 5~7 (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=For%20large%20model%20training%2C%20optimizers,please%20see%20our%20blog%20post">ZeRO-Offload - DeepSpeed</a>)119γ€‘ λ“±</li>
</ul>
<p>λ‹¨μ μΌλ΅λ” <strong>ν™κ²½ μ„¤μ •μ λ³µμ΅μ„±</strong>μ΄ μμµλ‹λ‹¤. μ„¤μ • νμΌ μ‘μ„±, λ°μ² λ…λ Ή λ“± μ²μ μ‚¬μ©μ‹ μ§„μ…μ¥λ²½μ΄ μμΌλ©°, μ‘μ€ κ·λ¨ μ‹¤ν—μ—λ” κ³Όν• μΈ΅λ©΄μ΄ μμ„ μ μμµλ‹λ‹¤. λν• <strong>λ™μΌν• μ—°μ‚°μ΄λΌλ„ μ•½κ°„μ μ¤λ²„ν—¤λ“</strong>(ν†µμ‹  λ€κΈ° λ“±)κ°€ μ΅΄μ¬ν•λ―€λ΅, λ¨λΈμ΄ μ¶©λ¶„ν ν¬κ±°λ‚ λ¶„μ‚°μ΄ ν•„μ”ν• κ²½μ°μ— κ°€μ¥ ν° ν¨κ³Όλ¥Ό λ΄…λ‹λ‹¤. κ·ΈλΌμ—λ„, <strong>μ‹¤λ¬΄μ—μ„ μμ‹­μ–µ~μμ²μ–µ νλΌλ―Έν„°</strong> λ¨λΈμ„ λ‹¤λ¤„μ•Ό ν•λ‹¤λ©΄ DeepSpeedλ” μ‚¬μ‹¤μƒ <strong>ν‘μ¤€ λ„κµ¬</strong>λ΅ μλ¦¬μ΅μ•μµλ‹λ‹¤.</p>
<h2 id="unsloth-llm">Unslothλ¥Ό ν™μ©ν• κ³ μ† LLM νμΈνλ‹<a class="headerlink" href="#unsloth-llm" title="Permanent link">&para;</a></h2>
<p><strong>Unsloth</strong>λ” 2023λ…„ μ»¤λ®¤λ‹ν‹°μ—μ„ λ“±μ¥ν• <strong>κ²½λ‰ν™” LLM νμΈνλ‹ λΌμ΄λΈλ¬λ¦¬</strong>λ΅, <strong>β€Hugging Face νΈν™β€</strong>μ„ ν‘λ°©ν•λ©΄μ„λ„ <strong>ν•™μµ μ†λ„λ¥Ό 2λ°° μ΄μƒ λ†’μ΄κ³  λ©”λ¨λ¦¬ μ‚¬μ©μ„ 40~70% μ¤„μ΄λ”</strong> νμ‹ μ„ λ³΄μ—¬μ£Όκ³   (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L92γ€‘. Unslothμ μ ‘κ·Όλ²•μ€ κΈ°μ΅΄ Hugging Face <code>Transformers</code> λ¨λΈμ μΌλ¶€ μ—°μ‚°μ„ <strong>Triton</strong> κΈ°λ°μ λ§μ¶¤ μ»¤λ„λ΅ λ€μ²΄ν•μ—¬ <strong>PyTorch μμ¤€μ—μ„μ λΉ„ν¨μ¨μ„ μ κ±°</strong>ν•λ”  (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L72γ€‘. κµ¬μ²΄μ μΌλ΅, <strong>Self-Attention, FFN λ“± Transformer ν•µμ‹¬ λ¨λ“μ backward μ—°μ‚°μ„ μμ‹μΌλ΅ μ§μ ‘ μ λ„</strong>ν•μ—¬ TritonμΌλ΅ κµ¬ν„ν•¨μΌλ΅μ¨, κ°™μ€ μ‘μ—…μ„ ν•λ©΄μ„λ„ <strong>λ©”λ¨λ¦¬ λ³µμ‚¬λ‚ μ¤‘κ°„ μ—°μ‚° overheadλ¥Ό μµμ†ν™”</strong> (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L72γ€‘. μ΄λ° <strong>μλ™ μµμ ν™”(manual backprop)</strong> κΈ°λ²• λ•λ¶„μ—, <strong>λ™μΌν• QLoRA νμΈνλ‹μ΄λΌλ„ Unsloth μ‚¬μ© μ‹ ν•™μµ μ†λ„κ°€ μ•½ 2λ°°λ΅ ν–¥μƒλκ³  GPU VRAM μ‚¬μ©μ€ μ λ° μ΄ν•λ΅ κ°μ†</strong>ν•λ” κ²°κ³Όλ¥Ό μ–» (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Llama,18.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L88γ€‘. λ†€λκ²λ„ <strong>λ¨λΈμ μµμΆ… μ„±λ¥ μ €ν•κ°€ 0%</strong>μ„μ΄ κ²€μ¦λμ—λ”λ°, μ΄λ” Unslothμ μ»¤λ„ μµμ ν™”κ°€ κ·Όλ³Έμ μΌλ΅ <strong>λ™μΌν• κ³„μ‚°μ„ λ” ν¨μ¨μ μΌλ΅ κµ¬ν„</strong>ν• κ²ƒμ΄λ―€λ΅ μ •ν™•λ„κ°€ λ³΄μ΅΄λκΈ° λ• (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L68γ€‘.</p>
<p>Unslothλ” <strong>Hugging Faceμ™€μ νΈν™μ„±</strong>μ„ κ°•μ΅°ν•©λ‹λ‹¤. μ‚¬μ©λ²•λ„ λ§¤μ° λΉ„μ·ν•μ—¬, <code>FastLanguageModel.from_pretrained()</code> ν•¨μλ΅ λ¨λΈμ„ λ¶λ¬μ¤λ©΄ λ‚΄λ¶€μ μΌλ΅ <code>transformers</code> λ¨λΈμ„ λν•‘ν• Unsloth λ¨λΈ κ°μ²΄μ™€ ν† ν¬λ‚μ΄μ €λ¥Ό λ° (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=,returns%20the%20model%20tokenizer%20for">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=model%2C%20tokenizer%20%3D%20FastLanguageModel,RoPE%20Scaling%20internally%2C%20so%20choose">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)L22γ€‘. μλ¥Ό λ“¤μ–΄ λ‹¤μκ³Ό κ°™μ΄ LLaMA κ³„μ—΄ λ¨λΈμ„ Unslothλ΅ λ΅λ“ν•  μ μμµλ‹λ‹¤:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">unsloth</span> <span class="kn">import</span> <span class="n">FastLanguageModel</span>

<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;unsloth/mistral-7b-bnb-4bit&quot;</span><span class="p">,</span>  <span class="c1"># HF ν—λΈ λ¨λΈλ… (4-bit μ–‘μν™”λ Mistral 7B)</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">2048</span>                       <span class="c1"># μµλ€ μ‹ν€€μ¤ κΈΈμ΄ (RoPE μ¤μΌ€μΌλ§ μλ™μ μ©)</span>
<span class="p">)</span>
</code></pre></div>
<p>λ¶λ¬μ¨ λ¨λΈμ€ Hugging Face <code>transformers</code>μ™€ κ±°μ λ™μΌν• μΈν„°νμ΄μ¤λ¥Ό μ κ³µν•λ―€λ΅, <code>transformers.Trainer</code>λ‚ π¤— TRLμ <code>SFTTrainer</code> λ“±μ— κ·Έλ€λ΅ λ„£μ–΄μ„ μ‚¬μ©ν•  μ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=most%20NVIDIA%20GPUs%20%E2%80%93from%20GTX,and%20Mistral%20architectures">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L66γ€‘. Unslothλ” ν„μ¬ <strong>LLaMA κ³„μ—΄(Llama-2, CodeLlama λ“±)κ³Ό Mistral, Qwen λ“± GPT μ μ‚¬ μ•„ν‚¤ν…μ²</strong>λ¥Ό μ§€μ›ν•λ©°, λ‹¤μ–‘ν• NVIDIA GPU(μ: GTX 16GBκΈ‰λ¶€ν„° A100/H100κΉμ§€)μ—μ„  (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L63γ€‘. νΉν <strong>FP16, BF16 νΌν•©μ •λ°€λ„</strong>λ„ μµμ…μΌλ΅ μΌ¤ μ μκ³ , <strong>μ–‘μν™”λ λ¨λΈ(<code>bnb-4bit</code>)λ„ μ§μ ‘ λ΅λ“</strong>ν•  μ μμ–΄ (bitsandbytes λΌμ΄λΈλ¬λ¦¬ ν•„μ”), Hugging Faceμ—μ„ ν•λ 4-bit QLoRA νμΈνλ‹μ„ κ±°μ κ·Έλ€λ΅ μ§„ν–‰ν•λ©΄μ„ μ„±λ¥ ν–¥μƒμ„ λ„λ¦΄ μ μμµλ‹λ‹¤.</p>
<p>Unslothμ νΉκΈ°ν•  λ§ν• κΈ°λ¥ μ¤‘ ν•λ‚λ” <strong>RoPE Scaling</strong>μ„ μλ™ μ²λ¦¬ν•λ” (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=,returns%20the%20model%20tokenizer%20for">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L15γ€‘. RoPE(Rotary Positional Embedding)λ” GPT κ³„μ—΄μ—μ„ μ“°μ΄λ” μ„μΉμΈμ½”λ”© κΈ°λ²•μΈλ°, Unsloth λ¨λΈ λ΅λ“μ‹ <code>max_seq_length</code>λ¥Ό ν¬κ² μ§€μ •ν•λ©΄ <strong>ν•™μµ μ‹ λ” κΈ΄ λ¬Έλ§¥κΈΈμ΄</strong>λ¥Ό μ‚¬μ©ν•  μ μλ„λ΅ λ‚΄λ¶€μ μΌλ΅ μ£Όνμλ¥Ό μ¤μΌ€μΌλ§ν•΄ μ¤λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ³Έλ 2048 ν† ν°κΉμ§€μ€λ LLaMA-2 λ¨λΈλ„ <strong>μµλ€ 4λ°° μ΄μƒ κΈ΄ μ»¨ν…μ¤νΈκΉμ§€</strong> νμΈνλ‹ν•  (<a href="https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=,tuning">Fine-tuning Guide | Unsloth Documentation</a>)L163γ€‘, μΌλ¶€ μµμ‹  λ¨λΈ(Llama-3.3 70B λ“±)μ€ Unslothλ΅ <strong>8λ§~3μ‹­λ§ ν† ν° μ΄μƒμ λ¬Έλ§¥ ν•™μµ</strong>λ„ μ‹λ„λκ³  (<a href="https://github.com/unslothai/unsloth#:~:text=with%20Llama%20%26%20Qwen%20distillations,13x%20longer">GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1 &amp; Reasoning LLMs 2x faster with 70% less memory! </a>)L308γ€‘. κΈ΄ λ¬Έλ§¥ λ€μ‘μ€ <strong>Decoder-Only λ¨λΈμ μ‹¤μ  ν™μ©λ„</strong>λ¥Ό λ†’μ΄λ” μ¤‘μ”ν• μµμ ν™”μΈλ°, Unslothκ°€ μ΄λ¥Ό νΈλ¦¬ν•κ² μ§€μ›ν•λ” μ μ€ μ‹¤μ©μ  μ¥μ μ΄λΌ ν•  μ μμµλ‹λ‹¤.</p>
<p>μ”μ•½ν•λ©΄ Unslothμ νΉμ§•κ³Ό μ¥μ :</p>
<ul>
<li><strong>Triton μ»¤λ„ κΈ°λ° μµμ ν™”</strong>λ΅ <strong>ν•™μµμ†λ„ ~2λ°° ν–¥μƒ</strong>, **λ©”λ¨λ¦¬ ~50% (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Llama,18.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L88γ€‘ (λ™μΌ ν•λ“μ›¨μ–΄/λ¨λΈ λ€λΉ„)</li>
<li>Hugging Face **Transformers/PEFTμ™€ μ™„μ „ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20is%20a%20lightweight%20library,and%20Mistral%20architectures">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L63γ€‘ β€“ μΉμ™ν• APIλ΅ μ‚¬μ© κ°€λ¥</li>
<li><strong>QLoRA(4-bit + LoRA) μ§€μ›</strong> β€“ μ €μλ“¤μ΄ μ κ³µν• λ‹¤μ΄λ‚λ―Ή 4λΉ„νΈ μ–‘μν™”λ΅ QLoRAμ λ―Έμ„Έ μ„±λ¥ μ €ν• (<a href="https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=improves%20accuracy%20%281%E2%80%932">Fine-tuning Guide | Unsloth Documentation</a>) (<a href="https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=We%20recommend%20starting%20with%20QLoRA%2C,LoRA%20is%20now%20largely%20recovered">Fine-tuning Guide | Unsloth Documentation</a>)L174γ€‘</li>
<li><strong>RoPE λ“± Decoderμ© μ¶”κ°€ κΈ°λ¥</strong> β€“ λ¬Έλ§¥κΈΈμ΄ ν™•μ¥ λ“± λ””μ½”λ” Transformerμ— μ μ©ν• μµμ ν™” μ κ³µ</li>
<li><strong>μ¤ν”μ†μ¤ κ°λ° ν™μ„±ν™”</strong> β€“ μ½λ© λ…ΈνΈλ¶, λ²¤μΉλ§ν¬ μ¤ν¬λ¦½νΈ κ³µκ° λ“±μΌλ΅ μ¬ν„μ„±κ³Ό μ ‘ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L92γ€‘</li>
</ul>
<p>Unslothμ ν„μ¬ ν•κ³„λ΅λ” <strong>μ§€μ› μ•„ν‚¤ν…μ²κ°€ μ ν•μ </strong>μ΄λΌλ” μ μ΄ μμµλ‹λ‹¤. μ£Όλ΅ Metaμ Llama κ³„μ—΄κ³Ό κ·Έ νμƒλ¨λΈμ— μ§‘μ¤‘λμ–΄ μκ³ , Transformer κµ¬μ΅°κ°€ λ‹¤λ¥Έ T5(Encoder-Decoder)λ‚ GLM μ–‘λ°©ν–¥ λ¨λΈ λ“±μ€ μ§€μ›ν•μ§€ μ•μµλ‹λ‹¤. λν• λ¶„μ‚° ν•™μµ(λ©€ν‹° GPU)μ— λ€ν• μ–ΈκΈ‰μ΄ μ μ€λ°, μ£Όλ΅ λ‹¨μΌ GPUμ—μ„μ κ·Ήν• μµμ ν™”μ— μ΄μ μ΄ λ§μ¶°μ Έ μμµλ‹λ‹¤. λ”°λΌμ„ μ•„μ£Ό ν° λ¨λΈμ„ μ—¬λ¬ GPUμ— λ‚λ„μ–΄ ν•™μµν•λ” μ©λ„λ” DeepSpeedλ§νΌ μ£Όμ•μ μ€ μ•„λ‹ μ μμµλ‹λ‹¤. κ·ΈλΌμ—λ„ <strong>λ‹¨μΌ/μ†μ GPUλ΅ LLMμ„ μµλ€ν• λΉ λ¥΄κ² νλ‹</strong>ν•΄μ•Ό ν•λ” μ‹¤λ¬΄ μƒν™©μ—μ„ Unslothλ” λ€λ‹¨ν λ§¤λ ¥μ μΈ μ„ νƒμ§€μ…λ‹λ‹¤. μμ»¨λ€, 1μ¥μ A100μΌλ΅ ν•λ£¨ κ±Έλ¦¬λ νμΈνλ‹ μ‘μ—…μ„ Unslothλ΅ λ°λ‚μ μ— (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L77γ€‘, κ°™μ€ GPUμ—μ„ λ” ν° λ°°μΉ μ‚¬μ΄μ¦λ‚ λ” κΈ΄ λ¬Έλ§¥μ„ μ‹¤ν—ν•  μ—¬μ λ¥Ό μ–»μ„ μ μμµλ‹λ‹¤. μ΄κ²ƒμ€ κ³§ <strong>κ°λ° μƒμ‚°μ„±κ³Ό μ‹¤ν— λ²”μ„μ ν™•λ€</strong>λ΅ μ΄μ–΄μ§€λ―€λ΅, μ•μΌλ΅ Unslothμ™€ κ°™μ€ μµμ ν™” ν΄μ ν™μ©λ„λ” λ”μ± λ†’μ•„μ§ μ „λ§μ…λ‹λ‹¤.</p>
<h2 id="_1">μ„±λ¥ λΉ„κµ λ° ν‰κ°€ λ°©λ²•<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>LLM νμΈνλ‹ κΈ°λ²•λ“¤μ„ ν‰κ°€ν•  λ•μ—λ” <strong>λ¨λΈμ μµμΆ… μ„±λ¥</strong> λΏ μ•„λ‹λΌ <strong>ν•™μµ ν¨μ¨ μ§€ν‘</strong>λ“¤λ„ μ¤‘μ”ν•©λ‹λ‹¤. μ£Όμ” λΉ„κµ κΈ°μ¤€μ€ <strong>λ©”λ¨λ¦¬ μ‚¬μ©λ‰(VRAM)</strong>, <strong>ν•™μµ μ†λ„(throughput)</strong>, <strong>ν•™μµ μ•μ •μ„± λ° ν¨μ¨μ„±</strong> λ“±μ΄ μμµλ‹λ‹¤. μ•„λ ν‘λ” Hugging Face κΈ°λ³Έ λ°©λ²•, DeepSpeed, Unslothμ μ£Όμ” νΉμ§•κ³Ό μ„±λ¥ μƒμ μ¥λ‹¨μ μ„ μ •λ¦¬ν• κ²ƒμ…λ‹λ‹¤:</p>
<table>
<thead>
<tr>
<th>μ ‘κ·Όλ²•</th>
<th>μ£Όμ” νΉμ§• λ° μµμ ν™”</th>
<th>λ©”λ¨λ¦¬ μ‚¬μ©λ‰</th>
<th>ν•™μµ μ†λ„</th>
<th>λΉ„κ³  (μ¥λ‹¨μ  μ”μ•½)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hugging Face κΈ°λ³Έ</strong></td>
<td>- Pretrained λ¨λΈ/λ°μ΄ν„° μ—μ½”μ‹μ¤ν…<br>- Trainer/Accelerate ν†µν• μ†μ‰¬μ΄ κµ¬ν„<br>- PEFT: LoRA, P-Tuning λ“± μ§€μ›<br>- 8/4-bit μ–‘μν™” λ΅λ“ μ§€μ›</td>
<td>κΈ°μ¤€ (100%)</td>
<td>κΈ°μ¤€ (1Γ—)</td>
<td>μ‰¬μ΄ κµ¬ν„κ³Ό μ»¤λ®¤λ‹ν‹° μ§€μ›μ΄ κ°•μ . λ€ν• λ¨λΈμ€ μ¶”κ°€ μµμ ν™” ν•„μ” (μ: DeepSpeed ν†µν•© κ°€λ¥).</td>
</tr>
<tr>
<td><strong>DeepSpeed (ZeRO)</strong></td>
<td>- ZeRO-1/2/3 μµν‹°λ§ (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)L110γ€‘<br>- CPU/NVMe Offlo (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)L109γ€‘<br>- λ³‘λ ¬ν™” μµμ  νλ‹ (μΌκ΄„ ν†µμ‹ , One-bit Adam λ“±)<br>- λ¶„μ‚° ν›λ ¨μ— νΉν™”</td>
<td><strong>λ§¤μ° μ μ</strong> (νλΌλ―Έν„°/κ·Έλλ””μ–ΈνΈ λ¶„μ‚°μΌλ΅ GPUλ‹Ή λ¶€ (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>)L110γ€‘</td>
<td><strong>λ†’μ</strong> (λ©€ν‹° GPUλ΅ μ„ ν• μ¤μΌ€μΌλ§, λ‹¨μΌ GPUμ—μ„  λ‹¤μ† μ¤λ²„ν—¤λ“)</td>
<td>μ΄λ€ν• λ¨λΈ ν•™μµ κ°€λ¥ (μμ‹­μ–µ~μμ²μ–µβ†‘  (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)L109γ€‘. μ΄κΈ° μ„¤μ • λ³µμ΅ν•μ§€λ§, λ€κ·λ¨ μ‹¤ν—μ—” ν•„μ λ„κµ¬.</td>
</tr>
<tr>
<td><strong>Unsloth (QLoRA κΈ°λ°)</strong></td>
<td>- Triton μ»¤λ„λ΅ λ¨λΈ μ—° (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L72γ€‘<br>- μλ™ backpropμΌλ΅ λ©”λ¨λ¦¬ μ μ•½<br>- RoPE μ¤μΌ€μΌλ§μΌλ΅ λ¬Έλ§¥ ν™•μ¥<br>- HF Transformersμ™€ νΈν™ API</td>
<td><strong>μ μ</strong> (λ™μΌ QLoRA λ€λΉ„ VRAM μµλ€ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L92γ€‘)</td>
<td><strong>λ§¤μ° λ†’μ</strong> (λ™μΌ QLoRA λ€λΉ„ ~ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L77γ€‘)</td>
<td>λ‹¨μΌ/μ†μ GPU ν™κ²½μ— μµμ ν™”. μ •ν™•λ„ μ†μ‹¤ μ—†μ΄  (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L68γ€‘. μ§€μ› λ¨λΈ ν•μ •μ μ΄λ‚ λΉ λ¥΄κ² ν™•λ€ μ¤‘.</td>
</tr>
</tbody>
</table>
<p><em>ν‘: Hugging Face vs. DeepSpeed vs. Unslothμ νΉμ§• λ° ν¨μ¨ λΉ„κµ</em></p>
<p>μ„ λΉ„κµμ—μ„ λ³΄λ“―μ΄, <strong>Hugging Face + κΈ°λ³Έ PyTorch</strong>λ” κµ¬ν„ νΈμμ„± μΈ΅λ©΄μ—μ„ λ›°μ–΄λ‚λ‚ <strong>λ€ν• λ¨λΈ ν•™μµ μ‹ λ©”λ¨λ¦¬ λ³‘λ©</strong>μ΄ μμ„ μ μμµλ‹λ‹¤. DeepSpeedλ” μ΄λ¥Ό ν•΄μ†ν•μ—¬ <strong>λ¨λΈ μ‚¬μ΄μ¦ ν•κ³„λ¥Ό ν¬κ² λ†’μ—¬μ£Όμ§€λ§</strong>, κµ¬μ„± λ³µμ΅μ„±κ³Ό <strong>ν†µμ‹  μ¤λ²„ν—¤λ“</strong>κ°€ μ•½κ°„ μ΅΄μ¬ν•©λ‹λ‹¤. Unslothλ” <strong>λ‚®μ€ μμ¤€μ μ»¤μ¤ν„°λ§μ΄μ§•μ„ ν†µν•΄</strong> κ°€μ¥ λ§μ΄ μ“°μ΄λ” μ‹λ‚λ¦¬μ¤(μ: LLaMA κ³„μ—΄μ SFT)μ—μ„ <strong>μµλ€μ μ†λ„/λ©”λ¨λ¦¬ ν¨μ¨</strong>μ„ λμ–΄μ¬λ¦° μ‚¬λ΅€μ…λ‹λ‹¤. νΉν QLoRAμ²λΌ <strong>4-bit μ–‘μν™”λ΅ μΈν• 16-bit λ€λΉ„ μ•½κ°„μ μ†λ„ μ €ν•</strong>κ°€ μ›λ (<a href="https://lightning.ai/pages/community/lora-insights/#:~:text=Code%20Framework,which%20is%20to%20be">Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of ...</a>) (<a href="https://ar5iv.labs.arxiv.org/html/2305.14314#:~:text=a%20significant%20shift%20in%20accessibility,finetunable%20on%20a%20single%20GPU">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv</a>)-L57γ€‘, Unsloth μµμ ν™”λ΅ μ΄λ¬ν• <strong>μ–‘μν™” μ¤λ²„ν—¤λ“κΉμ§€ μƒμ‡„</strong>ν• κ²ƒμ΄ ν° μ¥μ μ…λ‹λ‹¤.</p>
<p><strong>ν‰κ°€ λ°©λ²•</strong>μΌλ΅, <strong>λ©”λ¨λ¦¬ μ‚¬μ©λ‰</strong>μ€ μΌλ°μ μΌλ΅ <strong>ν›λ ¨ μ¤‘ μµλ€ GPU VRAM μ μ </strong>λ¥Ό μΈ΅μ •ν•©λ‹λ‹¤ (μ: <code>nvidia-smi</code> λ¨λ‹ν„°λ§). DeepSpeedμ κ²½μ° ZeRO-3λ¥Ό μ“°λ©΄ κ° GPUκ°€ λ¨λΈ μΌλ¶€λ§ κ°–κ³  μμΌλ―€λ΅ κ°λ³„ GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ ν¬κ² μ¤„κ³ , λ‚λ¨Έμ§€λ” CPU/NVMe μ‚¬μ©λ‰μΌλ΅  (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=GPU%20Peak%20Memory%20consumed%20during,begin%29%3A%200">DeepSpeed</a>) (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=GPU%20Peak%20Memory%20consumed%20during,begin%29%3A%200">DeepSpeed</a>)-L20γ€‘. <strong>μ²λ¦¬ μ†λ„</strong>λ” λ³΄ν†µ <strong>μ΄λ‹Ή μ²λ¦¬ ν† ν° μ (tokens per second)</strong> λλ” <strong>μ¤ν…λ‹Ή μ‹κ°„</strong>μΌλ΅ μ‚°μ¶ν•©λ‹λ‹¤. κ°™μ€ ν•λ“μ›¨μ–΄μ—μ„ λ°°μΉλ‹Ή ν† ν° throughputμ„ λΉ„κµν•λ©΄ μµμ ν™” ν¨κ³Όλ¥Ό μ •λ‰ν™”ν•  μ μμµλ‹λ‹¤. μμ»¨λ€, Unsloth ν€μ€ λ‹¤μ–‘ν• λ¨λΈ/λ°μ΄ν„°μ…‹μ— λ€ν•΄ <strong>μ΄λ‹Ή ν† ν° μ²λ¦¬λ‰</strong>μ„ μΈ΅μ •ν•μ—¬ Hugging Face λ€λΉ„ <strong>1.5Γ—~2.7Γ— μ†λ„ ν–¥μƒ</strong>μ„ λ³΄κ³  (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Free%20Colab%20T4%20Dataset%20Hugging,18.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L87γ€‘.</p>
<p>ν•™μµ ν¨μ¨ μ΄μ™Έμ—, <strong>λ¨λΈ μ„±λ¥ ν‰κ°€</strong> λν• ν•„μμ…λ‹λ‹¤. λ¨λΈμ΄ μ§€λ„νμΈνλ‹μ„ ν†µν•΄ λ©ν‘ μ‘μ—…μ— μ–Όλ§λ‚ ν–¥μƒλμ—λ”μ§€, λλ” νΉμ‹ <strong>κΈ°μ΅΄ μ§€μ‹μ„ ν›Όμ†</strong>ν•μ§€ μ•μ•λ”μ§€ λ“±μ„ ν™•μΈν•΄μ•Ό ν•©λ‹λ‹¤. <strong>Decoder-Only LLM</strong>μ κ²½μ° μΌλ°μ μΌλ΅ <strong>ν…μ¤νΈ μƒμ„± ν’μ§</strong>μ΄λ‚ <strong>λ‹¤μ–‘ν• λ‹¤μ΄μ¤νΈλ¦Ό νƒμ¤ν¬ μ„±λ¥</strong>μΌλ΅ ν‰κ°€ν•©λ‹λ‹¤. μλ¥Ό λ“¤μ–΄, <strong>μ§€λ„ν•™μµμΌλ΅ λ€ν™”ν• λ¨λΈ</strong>μ„ νλ‹ν–λ‹¤λ©΄ <strong>ChatGPTμ™€ μ μ‚¬ν• λ²¤μΉλ§ν¬(Vicuna Benchmark λ“±)</strong>μ—μ„ λ€ν™” ν’μ§μ„ μΈ΅μ •ν•κ±°λ‚, Human ν‰κ°€ νΉμ€ GPT-4λ¥Ό ν™μ©ν• λΉ„κµ ν‰κ°€λ¥Ό μν–‰ν•  μ (<a href="https://ar5iv.org/abs/2305.14314#:~:text=full%2016,innovations%20to%20save%20memory%20without">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>) (<a href="https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=on%20the%20Vicuna%20,Table%204"></a>)L107γ€‘. QLoRA λ…Όλ¬Έμ—μ„λ” <strong>GPT-4 κΈ°λ° μλ™ ν‰κ°€</strong>λ¥Ό ν†µν•΄, 65B λ¨λΈμ„ QLoRAλ΅ λ―Έμ„Έμ΅°μ •ν• Guanacoκ°€ ChatGPT λ€λΉ„ 99.3% μμ¤€μ— λ„λ‹¬ν–μμ„  (<a href="https://ar5iv.org/abs/2305.14314#:~:text=full%2016,innovations%20to%20save%20memory%20without">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>)-L18γ€‘. μ΄μ²λΌ <strong>λ¨λΈ μ¶λ ¥μ μ •λ‰Β·μ •μ„± ν‰κ°€</strong>λ¥Ό ν†µν•΄ νμΈνλ‹μ ν¨κ³Όλ¥Ό κ²€μ¦ν•΄μ•Ό ν•©λ‹λ‹¤. λν• <strong>perplexity</strong>(μ–Έμ–΄λ¨λΈμ λ΅κ·Έν™•λ¥  μ§€ν‘)λ„ μ‚¬μ©λλ”λ°, μ›λ λ¨λΈ λ€λΉ„ νΌν”λ ‰μ„ν‹° λ³€ν™”λ΅ <strong>κ³Όμ ν•© μ—¬λ¶€λ‚ μΌλ°ν™” μ„±λ¥</strong>μ„ κ°€λ ν•  μ μμµλ‹λ‹¤. μµμ‹  μ—°κµ¬μ— λ”°λ¥΄λ©΄ <strong>νμΈνλ‹ λ°μ΄ν„°μ ν’μ§μ΄ λ°μ΄ν„°λ‰λ³΄λ‹¤ μ¤‘μ”</strong>ν•λ©°, κ³ ν’μ§ μ†λ‰ λ°μ΄ν„°λ΅λ„ κ°•λ ¥ν• μ„±λ¥μ„ λ‚Ό μ (<a href="https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=Guanaco%2C%20,strong%20Vicuna%20chatbot%20benchmark%20performance"></a>) (<a href="https://openreview.net/pdf?id=OUIFPHEgJU#:~:text=analyze%20trends%20in%20the%20trained,strong%20Vicuna%20chatbot%20benchmark%20performance"></a>)L142γ€‘. Metaμ <strong>LIMA μ—°κµ¬(2023)</strong>μ—μ„λ” LLaMA 65B λ¨λΈμ„ <strong>μ—„μ„ λ 1000κ°μ μμ‹</strong>λ§μΌλ΅ μ§€λ„ν•™μµ νμΈνλ‹ ν•μ€μ„ λ• GPT-4 λ“± κ±°λ€ λ¨λΈμ— ν•„μ ν•λ” μ„±λ¥μ„ λ‹¬μ„±ν•κΈ°λ„ (<a href="https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling">Paper page - LIMA: Less Is More for Alignment - Hugging Face</a>)-L18γ€‘. μ΄λ” <strong>μ‚¬μ „ν•™μµλ κ±°λ€ LMμ μ μ¬λ ¥μ„ λμ–΄λ‚΄λ” λ° μμ–΄, λ°©λ€ν• μ–‘μ λ―Έμ„Έμ΅°μ • λ°μ΄ν„°λ³΄λ‹¤ μΈκ°„ μ „λ¬Έκ°€κ°€ κ³ λ¥Έ ν•µμ‹¬ λ°μ΄ν„°κ°€ ν¨κ³Όμ </strong>μΌ μ μμμ„ μ‹μ‚¬ν•©λ‹λ‹¤.</p>
<p>λ§μ§€λ§‰μΌλ΅, <strong>Decoder-Only Transformer μµμ ν™” κΈ°λ²•</strong>λ“¤μ„ μ •λ¦¬ν•λ©΄ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:</p>
<ul>
<li><strong>μ–‘μν™”(Quantization)</strong>: 16-bit λ€μ‹  8-bit, 4-bitλ΅ λ¨λΈ κ°€μ¤‘μΉλ¥Ό ν‘ν„ν•΄ λ©”λ¨λ¦¬ κ°μ† (μ: QLoRAμ 4-bit NF (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)L205γ€‘. μ μ ν• μ–‘μν™”λ” <strong>μ„±λ¥ μ μ§€ν•λ©΄μ„ λ©”λ¨λ¦¬ 4λ°° μ μ•½</strong> κ°€λ¥.</li>
<li><strong>νλΌλ―Έν„° ν¨μ¨ κΈ°λ²•(PEFT)</strong>: (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=More%20specifically%2C%20QLoRA%20uses%204,in%20the%20original%20LoRA%20paper">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)L210γ€‘, Adaptor, Prefix-Tuning λ“±μΌλ΅ <strong>μ†μμ νλΌλ―Έν„°λ§ ν•™μµ</strong>ν•μ—¬ μ—°μ‚°/λ©”λ¨λ¦¬ ν¨μ¨ κ°μ„ .</li>
<li><strong>Flash Attention λ“± λ©”λ¨λ¦¬ ν¨μ¨ Attention</strong>: μ‹ν€€μ¤ κΈΈμ΄κ°€ κΈΈμ–΄μ§ λ• λ©”λ¨λ¦¬ μ‚¬μ©μ„ μ¤„μ΄κ³  μ†λ„λ¥Ό λ†’μ΄λ” <strong>μµμ ν™” Attention μ•κ³ λ¦¬μ¦</strong>. PyTorch 2.xμ—μ„λ” μ΄λ¬ν• <strong>SDPA(Scaled Dot-Product Attention)</strong>κ°€ κΈ°λ³Έ ν†µν•©λμ–΄ μμ–΄ μ„±λ¥ ν–¥ (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20was%20benchmarked%20across%2059,are%20in%20Unsloth%E2%80%99s%20benchmarking%20details">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L92γ€‘.</li>
<li><strong>Gradient Checkpointing</strong>: μ¤‘κ°„ ν™μ„±κ°’μ„ μ €μ¥ν•μ§€ μ•κ³  μ¬κ³„μ‚°ν•λ” κΈ°λ²•μΌλ΅, <strong>GPU λ©”λ¨λ¦¬ μ‚¬μ©μ„ ν° ν­μΌλ΅ μ κ°</strong> (λ€μ‹  κ³„μ‚°λ‰ μ¦κ°€). λ€ν• λ¨λΈ νμΈνλ‹μ— κ±°μ ν•„μμ μΌλ΅ μ“°μ…λ‹λ‹¤.</li>
<li><strong>Mixed Precision Training</strong>: FP32 λ€μ‹  <strong>FP16/BF16</strong> λ“±μ„ μ‚¬μ©ν•μ—¬ <strong>μ—°μ‚° μ†λ„μ™€ λ©”λ¨λ¦¬ μ‚¬μ© μµμ ν™”</strong>. μµκ·Ό GPUλ” BF16/FP16 μ„±λ¥μ΄ λ›°μ–΄λ‚λ―€λ΅, μ •ν™•λ„μ— ν° λ¬Έμ μ—†μ΄ ν™μ©.</li>
<li><strong>λ¶„μ‚° λ³‘λ ¬ν™”</strong>: λ¨λΈ λ³‘λ ¬ν™”(λ μ΄μ–΄λ¥Ό μ—¬λ¬ GPUμ— λ¶„ν• ), λ°μ΄ν„° λ³‘λ ¬ν™”, νμ΄ν”„λΌμΈ λ³‘λ ¬ν™” λ“± μ΅°ν•©μΌλ΅ <strong>ν•λ“μ›¨μ–΄ μμ› ν™μ© κ·Ήλ€ν™”</strong>. DeepSpeed, FSDP, Megatron-LM λ“±μ΄ μ§€μ›.</li>
<li><strong>λ™μ  μ¥λΉ„ λ©”λ¨λ¦¬ ν™μ©</strong>: GPUμ™€ CPU, λ””μ¤ν¬λ¥Ό λ¨λ‘ ν™μ©ν•μ—¬ <strong>κ³„μ‚° μμ› λ€λΉ„ μµλ€ λ©”λ¨λ¦¬ ν™μ©</strong> (ZeRO-Offload/Infinit (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)L109γ€‘.</li>
<li><strong>μµμ‹  μµν‹°λ§μ΄μ € μ‚¬μ©</strong>: AdamW μ™Έμ— LAMB, Lion λ“±μ λ€μ• μµν‹°λ§μ΄μ €λ‚, DeepSpeedμ One-bit Adamμ²λΌ <strong>ν†µμ‹ λ‰μ„ μ¤„μΈ λ¶„μ‚° μµν‹°λ§μ΄μ €</strong>λ΅ ν¨μ¨ κ°μ„ .</li>
<li><strong>μ •κ·ν™” λ° μ•μ •ν™” κΈ°λ²•</strong>: λ€κ·λ¨ LM νμΈνλ‹ μ‹ <strong>λ¬λ‹λ μ΄νΈ μ›λ°μ—…</strong>, <strong>ν•™μµλ¥  μ¤μΌ€μ¤„</strong>, <strong>Gradient Clipping</strong> λ“±μΌλ΅ μ•μ •μ  μλ ΄μ„ λ„λ¨. μ΄λ” κ°„μ ‘μ μΌλ΅ ν¨μ¨(μ¬μ‹λ„ κ°μ† λ“±)μ— κΈ°μ—¬.</li>
<li><strong>Continuous Pretrainingκ³Ό SFT κ²°ν•©</strong>: κ²½μ°μ— λ”°λΌ <strong>μ‚¬μ „ν•™μµ μ—°μ¥(Continued Pretraining)</strong> ν›„ SFTλ¥Ό ν•λ©΄ λ” μΆ‹μ€ κ²°κ³Όλ¥Ό μ–»κ±°λ‚, SFT λ„μ¤‘ <strong>κΈ°μ΅΄ μ§€μ‹ μ μ§€</strong>λ¥Ό μ„ν• <strong>ζ··ε μ‚¬μ „ν•™μµ λ°μ΄ν„° μ‚¬μ©</strong> λ“±μ κΈ°λ²•λ„ μ—°κµ¬λκ³  (<a href="https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=the%20accuracy%20loss%20for%20QLoRA,LoRA%20is%20now%20largely%20recovered">Fine-tuning Guide | Unsloth Documentation</a>)L174γ€‘.</li>
</ul>
<h2 id="_2">μµμ‹  μ—°κµ¬ λ™ν–¥ λ° κ²°λ΅ <a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>μµκ·Ό 2λ…„κ°„ LLM νμΈνλ‹ λ¶„μ•Όλ” <strong>β€λ” μ μ€ μμ›μΌλ΅ λ” ν° λ¨λΈμ„ ν¨κ³Όμ μΌλ΅ λ‹¤λ£¨λ” λ²•β€</strong>μ— μ§‘μ¤‘λμ–΄ μ™”μµλ‹λ‹¤. <strong>QL (<a href="https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,innovations%20to%20save%20memory%20without">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>)-L18γ€‘μ λ“±μ¥μΌλ΅ μ΄‰λ°λ </strong>μ €λΉ„νΈ μ–‘μν™” + μ–΄λ‘ν„° ν•™μµ<strong> ν¨λ¬λ‹¤μ„μ€ ν„μ¬ μ—…κ³„ ν‘μ¤€μΌλ΅ μλ¦¬μ΅μ•κ³ , μ΄λ¥Ό λ„μ–΄ </strong>μ•„μ§ μ‹¤ν— λ‹¨κ³„μΈ 3λΉ„νΈ, 2λΉ„νΈ<strong> λ―Έμ„Ένλ‹ μ—°κµ¬λ„ μ§„ν–‰μ¤‘μ…λ‹λ‹¤. λν• </strong>LORAμ λ³€ν•<strong>μΌλ΅μ„ μ¤‘μ”λ„κ°€ λ†’μ€ λ μ΄μ–΄μ— κ°€μ¤‘μΉλ¥Ό λ” ν• λ‹Ήν•λ” </strong>AdaLoRA<strong> λ“±μ κΈ°λ²•λ„ μ μ•λμ—μµλ‹λ‹¤. ν•νΈ, </strong>νμΈνλ‹ λ°μ΄ν„° ν™•λ³΄<strong> μΈ΅λ©΄μ—μ„λ” Stanfordμ </strong>Alpaca<strong> ν”„λ΅μ νΈμ²λΌ </strong>κΈ°μ΅΄ λ¨λΈ(μ: GPT-3)λ¥Ό μ΄μ©ν• Self-Instruct λ°μ΄ν„° μƒμ„±<strong>μ΄ μ ν–‰ν•μ—¬, λΉ„κµμ  μ €λ ΄ν•κ² μ§€λ„ν•™μµ λ°μ΄ν„°λ¥Ό λ¨μΌλ” νλ¦„μ΄ μμµλ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ νƒ„μƒν• </strong>Vicuna<strong>, </strong>WizardLM<strong>, </strong>OpenAssistant<strong> λ“±μ </strong>μ¤ν”μ†μ¤ λ€ν™”ν• λ¨λΈ<strong>λ“¤μ€ λ¨λ‘ κ³µκ° λ°μ΄ν„°λ‚ μƒμ„± λ°μ΄ν„°λ΅ SFTλ μ‚¬λ΅€λ“¤μ…λ‹λ‹¤. μ„±λ¥ λ©΄μ—μ„, μ•μ„ μ–ΈκΈ‰ν• </strong>LIMA (Less is More for Alignme (<a href="https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling">Paper page - LIMA: Less Is More for Alignment - Hugging Face</a>)-L18γ€‘ μ—°κµ¬λ” <strong>κ³ ν’μ§ μ†κ·λ¨ λ°μ΄ν„°μ μ„λ ¥</strong>μ„ λ³΄μ—¬μ£Όμ—κ³ , OpenAIλ„ <strong>InstructGPT λ…Όλ¬Έ(2022)</strong>μ—μ„ μΈκ°„ ν”Όλ“λ°± μ™Έμ— <strong>μ΄κΈ° λ‹¨κ³„μ μνΌλ°”μ΄μ¦λ“ νμΈνλ‹(SFT)</strong>μ΄ ν•µμ‹¬μ μΌλ΅ μ¤‘μ”ν•¨μ„ λ°ν λ°” μμµλ‹λ‹¤. μµκ·Όμ—λ” <strong>RLHF</strong>(κ°•ν™”ν•™μµ ν΄λ¨Ό ν”Όλ“λ°±) λ€μ‹  <strong>DPO</strong>(Direct Preference Optimization)λ‚ <strong>RLAIF</strong>(AI ν”Όλ“λ°±) λ“± <strong>μμ μ§€λ„ μ‹ νΈλ§μΌλ΅ μ„ νΈλ„λ¥Ό ν•™μµ</strong>ν•λ ¤λ” μ‹λ„λ„ λ‚μ¤κ³  μμ–΄, <strong>μ§€λ„ νμΈνλ‹μ λ²”μ„κ°€ ν™•μ¥</strong>λκ³  μμµλ‹λ‹¤.</p>
<p>μ •λ¦¬ν•λ©΄, <strong>Decoder-Only LLMμ μ§€λ„ νμΈνλ‹</strong>μ€ μ—¬μ „ν <strong>λ¨λΈ μ„±λ¥ κ°μ„ κ³Ό ν¨μ¨μ  ν•™μµ</strong>μ„ μ–‘λ¦½ν•κΈ° μ„ν• λ‹¤μ–‘ν• μ—°κµ¬λ΅ ν™λ°ν μ§„ν™”ν•κ³  μμµλ‹λ‹¤. Hugging Face, DeepSpeed, Unslothμ™€ κ°™μ€ λ„κµ¬λ“¤μ€ μ΄λ¬ν• μ—°κµ¬ μ„±κ³Όλ¥Ό ν„μ—…μ— μ μ©ν•λ” λ‹¤λ¦¬ μ—­ν• μ„ ν•λ©°, κ°κΈ° <strong>μ‚¬μ©μ μ”κµ¬μ™€ ν™κ²½μ— λ§λ” μ†”λ£¨μ…</strong>μ„ μ κ³µν•©λ‹λ‹¤. μ‹¤λ¬΄μ—μ„λ” μ„Έ κ°€μ§€ μ ‘κ·Όλ²•μ„ <strong>μƒν™©μ— λ”°λΌ μ΅°ν•©</strong>ν•κΈ°λ„ ν•©λ‹λ‹¤. μλ¥Ό λ“¤μ–΄, <strong>μ¤‘κ°„ κ·λ¨ λ¨λΈμ€ Unslothλ΅ μ‹±κΈ€ GPU λΉ λ¥΄κ² νλ‹</strong>ν•κ³ , <strong>μ΄κ±°λ€ λ¨λΈμ€ DeepSpeedλ΅ λ©€ν‹° GPU λ¶„μ‚° ν•™μµ</strong>ν•λ©°, μ „λ°μ μΈ μ›ν¬ν”λ΅μ°λ” Hugging Face μ—μ½”μ‹μ¤ν…μΌλ΅ κ΄€λ¦¬ν•λ” μ‹μ…λ‹λ‹¤. μ¤‘μ”ν• κ²ƒμ€ <strong>λ¨λΈμ λ©ν‘μ™€ μ μ•½μ— λ§μ¶° μµμ μ κΈ°λ²•μ„ μ„ νƒ</strong>ν•λ” κ²ƒμ…λ‹λ‹¤. μ•μΌλ΅λ„ ν•λ“μ›¨μ–΄μ™€ μ•κ³ λ¦¬μ¦ μΈ΅λ©΄μ λ°μ „μΌλ΅ LLM νμΈνλ‹μ€ λ”μ± μµμ ν™”λκ³  λ€μ¤‘ν™”λ  κ²ƒμ΄λ©°, <strong>β€λ” λ‚®μ€ λΉ„μ©μΌλ΅ λ” λ‘λ‘ν• λ¨λΈβ€</strong>μ„ λ§λ“λ” λ°©ν–¥μΌλ΅ λ‚μ•„κ° κ²ƒμ…λ‹λ‹¤.</p>
<p><strong>μ°Έκ³  λ¬Έν— λ° λ§ν¬:</strong> μµμ‹  νμΈνλ‹ κΈ°λ²•κ³Ό μ‚¬λ΅€μ— λ€ν• μμ„Έν• λ‚΄μ©μ€ Hugging Face λΈ”λ΅κ·Έ λ° κ° λ…Όλ¬Έμ μ›λ¬Έμ„ μ°Έκ³ ν•μ‹κΈ° λ°”λλ‹λ‹¤. μ•„λλ” λ³Έ λ¬Έμ„μ—μ„ μ–ΈκΈ‰λ μλ£λ“¤μ μ¶μ²μ…λ‹λ‹¤.</p>
<ul>
<li>Hugging Face λΈ”λ΅κ·Έ: *Making LLMs even more accessible with 4-bit quantization and Q (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=In%20few%20words%2C%20QLoRA%20reduces,on%20a%20single%2046GB%20GPU">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>) (<a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=QLoRA%20tuning%20is%20shown%20to,the%20power%20of%20QLoRA%20tuning">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>)L222γ€‘</li>
<li>Hugging Face λΈ”λ΅κ·Έ: *Make LLM fine-tuning 2x faster with Unsloth and (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=Unsloth%20works%20by%20overwriting%20some,made%20in%20the%20optimized%20code">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L80γ€‘</li>
<li>Hugging Face Docs: *DeepSpeed &amp; Accelerate Integration G (<a href="https://huggingface.co/docs/peft/main/en/accelerate/deepspeed#:~:text=DeepSpeed%20is%20a%20library%20designed,leveraging%20CPU%20resources%20during%20optimization">DeepSpeed</a>) (<a href="https://www.deepspeed.ai/tutorials/zero-offload/#:~:text=ZeRO,No%20code%20changes%20are%20needed">ZeRO-Offload - DeepSpeed</a>)L109γ€‘</li>
<li>Unsloth κ³µμ‹ λ¬Έ (<a href="https://docs.unsloth.ai/get-started/fine-tuning-guide#:~:text=,tuning">Fine-tuning Guide | Unsloth Documentation</a>) (<a href="https://huggingface.co/blog/unsloth-trl#:~:text=1%20A100%2040GB%20Dataset%20Hugging,11.6">Make LLM Fine-tuning 2x faster with Unsloth and  TRL</a>)-L77γ€‘</li>
<li>QLoRA λ…Όλ¬Έ (Dettmers et al.,  (<a href="https://ar5iv.org/abs/2305.14314#:~:text=We%20present%20QLoRA%2C%20an%20efficient,innovations%20to%20save%20memory%20without">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>) (<a href="https://ar5iv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,of">[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs</a>)-L27γ€‘</li>
<li>LIMA λ…Όλ¬Έ (Zhou et al.,  (<a href="https://huggingface.co/papers/2305.11206#:~:text=Face%20huggingface,learning%20or%20human%20preference%20modeling">Paper page - LIMA: Less Is More for Alignment - Hugging Face</a>)-L18γ€‘</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.path", "navigation.instant", "navigation.tracking", "navigation.sections", "navigation.footer", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "\ud074\ub9bd\ubcf4\ub4dc\uc5d0 \ubcf5\uc0ac\ub428", "clipboard.copy": "\ud074\ub9bd\ubcf4\ub4dc\ub85c \ubcf5\uc0ac", "search.result.more.one": "\uc774 \ubb38\uc11c\uc5d0\uc11c 1\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.more.other": "\uc774 \ubb38\uc11c\uc5d0\uc11c #\uac1c\uc758 \uac80\uc0c9 \uacb0\uacfc \ub354 \ubcf4\uae30", "search.result.none": "\uac80\uc0c9\uc5b4\uc640 \uc77c\uce58\ud558\ub294 \ubb38\uc11c\uac00 \uc5c6\uc2b5\ub2c8\ub2e4", "search.result.one": "1\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.other": "#\uac1c\uc758 \uc77c\uce58\ud558\ub294 \ubb38\uc11c", "search.result.placeholder": "\uac80\uc0c9\uc5b4\ub97c \uc785\ub825\ud558\uc138\uc694", "search.result.term.missing": "\ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uac80\uc0c9\uc5b4", "select.version": "\ubc84\uc804 \uc120\ud0dd"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>