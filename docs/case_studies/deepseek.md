# DeepSeek - 중국의 오픈소스 MoE 모델 혁신 사례

<div style="text-align: center;">
  <img src="../../rscs/deepseek_logo.svg" alt="deepseek_logo">
</div>

> 그 유명한 DeepSeek 입니다. 미국 증시 시장을 떨어뜨린 모델이죠... 25년 설날에 집에 가니까 91세 할머니도 저한테 deepseek 가 뭔데 뉴스에서 저러냐고 물어보시더라고요...?!

## DeepSeek V3 & R1 시리즈 - 2023.12

DeepSeek은 중국 스타트업이 개발한 **오픈 웨이트** LLM 시리즈입니다. 특히 V3는 Mixture-of-Experts(MoE) 구조를 채택하여 671B 파라미터 중 토큰당 37B 개만 활성화하는 효율적인 설계가 특징입니다. R1은 V3를 기반으로 강화학습을 통해 Reasoning 능력을 갖추도록 특화된 모델입니다.

- **개발 배경**: 고성능 AI를 저렴한 비용으로 제공하고자 했습니다. OpenAI 대비 "50배 저렴한 이용료"를 목표로 했다네요, 실제로 엄청 싸게 만들었습니다.  
- **기술적 혁신**: MoE 구조, 다중 토큰 예측(MTP), FP8 (보통은 BF16을 가장 많이 씁니다) 연산, Multi-Head Latent Attention(MLA) 등 여러 최신 기법을 도입했습니다.
- **반쯤 오픈소스**: 데이터는 공개하지 않았지만, 어떻게 만들었는지에 대한 꽤 기술 보고서, 그리고 엄청나게 많은 도구들을 오픈소스로 공개하여 누구나 활용할 수 있습니다.

| 💡 필자의 의견 |
|---------|
| DeepSeek이 대중적으로 이슈를 만들게 된 원인은 크게 2가지 입니다,  첫번째는 V3 라는 671B의 크고 좋은 모델은 (본인들 주장) 100억에 정도의 싼 비용으로 달성했고, 두번째는 R1 이라는 Reasoning 모델 (OpenAI o1 같은) 을 만드는 비법 소스를 상세하게 공개한 것이죠. |
| 갑론을박이 있습니다만, 다소 과정이 있더라도 2가지 요소 다 contribution이 매우 크다고 생각합니다. | 


(TODO) 보다 상세히 적을 예정.

## 모델 구조 및 기술적 특징

### 1. 기본 아키텍처
- **Transformer 기반**: 61층의 Transformer 구조를 채택했습니다.
- **MoE 구조**: 총 6,710억 개(671B)의 파라미터 중 토큰당 약 370억 개(37B)만 활성화됩니다.
- **전문가 네트워크**: 각 MoE 층에 256개의 전문가 중 8개만 선택적으로 활성화됩니다.
- **내부 구성**: 숨김 크기(dim) 7168, 주의 헤드(head) 수 128개로 구성되었습니다.

### 2. 기술적 혁신
- **부하 균형 전략**: 기존 MoE 모델에서 사용하던 보조손실 없이도 전문가 부하를 균등하게 분산시켰습니다.
- **다중 토큰 예측(MTP)**: 한 번에 여러 토큰을 예측하도록 학습하여 생성 성능을 향상시켰습니다.
- **FP8 저정밀도 연산**: 8비트 부동소수점 정밀도로 학습하여 메모리 사용량을 줄이고 연산속도를 높였습니다.
- **분산 병렬 학습 최적화**: 노드 간 통신과 계산을 거의 완전히 중첩시켜 학습 효율을 극대화했습니다.
- **Multi-Head Latent Attention(MLA)**: 자체 최적화된 어텐션 기법으로 메모리-통신 효율을 유지했습니다.

### 3. V3와 R1의 차이점
- **V3**: 일반적 대화 및 다목적 활용에 최적화된 기본 모델입니다.
- **R1(DeepThink)**: V3를 기반으로 강화학습을 통해 고난도 추론에 특화되었습니다.
- **추론 방식**: R1은 내부적으로 자신만의 추론 과정을 텍스트 형태로 전개하는 chain-of-thought 능력이 뛰어납니다.
- **응답 속도**: V3는 약 60 토큰/초로 빠른 반면, R1은 더 깊은 추론 과정으로 인해 상대적으로 느립니다.

## 학습 데이터 및 훈련 방식

### 1. V3의 학습 데이터
- **규모**: 약 14.8조(Trillion) 토큰 규모의 방대한 텍스트로 사전학습되었습니다.
- **구성**: 영어와 중국어를 중심으로 다양한 언어를 포함하며, 수학 및 프로그래밍 샘플 비중을 늘렸습니다.
- **다양성**: 뉴스, 백과사전, 웹 크롤링, 문학, 코드, 과학 논문 등 다양한 도메인의 고품질 텍스트를 포함합니다.
- **전처리**: 중복 제거 및 정제를 철저히 했으며, 문서 단위로 시퀀스를 패킹하여 일관성 있는 학습 샘플을 만들었습니다.

### 2. V3의 사전학습 및 미세조정
- **학습 환경**: 대규모 병렬 GPU 클러스터(약 2,000개의 H800 GPU)에서 수백억 단계로 학습되었습니다.
- **학습 비용**: 약 278만 GPU-시간(약 $560만 달러) 수준으로 추산됩니다.
- **컨텍스트 확장**: 4K 토큰 시퀀스 길이로 학습한 뒤 단계적으로 128K까지 확장했습니다.
- **미세조정**: 인간 피드백을 활용한 지도 미세조정(SFT)과 강화학습 단계를 포함한 최적화를 진행했습니다.
- **지식 증류**: R1 모델의 추론 패턴을 활용해 V3를 한번 더 강화하는 독특한 접근을 취했습니다.

### 3. R1의 훈련 과정
- **기반 모델**: V3-Base(사전학습 모델)을 토대로 고난도 문제해결 능력을 배양하는 별도 훈련을 거쳤습니다.
- **R1-Zero**: 전적으로 강화학습(RL)만으로 학습시킨 실험적 모델을 먼저 개발했습니다.
- **자율 학습**: 인간이 만든 정답 예시 없이, 모델이 주어진 문제에 대해 여러 시도를 생성하고 규칙 기반 보상을 통해 개선했습니다.
- **보완 조치**: 소량의 고품질 데이터로 지도 미세조정(SFT)을 진행하여 "콜드 스타트" 단계를 거쳤습니다.
- **혼합 강화학습**: 단순한 규칙 보상뿐 아니라 모델 출력에 대한 평가관점 보상도 활용했습니다.
- **GRPO 기법**: DeepSeek 고유의 Group Relative Policy Optimization이라는 대규모 RL 최적화 기법을 활용했습니다.
- **지식 증류**: R1의 논리력을 압축해 담은 소형 모델들(R1-Lite)도 여러 버전 공개했습니다.

## 성능

### 1. 벤치마크 결과
- **MMLU**: V3는 약 88~89%, R1은 89.8~90.8%의 정확도를 보여 GPT-4 계열과 비슷한 성능을 냈습니다.
- **MATH-500**: V3가 90% 내외, R1은 92% 이상의 정확도를 기록해 최첨단 수준입니다.
- **AIME 2024**: 고등 수학 경시 대회 문제에서 R1이 79.8% 정답률을 달성하여, V3(약 39%)보다 크게 향상된 문제해결력을 보였습니다.
- **HumanEval**: V3는 코딩 능력 평가에서 82.6%의 정답률(Pass@1)을 기록해 OpenAI GPT-4에 필적하거나 앞섰습니다.

### 2. 속도 및 효율성
- **추론 속도**: V3는 약 60 토큰/초로 이전 세대(V2)보다 3배 이상 빠릅니다.
- **컨텍스트 길이**: 최대 128K 토큰까지 긴 문맥을 처리할 수 있습니다(API에서는 64K로 제한).
- **R1의 특성**: 복잡한 문제에서 R1은 단계별 추론을 수행하기 때문에 응답 생성이 다소 느립니다.
- **효율성**: MoE 구조 덕분에 필요한 일부 파라미터만 활성화하여 응답 지연이 적고 효율적입니다.

## 활용 사례

### 1. 대표 사용 분야
- **V3**: ChatGPT와 유사한 AI 비서로서 일상 언어 대화, 글쓰기 보조, 정보 검색, 번역 등에 두루 활용됩니다.
- **R1**: 고난도 문제 해결 및 전문 분야에 강점을 보이며, 수학 문제 풀이, 알고리즘 설계, 논리 퍼즐, 복잡한 질의 응답 등에 적합합니다.
- **다국어 지원**: V3는 다중언어 지원이 우수하여 국제 서비스에 적합하고, R1은 단일언어 고난도 분석에 적합합니다.
- **긴 맥락 처리**: 두 모델 모두 최대 64K 토큰 이상의 긴 맥락을 처리할 수 있어, 장문 문서 요약이나 긴 대화 유지에 활용됩니다.

### 2. 도입 현황
- **자체 서비스**: 딥시크 자체 앱과 웹(chat.deepseek.com)을 통해 일반 사용자가 V3와 R1을 직접 체험할 수 있습니다.
- **API 제공**: 기업과 개발자를 위해 API 형태로 V3/R1 접근을 제공하고 있습니다.
- **클라우드 통합**: 구글 GCP의 Vertex AI, AWS의 Bedrock, MS Azure AI 등의 서비스에 DeepSeek 모델이 등록되어 있습니다.
- **연구 활용**: Hugging Face의 Open-R1 프로젝트처럼 DeepSeek의 공개 가중치를 활용한 재현 실험이 진행 중입니다.

## 가격 및 접근성

### 1. 이용 비용
- **무료 옵션**: 개인 사용자는 딥시크 공식 웹이나 모바일 앱에서 DeepSeek-V3를 무료로 이용할 수 있습니다.
- **API 요금**: V3는 입력 100만 토큰당 $0.27, 출력 100만 토큰당 $1.10의 기본요금입니다.
- **R1 요금**: 입력 $0.55/백만, 출력 $2.19/백만 토큰으로 V3보다 약 2배 수준입니다.
- **할인 정책**: 비혼잡 시간대에는 V3 비용 50%, R1 비용 75% 할인을 제공합니다.
- **비용 효율성**: OpenAI 대비 "30~50배 저렴한 이용료"를 강점으로 내세우고 있습니다.

### 2. 접근성
- **오픈소스**: 완전 오픈소스로 출시되어, 누구나 모델 가중치와 기술 보고서를 열람할 수 있습니다.
- **자체 배포**: 충분한 컴퓨팅 자원이 있는 조직이라면 직접 다운로드 받아 자체 인프라에서 배포할 수 있습니다.
- **클라우드 API**: 일반 사용자나 소규모 개발팀은 DeepSeek의 클라우드 API를 통해 손쉽게 모델을 활용할 수 있습니다.
- **경량 버전**: R1-Lite 등 경량화된 버전은 개별 서버나 PC에서도 실행 가능합니다.

## 의의

### 1. 기술적 혁신
- MoE 구조를 대규모로 성공적으로 적용하여 효율적인 초거대 모델을 구현했습니다.
- 복잡한 강화학습을 통해 모델이 스스로 추론 능력을 향상시키는 방법론을 개발했습니다.
- 오픈소스 공개를 통해 AI 연구 커뮤니티에 기여했습니다.

### 2. 산업적 영향
- 고성능 AI를 저렴한 비용으로 제공하여 AI 도입 장벽을 낮췄습니다.
- 중국 기업이 글로벌 AI 시장에서 기술적 경쟁력을 입증한 사례가 되었습니다.
- 다양한 산업 분야에서 활용 가능한 범용 및 특화 모델을 제공했습니다.

참고 자료:
- DeepSeek 공식 홈페이지 및 API 문서 (https://www.deepseek.com/)
- DeepSeek-V3 Technical Report (https://arxiv.org/html/2412.19437v1)
- HuggingFace DeepSeek-V3 모델 카드 (https://huggingface.co/deepseek-ai/DeepSeek-V3)
- Open-R1: a fully open reproduction of DeepSeek-R1 (https://huggingface.co/blog/open-r1) 